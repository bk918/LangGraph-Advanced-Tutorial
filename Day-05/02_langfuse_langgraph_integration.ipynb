{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정 및 Langfuse 연결\n",
    "\n",
    "### 필수 패키지 확인\n",
    "\n",
    "**참고**: LangFuse 3.9.0 버전에서는 `CallbackHandler`가 `langfuse.langchain` 모듈에 위치합니다.\n",
    "이전 버전과의 호환성을 위해 import 경로에 fallback 처리가 포함되어 있습니다.\n",
    "\n",
    "### 참고문서\n",
    "- Langfuse 가이드: https://langfuse.com/docs\n",
    "- RAGAS 프레임워크: https://docs.ragas.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 환경 변수 확인\n",
    "\n",
    "Langfuse를 사용하기 위해 다음 환경 변수가 필요합니다:\n",
    "\n",
    "- `LANGFUSE_PUBLIC_KEY`: Langfuse Public Key (pk-lf-로 시작)\n",
    "- `LANGFUSE_SECRET_KEY`: Langfuse Secret Key (sk-lf-로 시작)\n",
    "- `LANGFUSE_HOST`: Langfuse 서버 URL (기본값: http://localhost:3000, Cloud 기본값: https://us.cloud.langfuse.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.agents import create_agent\n",
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage, filter_messages\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_tavily import TavilySearch\n",
    "from langfuse.langchain import CallbackHandler\n",
    "from openrouter_llm import create_embedding_model, create_openrouter_llm\n",
    "\n",
    "load_dotenv()\n",
    "llm = model = create_openrouter_llm(\"openai/gpt-4.1\", temperature=0)\n",
    "embeddings = create_embedding_model(\"openai/text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langfuse 연결 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No Langfuse client with public key pk-lf-a62c70c5-57cc-4147-8d63-525204fa10f4 has been initialized. Skipping tracing for decorated function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langfuse.langchain.CallbackHandler.LangchainCallbackHandler at 0x11dcd5a70>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Langfuse 콜백 핸들러 초기화\n",
    "# 참고: CallbackHandler는 public_key만 받고, secret_key와 host는 환경 변수에서 자동으로 읽습니다.\n",
    "# 환경 변수가 설정되어 있다면 public_key만 전달해도 됩니다.\n",
    "langfuse_handler = CallbackHandler(\n",
    "    public_key=os.getenv(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "    update_trace=True,\n",
    ")\n",
    "langfuse_handler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langfuse 콜백 통합\n",
    "\n",
    "### 트레이스 이름 및 메타데이터 설정\n",
    "\n",
    "**참고**: LangFuse 3.9.0에서는 `CallbackHandler`가 자동으로 트레이스를 생성합니다.\n",
    "트레이스 이름과 메타데이터는 LangGraph 실행 시 `config`의 `metadata`를 통해 전달할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Answer ===\n",
      "RAGAS와 DeepEval은 둘 다 LLM(대형 언어 모델)·RAG(Retrieval-Augmented Generation) 시스템의 평가를 위한 오픈소스 라이브러리이지만, 주요 차이점은 다음과 같습니다.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. 평가 목적과 기능의 범위\n",
      "\n",
      "- **RAGAS**\n",
      "  - 주로 RAG 시스템, 즉 '검색-증강 생성' 파이프라인의 품질 평가에 특화되어 있습니다.\n",
      "  - 대표적으로 네 가지 평가 지표(RAGASAnswerRelevancy, RAGASFaithfulness, RAGASContextualPrecision, RAGASContextualRecall)로 정량적 수치를 제공합니다.\n",
      "  - 데이터 기반의 실험적 평가, 혹은 빠른 실험에 적합하며, LangChain, LlamaIndex 등 RAG 프레임워크와 밀접하게 연동됩니다.\n",
      "  - 커스터마이즈나 CI/CD, 엔터프라이즈 통합보다는 연구적, 데이터 분석적 실험에 적합합니다.\n",
      "  - 숫자 중심의 메트릭 제공에 중점을 두지만, 결과 해석이 직관적이지 않을 수 있습니다.\n",
      "\n",
      "- **DeepEval**\n",
      "  - RAG뿐 아니라 챗봇, LLM 에이전트 등 다양한 LLM 어플리케이션 평가에 쓸 수 있습니다.\n",
      "  - 기존 RAGAS의 평가 지표들도 포함하지만, pass/fail 기준 도입, 개발자 친화적인 테스트 통합(예, Pytest 연동), 다양한 맞춤형 평가 지표(예, 독성·데이터 유출 탐지 등 안전성 테스트) 기능을 제공합니다.\n",
      "  - 커스텀 메트릭과 데이터셋 생성, CI/CD(지속적 통합/배포) 파이프라인에서의 자동화된 테스트, 기업용 협업·리포팅 지원 등 대규모 개발 환경에 최적화되어 있습니다.\n",
      "  - 테스트 결과를 단순 수치(스코어) 외에, 합격/불합격 기준 등으로 직관적으로 해석하도록 돕습니다.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. 사용성 및 커스터마이징\n",
      "\n",
      "- RAGAS: 주로 연구자, 데이터과학자에게 어울리며, 개발경험이나 맞춤형 테스트, 확장성은 제한적입니다.\n",
      "- DeepEval: 개발자 및 엔지니어를 주 사용자로, 코드 몇 줄로 맞춤형 생성형 데이터셋이나 평가 기준을 쉽게 만들 수 있습니다.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. 생태계 및 확장성\n",
      "\n",
      "- RAGAS는 LangChain, LlamaIndex 내에 통합되어 사용이 제한적이며 외부 확장성은 약합니다.\n",
      "- DeepEval은 자체 프레임워크로, 다양한 곳에 쉽게 붙일 수 있고, 엔터프라이즈 기능(예, Confident AI 플랫폼 연동 등)과 넓은 커뮤니티, 빠른 업데이트를 자랑합니다.\n",
      "\n",
      "---\n",
      "\n",
      "### 요약 표\n",
      "\n",
      "| 항목        | RAGAS                        | DeepEval                      |\n",
      "|-------------|------------------------------|-------------------------------|\n",
      "| 주요용도    | RAG 품질 분석                | LLM 전반, CI/CD, 안전성 등    |\n",
      "| 커스터마이징| 제한적                       | 매우 유연(테스트 기준, 데이터셋 등) |\n",
      "| 사용대상    | 연구자, 데이터과학자          | 개발자, 엔터프라이즈 팀       |\n",
      "| 플랫폼      | LangChain, LlamaIndex 연계    | 독립적, Confident AI 통합 가능|\n",
      "| 결과해석    | 숫자지표 중심, 직관성 부족 가능| pass/fail 등 직관적 해석 지원 |\n",
      "\n",
      "---\n",
      "\n",
      "#### 참고 출처\n",
      "- [DeepEval vs Ragas - DeepEval 공식 블로그](https://deepeval.com/blog/deepeval-vs-ragas)\n",
      "- [RAGAS | DeepEval 공식 문서](https://deepeval.com/docs/metrics-ragas)\n",
      "- [최신 RAG 평가 도구 소개 블로그](https://www.cohorte.co/blog/evaluating-rag-systems-in-2025-ragas-deep-dive-giskard-showdown-and-the-future-of-context)\n",
      "\n",
      "추가 궁금한 점이 있다면 사례나 사용 목적에 맞는 도구 추천도 드릴 수 있습니다!\n",
      "\n",
      "====================================================================================================\n",
      "=== Search Contexts===\n",
      "\n",
      "[Context 1]\n",
      "Title: RAGAS | DeepEval - The Open-Source LLM Evaluation Framework\n",
      "Content: The `RAGASMetric` uses the `ragas` library under the hood and are available on `deepeval` with the intention to allow users of `deepeval` can have access to `ragas` in `deepeval`'s ecosystem as well. They are implemented in an almost identical way to `deepeval`'s default RAG metrics. However there are a few differences, including but not limited to: [...] DeepEval Logo\n",
      "DeepEval Logo\n",
      "\n",
      "# RAGAS\n",
      "\n",
      "The RAGAS metric is the average of four distinct metrics:\n",
      "\n",
      "`RAGASAnswerRelevancyMetric`\n",
      "`RAGASFaithfulnessMetric`\n",
      "`RAGASContextualPrecisionMetric`\n",
      "`RAGASContextualRecallMetric`\n",
      "\n",
      "It provides a score to holistically evaluate of your RAG pipeline's generator and retriever. [...] `deepeval`\n",
      "`from deepeval import evaluate  \n",
      "from deepeval.metrics.ragas import RagasMetric  \n",
      "from deepeval.test_case import LLMTestCase  \n",
      "  \n",
      "# Replace this with the actual output from your LLM application  \n",
      "actual_output = \"We offer a 30-day full refund at no extra cost.\"  \n",
      "  \n",
      "# Replace this with the expected output from your RAG generator  \n",
      "expected_output = \"You are eligible for a 30 day full refund at no extra cost.\"\n",
      "URL: https://deepeval.com/docs/metrics-ragas\n",
      "--------------------------------------------------\n",
      "\n",
      "[Context 2]\n",
      "Title: Evaluating RAG Systems in 2025: RAGAS Deep Dive, Giskard ...\n",
      "Content: will fail the test if not‍. This is great for developers who want to enforce quality via CI pipelines. DeepEval’s philosophy is similar to RAGAS (LLM-as-a-judge metrics), and indeed it overlaps on metrics; one difference noted by reviewers is that RAGAS’s numeric metrics, while thorough, sometimes aren’t self-explanatory in isolation, whereas DeepEval encourages explicit pass/fail criteria which can be easier to interpret in a test context. [...] multi-tool for AI evaluation – it’s not limited to RAG and adds trust metrics (catching things like toxicity or data leaks), with a focus on explainability and customization for enterprise needs. Other tools like DeepEval, LlamaIndex’s evaluators, and more are also in the mix, each with their strengths. As LLMs get bigger brains (contexts), we may lean on retrieval a bit less, but we’ll always need to verify that our giant-brained models are using their brains correctly. So whether you stick to [...] The list goes on – new tools and frameworks are popping up (LangSmith by LangChain for example, or NeuralTrust’s evaluators‍). The good news is the community recognizes that evaluation is critical for LLM applications, and we’re seeing rapid innovation to make it easier. Whether you prioritize deep metrics (RAGAS), broad safety checks (Giskard), or integration into CI (DeepEval) or MLOps platforms, there’s likely an open-source project out there to help. Don’t be afraid to mix and match these\n",
      "URL: https://www.cohorte.co/blog/evaluating-rag-systems-in-2025-ragas-deep-dive-giskard-showdown-and-the-future-of-context\n",
      "--------------------------------------------------\n",
      "\n",
      "[Context 3]\n",
      "Title: DeepEval vs Ragas - The Open-Source LLM Evaluation Framework\n",
      "Content: If DeepEval is so good, why is Ragas so popular? Ragas started off as a research paper that focused on the reference-less evaluation of RAG pipelines in early 2023 and got mentioned by OpenAI during their dev day in November 2023.\n",
      "\n",
      "But the very research nature of Ragas means that you're not going to get as good a developer experience compared to DeepEval. In fact, we had to re-implement all of Ragas's metrics into our own RAG metrics back in early 2024 because they didn't offer things such as: [...] DeepEval and Ragas both offers in dataset generation, and while Ragas is deeply locked into the Langchain and LlamaIndex ecosystem, meaning you can't easily generate from any documents, and offers limited customizations, DeepEval's synthesizer is 100% customizable within a few lines of code\n",
      "\n",
      "If you look at the table below, you'll see that DeepEval's synthesizer is very flexible.\n",
      "\n",
      "DeepEval\n",
      "\n",
      "Ragas\n",
      "\n",
      "Generate from documents\n",
      "\n",
      "Synthesize goldens that are grounded in documents [...] How is DeepEval Different?\n",
      "  + 1. We're built for developers\n",
      "  + 2. We care about your experience, a lot\n",
      "  + 3. We have a vibrant community\n",
      "  + 4. We ship extremely fast\n",
      "  + 5. We offer more features, with less bugs\n",
      "  + 6. We scale with your evaluation needs\n",
      " Comparing DeepEval and Ragas\n",
      "  + Metrics\n",
      "  + Dataset Generation\n",
      "  + Red teaming\n",
      "  + Benchmarks\n",
      "  + Integrations\n",
      "  + Platform\n",
      " Conclusion\n",
      "\n",
      "# This page crashed\n",
      "\n",
      "cannot read property 'appendChild' of undefined\n",
      "URL: https://deepeval.com/blog/deepeval-vs-ragas\n",
      "--------------------------------------------------\n",
      "\n",
      "[Context 4]\n",
      "Title: All DeepEval Alternatives, Compared\n",
      "Content: 2. Breadth of features: DeepEval supports a wide range of LLM evaluation types beyond RAG, including chatbot, agents, and scales to safety testing, whereas Ragas is more narrowly focused on RAG-specific evaluation metrics.\n",
      "3. Platform support: DeepEval is integrated natively with Confident AI, which makes it easy to bring LLM evaluation to entire organizations. Ragas on the other hand barely has a platform and all it does is an UI for metric annotation. [...] 1. Developer Experience: DeepEval offers a highly customizable and developer-friendly experience with plug-and-play metrics, Pytest CI/CD integration, graceful error handling, great documentation, while Ragas provides a data science approach and can feel more rigid and lackluster in comparison. [...] This guide is an overview of some alternatives to DeepEval, how they compare, and why people choose DeepEval.\n",
      "\n",
      "## Ragas​\n",
      "\n",
      " Company: Exploding Gradients, Inc.\n",
      " Founded: 2023\n",
      " Best known for: RAG evaluation\n",
      " Best for: Data scientist, researchers\n",
      "\n",
      "Ragas is most known for RAG evaluation, where the founders originally released a paper on the referenceless evaluation of RAG pipelines back in early 2023.\n",
      "\n",
      "### Ragas vs Deepeval Summary​\n",
      "\n",
      "DeepEval\n",
      "\n",
      "Ragas\n",
      "\n",
      "RAG metrics\n",
      "URL: https://deepeval.com/blog/deepeval-alternatives-compared\n",
      "--------------------------------------------------\n",
      "\n",
      "[Context 5]\n",
      "Title: RAG Evaluation: The Definitive Guide to Unit Testing RAG in CI/CD\n",
      "Content: If you aim to implement your own evaluation metrics to address the shortcomings of generic RAG metrics and seek a production-grade testing framework for inclusion in CI/CD pipelines, DeepEval is an excellent option. We’ve done all the hard work for you already, and it features over 14 evaluation metrics, supports parallel test execution, and is deeply integrated with Confident AI, the world’s first open-source evaluation infrastructure for LLMs. [...] Besides the generic RAG evaluation metrics, here’s how you can incorporate additional LLM evaluation metrics into your RAG evaluation pipeline using DeepEval. First, install DeepEval:\n",
      "\n",
      "```\n",
      "install\n",
      "```\n",
      "\n",
      "Then, import and define your RAGAs metrics:\n",
      "\n",
      "```\n",
      "from.. import(,,,,) =() =() =() =()\n",
      "```\n",
      "\n",
      "And include any additional metrics apart from the RAG metrics, using G-Eval: [...] Although we showed how to use DeepEval’s evaluate function in the previous section, we’re going to ditch that approach here and leverage DeepEval’s Pytest integration instead.\n",
      "\n",
      "To begin, create a test file:\n",
      "\n",
      "```\n",
      "touch\n",
      "```\n",
      "\n",
      "#### Initialize Evaluation Metrics\n",
      "\n",
      "Similar to the previous example, initialize your evaluation metrics in the newly created test file:\n",
      "\n",
      "```\n",
      "from.. import(,,,,) from. import =(=0.5) =(=0.5) =(=0.5) =(=0.5) =(=0.5)\n",
      "```\n",
      "URL: https://www.confident-ai.com/blog/how-to-evaluate-rag-applications-in-ci-cd-pipelines-with-deepeval\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "question = \"RAGAS와 DeepEval의 차이점은 무엇인가요?\"\n",
    "\n",
    "# RunnableConfig에 콜백 추가\n",
    "# 참고: CallbackHandler를 config에 전달하면 자동으로 LLM 호출, Tool 사용이 추적됩니다.\n",
    "config = RunnableConfig(\n",
    "    callbacks=[langfuse_handler],\n",
    "    run_name=\"agent-execution\",  # LangFuse에서 표시될 실행 이름\n",
    "    metadata={\n",
    "        \"session\": \"day5-session3\",\n",
    "        \"user\": \"sds-superman\",\n",
    "        \"environment\": \"jupyter\",\n",
    "        \"trace_name\": \"sds-day5-class-20251114-1\",\n",
    "    },\n",
    ")\n",
    "\n",
    "system_prompt = \"\"\"주어진 도구를 무조건 활용해서 사용자가 입력한 질문에 대해 최대한 전문적이고 친절하게 답변해주세요.\"\"\"\n",
    "\n",
    "# TavilySearch 도구 정의\n",
    "tavily_search = TavilySearch(max_result=5, topic=\"general\", search_depth=\"advanced\")\n",
    "\n",
    "# 에이전트 생성\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[tavily_search],\n",
    "    system_prompt=system_prompt,\n",
    "    name=\"langfuse-sample-agent\",\n",
    ")\n",
    "\n",
    "# 그래프 실행\n",
    "result_with_langfuse = agent.invoke(\n",
    "    input={\"messages\": [HumanMessage(content=question)]},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "import json\n",
    "\n",
    "# 도구 사용 컨텍스트 추출\n",
    "# ToolMessage.content는 JSON 문자열이므로 파싱 필요\n",
    "content_str = filter_messages(result_with_langfuse[\"messages\"], include_types=[ToolMessage])[\n",
    "    0\n",
    "].content\n",
    "\n",
    "# JSON 파싱\n",
    "content_dict = json.loads(content_str)\n",
    "\n",
    "# results 배열에서 각 항목을 문자열로 변환\n",
    "results = content_dict.get(\"results\", [])\n",
    "context = [\n",
    "    f\"Title: {r.get('title', 'N/A')}\\nContent: {r.get('content', 'N/A')}\\nURL: {r.get('url', 'N/A')}\"\n",
    "    for r in results\n",
    "]\n",
    "\n",
    "# 결과 추출\n",
    "answer = result_with_langfuse[\"messages\"][-1].content\n",
    "\n",
    "print(\"=== Answer ===\")\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"=== Search Contexts===\")\n",
    "for i, ctx in enumerate(context, 1):\n",
    "    print(f\"\\n[Context {i}]\")\n",
    "    print(ctx)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAGAS 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2403c90a6642cdbbffc1e02b510aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a9423c0d154a39a1cb632e47b3d175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b49437c8b65f49a481a5c6b5346102c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RAGAS 평가 결과 ===\n",
      "- Faithfulness: 0.938\n",
      "- Answer Relevancy: 0.749\n",
      "- Passed: True\n",
      "\n",
      "=== 평가 점수 요약 ===\n",
      "  faithfulness: 0.9375\n",
      "  answer_relevancy: 0.7486281306894892\n",
      "  passed: True\n"
     ]
    }
   ],
   "source": [
    "def evaluate_with_ragas(\n",
    "    question: str, answer: str, contexts: list[str], ground_truth: str = None\n",
    ") -> dict:\n",
    "    \"\"\"RAGAS 평가\n",
    "\n",
    "    Args:\n",
    "        question: 사용자 질문\n",
    "        answer: LLM 생성 답변\n",
    "        contexts: 검색된 컨텍스트 리스트\n",
    "        ground_truth: 정답 (선택적, context_precision/context_recall 계산에 필요)\n",
    "\n",
    "    Returns:\n",
    "        RAGAS 평가 점수 딕셔너리\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # RAGAS import\n",
    "        from ragas import EvaluationDataset, SingleTurnSample, evaluate\n",
    "        from ragas.metrics import answer_relevancy, context_precision, context_recall, faithfulness\n",
    "\n",
    "        # SingleTurnSample 생성\n",
    "        sample_kwargs = {\n",
    "            \"user_input\": question,\n",
    "            \"response\": answer,\n",
    "            \"retrieved_contexts\": contexts,\n",
    "        }\n",
    "\n",
    "        # ground_truth가 있으면 reference 추가\n",
    "        if ground_truth:\n",
    "            sample_kwargs[\"reference\"] = ground_truth\n",
    "\n",
    "        sample = SingleTurnSample(**sample_kwargs)\n",
    "\n",
    "        # EvaluationDataset 생성\n",
    "        dataset = EvaluationDataset(samples=[sample])\n",
    "\n",
    "        # 메트릭 선택 (ground_truth 유무에 따라)\n",
    "        if ground_truth:\n",
    "            metrics = [faithfulness, answer_relevancy, context_precision, context_recall]\n",
    "        else:\n",
    "            # ground_truth가 없으면 faithfulness와 answer_relevancy만 사용\n",
    "            metrics = [faithfulness, answer_relevancy]\n",
    "\n",
    "        # RAGAS 평가 실행\n",
    "        result = evaluate(\n",
    "            llm=llm,\n",
    "            embeddings=embeddings,\n",
    "            dataset=dataset,\n",
    "            metrics=metrics,\n",
    "        )\n",
    "\n",
    "        # 결과 추출\n",
    "        scores_df = result.to_pandas()\n",
    "        scores = scores_df.iloc[0]\n",
    "\n",
    "        faithfulness_score = float(scores.get(\"faithfulness\", 0.0))\n",
    "        relevancy_score = float(scores.get(\"answer_relevancy\", 0.0))\n",
    "\n",
    "        result_dict = {\n",
    "            \"faithfulness\": faithfulness_score,\n",
    "            \"answer_relevancy\": relevancy_score,\n",
    "        }\n",
    "\n",
    "        # ground_truth가 있으면 추가 메트릭 포함\n",
    "        if ground_truth:\n",
    "            precision_score = float(scores.get(\"context_precision\", 0.0))\n",
    "            recall_score = float(scores.get(\"context_recall\", 0.0))\n",
    "            result_dict[\"context_precision\"] = precision_score\n",
    "            result_dict[\"context_recall\"] = recall_score\n",
    "            passed = (\n",
    "                faithfulness_score >= 0.7\n",
    "                and relevancy_score >= 0.7\n",
    "                and precision_score >= 0.7\n",
    "                and recall_score >= 0.7\n",
    "            )\n",
    "        else:\n",
    "            # ground_truth 없이는 faithfulness와 relevancy만 체크\n",
    "            passed = faithfulness_score >= 0.7 and relevancy_score >= 0.7\n",
    "\n",
    "        result_dict[\"passed\"] = passed\n",
    "\n",
    "        print(\"=== RAGAS 평가 결과 ===\")\n",
    "        print(f\"- Faithfulness: {faithfulness_score:.3f}\")\n",
    "        print(f\"- Answer Relevancy: {relevancy_score:.3f}\")\n",
    "        if ground_truth:\n",
    "            print(f\"- Context Precision: {precision_score:.3f}\")\n",
    "            print(f\"- Context Recall: {recall_score:.3f}\")\n",
    "        print(f\"- Passed: {passed}\")\n",
    "\n",
    "        return result_dict\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"RAGAS 평가 실패: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            \"faithfulness\": 0.0,\n",
    "            \"answer_relevancy\": 0.0,\n",
    "            \"passed\": False,\n",
    "            \"error\": str(e),\n",
    "        }\n",
    "\n",
    "\n",
    "# Ground truth 없이 평가\n",
    "ragas_scores = evaluate_with_ragas(question, answer, context)\n",
    "\n",
    "print(\"\\n=== 평가 점수 요약 ===\")\n",
    "for metric, value in ragas_scores.items():\n",
    "    if metric != \"error\":\n",
    "        print(f\"  {metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langfuse에 점수 기록\n",
    "\n",
    "LangFuse `CallbackHandler`에 직접 `score()` 할 수 있는 메서드가 없습니다.\n",
    "대신 별도의 `Langfuse` 클라이언트를 생성하고, `CallbackHandler.last_trace_id`를 사용하여\n",
    "트레이스 ID를 가져온 후 `langfuse_client.score()`로 점수를 기록합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트레이스 ID: 00000000000000000000000000000000\n",
      "\n",
      "  ✓ faithfulness: 0.938 (NUMERIC)\n",
      "  ✓ answer_relevancy: 0.749 (NUMERIC)\n",
      "  ✓ passed: 1.000 (NUMERIC)\n"
     ]
    }
   ],
   "source": [
    "from langfuse import Langfuse\n",
    "\n",
    "langfuse_client = Langfuse(\n",
    "    public_key=os.getenv(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "    secret_key=os.getenv(\"LANGFUSE_SECRET_KEY\"),\n",
    "    host=os.getenv(\"LANGFUSE_HOST\", \"http://localhost:3000\"),\n",
    ")\n",
    "\n",
    "# CallbackHandler에서 생성된 트레이스 ID 가져오기\n",
    "# 참고: last_trace_id는 그래프 실행 후 자동으로 설정됩니다.\n",
    "trace_id = langfuse_handler.last_trace_id\n",
    "\n",
    "if trace_id:\n",
    "    print(f\"트레이스 ID: {trace_id}\\n\")\n",
    "\n",
    "    # 각 스코어를 트레이스에 추가\n",
    "    for metric, value in ragas_scores.items():\n",
    "        if metric == \"error\":\n",
    "            continue  # 에러는 스킵\n",
    "\n",
    "        # 숫자 값 처리\n",
    "        if isinstance(value, (int, float)):\n",
    "            langfuse_client.create_score(\n",
    "                trace_id=trace_id,\n",
    "                name=metric,\n",
    "                value=float(value),\n",
    "                data_type=\"NUMERIC\",  # data_type 필수\n",
    "                comment=f\"{metric} 평가 점수\",\n",
    "            )\n",
    "            print(f\"  ✓ {metric}: {value:.3f} (NUMERIC)\")\n",
    "\n",
    "        # Boolean 값 처리\n",
    "        elif isinstance(value, bool):\n",
    "            # Boolean은 0 또는 1로 변환\n",
    "            langfuse_client.create_score(\n",
    "                trace_id=trace_id,\n",
    "                name=metric,\n",
    "                value=1.0 if value else 0.0,\n",
    "                data_type=\"BOOLEAN\",  # data_type 명시\n",
    "                comment=f\"{metric} 평가 점수\",\n",
    "            )\n",
    "            print(f\"  ✓ {metric}: {value} → {1.0 if value else 0.0} (BOOLEAN)\")\n",
    "\n",
    "    # 데이터 flush (서버에 전송)\n",
    "    langfuse_client.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
