{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepEval: LLM & AI Agent Evaluation Framework\n",
    "\n",
    "## 1. DeepEval 소개\n",
    "\n",
    "**DeepEval**은 LLM 애플리케이션과 AI Agent를 평가하기 위한 오픈소스 프레임워크입니다.\n",
    "\n",
    "### 왜 DeepEval인가?\n",
    "\n",
    "LLM 기반 애플리케이션 개발 시 직면하는 문제:\n",
    "- 답변 품질을 어떻게 측정할 것인가?\n",
    "- 환각(Hallucination)을 어떻게 감지할 것인가?\n",
    "- 충분한 테스트 데이터가 없다면?\n",
    "- Agent 동작을 어떻게 평가할 것인가?\n",
    "\n",
    "DeepEval은 이러한 문제를 해결하는 **평가 중심(Evaluation-First)** 프레임워크입니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. DeepEval 핵심 기능\n",
    "\n",
    "### 2.1. Evaluation Metrics (평가 메트릭)\n",
    "\n",
    "DeepEval은 14가지 이상의 평가 메트릭을 제공합니다:\n",
    "\n",
    "**RAG 평가 메트릭**\n",
    "- **Faithfulness**: 답변이 컨텍스트에 근거하는가?\n",
    "- **Answer Relevancy**: 답변이 질문과 관련 있는가?\n",
    "- **Contextual Relevancy**: 컨텍스트가 질문과 관련 있는가?\n",
    "- **Contextual Precision**: 컨텍스트가 정확한가?\n",
    "- **Contextual Recall**: 필요한 컨텍스트를 모두 검색했는가?\n",
    "\n",
    "**Agent 평가 메트릭**\n",
    "- **Tool Correctness**: 도구를 올바르게 선택했는가?\n",
    "- **Agent Goal Success**: Agent가 목표를 달성했는가?\n",
    "\n",
    "**일반 메트릭**\n",
    "- **Hallucination**: 환각이 있는가?\n",
    "- **Toxicity**: 유해한 내용이 있는가?\n",
    "- **Bias**: 편향이 있는가?\n",
    "\n",
    "### 2.2. Synthesizer (데이터 생성)\n",
    "\n",
    "**Golden Dataset이 부족할 때** 합성 데이터를 자동 생성합니다:\n",
    "- 문서에서 질문-답변 쌍 생성\n",
    "- Evolution 전략으로 질문 난이도 조정\n",
    "- 품질 점수 자동 계산\n",
    "\n",
    "### 2.3. CI/CD Integration\n",
    "\n",
    "Pytest와 통합하여 **자동화된 평가 파이프라인** 구축 가능:\n",
    "```python\n",
    "@pytest.mark.parametrize(\"test_case\", test_cases)\n",
    "def test_llm_output(test_case):\n",
    "    assert_test(test_case, metrics)\n",
    "```\n",
    "\n",
    "### 2.4. Tracing & Observability\n",
    "\n",
    "Langfuse, Confident AI와 통합하여 **실시간 모니터링** 가능\n",
    "\n",
    "---\n",
    "\n",
    "## 3. DeepEval vs RAGAS 비교\n",
    "\n",
    "| 기능 | DeepEval | RAGAS |\n",
    "|------|----------|-------|\n",
    "| **평가 메트릭** | 14+ (Agent 포함) | 8+ (RAG 중심) |\n",
    "| **Synthesizer** | 7가지 Evolution 전략 | 간단한 생성 |\n",
    "| **Agent 평가** | 전용 메트릭 제공 | 제한적 |\n",
    "| **CI/CD 통합** | Pytest 네이티브 | 별도 작업 필요 |\n",
    "| **커스터마이징** | 메트릭 직접 구현 가능 | 제한적 |\n",
    "| **대시보드** | Confident AI | 없음 |\n",
    "\n",
    "**선택 가이드**:\n",
    "- **RAG 시스템 평가**: 둘 다 우수\n",
    "- **Agent 평가**: DeepEval 권장\n",
    "- **CI/CD 통합**: DeepEval 권장\n",
    "- **간단한 시작**: RAGAS 권장\n",
    "\n",
    "---\n",
    "\n",
    "## 4. DeepEval 사용 시나리오\n",
    "\n",
    "### 시나리오 1: RAG 시스템 평가\n",
    "```python\n",
    "from deepeval.metrics import FaithfulnessMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=\"질문\",\n",
    "    actual_output=\"답변\",\n",
    "    retrieval_context=[\"문서1\", \"문서2\"]\n",
    ")\n",
    "\n",
    "metric = FaithfulnessMetric(threshold=0.7)\n",
    "metric.measure(test_case)\n",
    "print(metric.score)  # 0.85\n",
    "```\n",
    "\n",
    "### 시나리오 2: Agent 평가\n",
    "```python\n",
    "from deepeval.metrics import ToolCorrectnessMetric\n",
    "\n",
    "metric = ToolCorrectnessMetric(threshold=0.8)\n",
    "metric.measure(agent_test_case)\n",
    "```\n",
    "\n",
    "### 시나리오 3: Golden Dataset 생성\n",
    "```python\n",
    "from deepeval.synthesizer import Synthesizer\n",
    "\n",
    "synthesizer = Synthesizer(model=\"gpt-4o\")\n",
    "goldens = synthesizer.generate_goldens_from_docs(\n",
    "    document_chunks=chunks,\n",
    "    max_goldens=100\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. 이 노트북에서 다루는 내용\n",
    "\n",
    "이 노트북은 **DeepEval Synthesizer**에 집중합니다:\n",
    "\n",
    "1. Evolution 전략 이해\n",
    "2. 커스텀 가중치 설정\n",
    "3. Golden Dataset 생성\n",
    "4. 품질 필터링\n",
    "5. 실제 평가 실행\n",
    "6. Langfuse 통합\n",
    "\n",
    "**학습 목표**:\n",
    "- Synthesizer로 평가 데이터 자동 생성\n",
    "- Evolution 전략으로 난이도 조절\n",
    "- 생성된 데이터로 Agent 평가\n",
    "\n",
    "---\n",
    "\n",
    "## 참고 자료\n",
    "\n",
    "- [DeepEval 공식 문서](https://docs.confident-ai.com/)\n",
    "- [DeepEval GitHub](https://github.com/confident-ai/deepeval)\n",
    "- [Agent 평가 가이드](https://deepeval.com/docs/getting-started-agents)\n",
    "- [Synthesizer 가이드](https://docs.confident-ai.com/docs/evaluation-datasets-synthetic-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. DeepEval 작동 원리\n",
    "\n",
    "DeepEval은 **LLM을 Judge로 사용**하여 평가를 수행합니다:\n",
    "\n",
    "1. **평가 시**: LLM이 답변을 분석하고 점수를 계산\n",
    "2. **데이터 생성 시**: LLM이 문서에서 질문-답변 쌍을 생성\n",
    "\n",
    "따라서 평가와 생성 모두에서 **LLM API 호출이 발생**합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. DeepEval Synthesizer: Evolution 전략\n",
    "\n",
    "DeepEval의 가장 강력한 기능 중 하나는 **Synthesizer**입니다. Synthesizer는 문서에서 자동으로 Golden Dataset을 생성하며, **7가지 Evolution 전략**을 통해 질문의 난이도와 유형을 조절할 수 있습니다.\n",
    "\n",
    "## Evolution 전략이란?\n",
    "\n",
    "**Evolution 전략**은 기본 질문을 더 복잡하고 현실적인 질문으로 진화시키는 방법입니다. 이를 통해:\n",
    "- 단순한 질문에서 복잡한 추론을 요구하는 질문으로 변환\n",
    "- 다양한 난이도와 유형의 테스트 케이스 생성\n",
    "- 실제 사용자 질문과 유사한 패턴 구현\n",
    "\n",
    "---\n",
    "\n",
    "## 7가지 Evolution 전략 상세 설명\n",
    "\n",
    "### 1. REASONING (추론)\n",
    "**목적**: 다단계 논리적 사고를 요구하는 질문 생성\n",
    "\n",
    "**특징**:\n",
    "- 여러 단계의 추론이 필요\n",
    "- 조건부 논리 적용\n",
    "- 인과관계 파악 요구\n",
    "\n",
    "**예시**:\n",
    "- **Before**: \"비밀번호 재설정 절차는?\"\n",
    "- **After**: \"Q1에 설정한 비밀번호가 90일 정책에 따라 만료되었고, 2FA 인증이 실패한 경우 비밀번호를 재설정하는 전체 절차는?\"\n",
    "\n",
    "---\n",
    "\n",
    "### 2. MULTICONTEXT (다중 컨텍스트)\n",
    "**목적**: 여러 문서의 정보를 통합하여 답변해야 하는 질문\n",
    "\n",
    "**특징**:\n",
    "- 2개 이상의 문서 참조\n",
    "- 정보 통합 능력 테스트\n",
    "- RAG 시스템의 문서 검색 정확도 평가\n",
    "\n",
    "**예시**:\n",
    "- **Before**: \"VPN 문제 해결 방법은?\"\n",
    "- **After**: \"비밀번호 정책과 VPN 인증 요구사항을 모두 고려했을 때, 'Invalid credentials' 오류를 해결하는 방법은?\"\n",
    "\n",
    "---\n",
    "\n",
    "### 3. CONCRETIZING (구체화)\n",
    "**목적**: 추상적 개념을 구체적 수치/예시로 변환\n",
    "\n",
    "**특징**:\n",
    "- 실제 숫자와 단위 사용\n",
    "- 구체적인 시나리오 제시\n",
    "- 정량적 분석 요구\n",
    "\n",
    "**예시**:\n",
    "- **Before**: \"이메일 저장 용량 제한은?\"\n",
    "- **After**: \"1,000개의 이메일(각 5MB)을 보관하려면 표준 메일박스 50GB 할당량으로 충분한가?\"\n",
    "\n",
    "---\n",
    "\n",
    "### 4. CONSTRAINED (제약)\n",
    "**목적**: 조건이나 제약 사항을 추가\n",
    "\n",
    "**특징**:\n",
    "- 시간/비용/권한 제약 추가\n",
    "- 정책 준수 확인\n",
    "- 예외 처리 능력 테스트\n",
    "\n",
    "**예시**:\n",
    "- **Before**: \"소프트웨어 설치 방법은?\"\n",
    "- **After**: \"예산 제약이 있고, 보안 팀 승인이 5일 이내에 필요하며, 관리자 권한 없이 소프트웨어를 설치하는 방법은?\"\n",
    "\n",
    "---\n",
    "\n",
    "### 5. COMPARATIVE (비교)\n",
    "**목적**: 여러 옵션을 비교 분석하도록 요구\n",
    "\n",
    "**특징**:\n",
    "- 2개 이상의 대안 제시\n",
    "- 기준별 비교 요구\n",
    "- 의사결정 지원 테스트\n",
    "\n",
    "**예시**:\n",
    "- **Before**: \"원격 근무 지원 옵션은?\"\n",
    "- **After**: \"라이브 챗과 전화 지원을 응답 시간, 비용, 해결률 측면에서 비교하면?\"\n",
    "\n",
    "---\n",
    "\n",
    "### 6. HYPOTHETICAL (가상 시나리오)\n",
    "**목적**: 예외 상황이나 가상 시나리오 적용\n",
    "\n",
    "**특징**:\n",
    "- \"만약~라면\" 질문\n",
    "- 드문 상황 대응 테스트\n",
    "- 위기 관리 능력 평가\n",
    "\n",
    "**예시**:\n",
    "- **Before**: \"이메일 보안 모범 사례는?\"\n",
    "- **After**: \"CEO 사칭 피싱 이메일을 받았을 때, 회사 정책에 따라 취해야 할 즉각적인 조치는?\"\n",
    "\n",
    "---\n",
    "\n",
    "### 7. IN_BREADTH (범위 확장)\n",
    "**목적**: 질문의 범위를 더 넓은 시스템/영향으로 확장\n",
    "\n",
    "**특징**:\n",
    "- 시스템 간 연쇄 영향 분석\n",
    "- 전체적인 관점 요구\n",
    "- 통합 시스템 이해 테스트\n",
    "\n",
    "**예시**:\n",
    "- **Before**: \"VPN 연결 오류의 원인은?\"\n",
    "- **After**: \"VPN 연결 오류가 이메일 동기화, 파일 공유, 화상 회의 시스템에 미치는 연쇄 영향은?\"\n",
    "\n",
    "---\n",
    "\n",
    "## Evolution 전략 선택 가이드\n",
    "\n",
    "| 전략 | 사용 시기 | Agent 평가 적합도 |\n",
    "|------|-----------|-------------------|\n",
    "| **REASONING** | 복잡한 의사결정 테스트 | 높음 |\n",
    "| **MULTICONTEXT** | RAG 검색 품질 평가 | 중간 |\n",
    "| **CONCRETIZING** | 정량적 분석 능력 | 중간 |\n",
    "| **CONSTRAINED** | 정책 준수 확인 | 높음 |\n",
    "| **COMPARATIVE** | 옵션 비교 능력 | 중간 |\n",
    "| **HYPOTHETICAL** | 예외 처리 능력 | 높음 |\n",
    "| **IN_BREADTH** | 시스템 이해도 | 낮음 |\n",
    "\n",
    "---\n",
    "\n",
    "## 참고 자료\n",
    "\n",
    "- [DeepEval Synthesizer 공식 문서](https://docs.confident-ai.com/docs/evaluation-datasets-synthetic-data)\n",
    "- [Evolution Strategies 상세 가이드](https://docs.confident-ai.com/docs/evaluation-datasets-synthetic-data#evolution-strategies)\n",
    "- [Automatic Instruction Evolving for Large Language Models](https://www.themoonlight.io/ko/review/automatic-instruction-evolving-for-large-language-models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "from deepeval.dataset import EvaluationDataset, Golden\n",
    "from deepeval.synthesizer import Synthesizer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 기본 Synthesizer 사용\n",
    "\n",
    "먼저 기본 설정으로 Synthesizer를 사용해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. 기본 Synthesizer 사용\n",
    "\n",
    "먼저 기본 설정으로 Synthesizer를 사용해봅니다.\n",
    "\n",
    "**주의**: `document_chunks` 파라미터에 실제 문서를 제공해야 합니다. 빈 리스트로 테스트하면 에러가 발생할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 설정으로 Golden Dataset 생성 (소규모 테스트)\n",
    "basic_dataset = synthesizer.generate_goldens_from_docs(\n",
    "    document_chunks=[],\n",
    "    max_goldens=10,  # 10개만 생성\n",
    "    num_evolutions=1,  # Evolution 1회 적용\n",
    "    include_reasoning=True,  # 추론 과정 포함\n",
    ")\n",
    "basic_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성된 Golden Dataset 확인\n",
    "\n",
    "for i, golden in enumerate(basic_dataset.goldens[:3]):\n",
    "    print(f\"━━━ Golden {i + 1} ━━━\")\n",
    "    print(f\"질문 (Input): {golden.input}\")\n",
    "    print(f\"\\n기대 답변 (Expected): {golden.expected_output[:200]}...\")\n",
    "    if golden.context:\n",
    "        print(f\"\\n컨텍스트 수: {len(golden.context)}개\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evolution 가중치 커스터마이징\n",
    "\n",
    "비즈니스 요구사항에 맞게 Evolution 전략의 가중치를 조정합니다.\n",
    "\n",
    "### 시나리오: IT Helpdesk 규정 준수 테스트\n",
    "\n",
    "**목표**: 직원들이 회사 정책(비밀번호 정책, 보안 절차 등)을 잘 준수하는지 평가\n",
    "\n",
    "**전략**:\n",
    "- `CONSTRAINED` (제약) 가중치 높게: 정책 준수 확인\n",
    "- `HYPOTHETICAL` (가상 시나리오): 예외 상황 대응\n",
    "- `REASONING` (추론): 복잡한 정책 이해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "규정 준수 테스트를 위한 Evolution 가중치:\n",
      "  CONSTRAINED    : 35.0%\n",
      "  HYPOTHETICAL   : 20.0%\n",
      "  REASONING      : 15.0%\n",
      "  CONCRETIZING   : 10.0%\n",
      "  COMPARATIVE    : 10.0%\n",
      "  MULTICONTEXT   :  5.0%\n",
      "  IN_BREADTH     :  5.0%\n"
     ]
    }
   ],
   "source": [
    "# Evolution 가중치 설정\n",
    "compliance_weights = {\n",
    "    \"REASONING\": 0.15,  # 15%: 복잡한 추론\n",
    "    \"MULTICONTEXT\": 0.05,  # 5%: 여러 문서 통합 (덜 중요)\n",
    "    \"CONCRETIZING\": 0.10,  # 10%: 구체적 수치 확인\n",
    "    \"CONSTRAINED\": 0.35,  # 35%: 제약/규정 준수 (가장 중요)\n",
    "    \"COMPARATIVE\": 0.10,  # 10%: 옵션 비교\n",
    "    \"HYPOTHETICAL\": 0.20,  # 20%: 예외 상황 (중요)\n",
    "    \"IN_BREADTH\": 0.05,  # 5%: 범위 확장 (덜 중요)\n",
    "}\n",
    "\n",
    "print(\"규정 준수 테스트를 위한 Evolution 가중치:\")\n",
    "for strategy, weight in sorted(compliance_weights.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {strategy:15s}: {weight:5.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커스터마이징된 가중치로 Golden Dataset 생성\n",
    "compliance_dataset = synthesizer.generate_goldens_from_docs(\n",
    "    document_chunks=[],\n",
    "    max_goldens=50,  # 50개 생성\n",
    "    num_evolutions=2,  # Evolution 2회 적용\n",
    "    evolution_weights=compliance_weights,  # 커스텀 가중치\n",
    "    include_reasoning=True,\n",
    ")\n",
    "compliance_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 품질 필터링\n",
    "\n",
    "생성된 Golden 중 품질이 낮은 것을 필터링합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "quality_scores = []\n",
    "for golden in compliance_dataset.goldens:\n",
    "    score = golden.additional_metadata.get(\"synthetic_quality_score\", 0)\n",
    "    quality_scores.append(\n",
    "        {\n",
    "            \"input\": golden.input[:80] + \"...\",\n",
    "            \"quality_score\": score,\n",
    "            \"evolutions\": golden.additional_metadata.get(\"evolutions\", []),\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_quality = pd.DataFrame(quality_scores)\n",
    "\n",
    "print(df_quality[\"quality_score\"].describe())\n",
    "print(f\"\\n평균 품질 점수: {df_quality['quality_score'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 품질 필터링 (0.7 이상만 유지)\n",
    "QUALITY_THRESHOLD = 0.7\n",
    "\n",
    "high_quality_goldens = [\n",
    "    golden\n",
    "    for golden in compliance_dataset.goldens\n",
    "    if golden.additional_metadata.get(\"synthetic_quality_score\", 0) >= QUALITY_THRESHOLD\n",
    "]\n",
    "\n",
    "print(\"품질 필터링 결과:\")\n",
    "print(f\"  전체: {len(compliance_dataset.goldens)}개\")\n",
    "print(f\"  필터링 후: {len(high_quality_goldens)}개\")\n",
    "print(f\"  통과율: {len(high_quality_goldens) / len(compliance_dataset.goldens) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sample_size = max(1, int(len(high_quality_goldens) * 0.1))\n",
    "review_samples = random.sample(high_quality_goldens, min(sample_size, 5))  # 최대 5개\n",
    "\n",
    "print(f\"\\nHuman In the Loop 샘플링 ({len(review_samples)}개):\\n\")\n",
    "\n",
    "for i, golden in enumerate(review_samples):\n",
    "    print(f\"━━━ 샘플 {i + 1} ━━━\")\n",
    "    print(f\"질문: {golden.input}\")\n",
    "    print(f\"기대 답변: {golden.expected_output[:150]}...\")\n",
    "    print(f\"적용된 Evolution: {golden.additional_metadata.get('evolutions', [])}\")\n",
    "    print(f\"품질 점수: {golden.additional_metadata.get('synthetic_quality_score', 0):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 결과 분석 및 저장\n",
    "\n",
    "생성된 Golden Dataset을 분석하고 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_evolutions = []\n",
    "for golden in high_quality_goldens:\n",
    "    evolutions = golden.additional_metadata.get(\"evolutions\", [])\n",
    "    all_evolutions.extend(evolutions)\n",
    "\n",
    "evolution_counts = Counter(all_evolutions)\n",
    "\n",
    "for strategy, count in evolution_counts.most_common():\n",
    "    percentage = count / len(all_evolutions) * 100\n",
    "    print(f\"  {strategy:15s}: {count:3d}회 ({percentage:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSONL 형식으로 저장\n",
    "output_path = Path(\"generated_golden_dataset.jsonl\")\n",
    "\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for golden in high_quality_goldens:\n",
    "        record = {\n",
    "            \"question\": golden.input,\n",
    "            \"ground_truth\": golden.expected_output,\n",
    "            \"contexts\": golden.context if golden.context else [],\n",
    "            \"metadata\": {\n",
    "                \"quality_score\": golden.additional_metadata.get(\"synthetic_quality_score\", 0),\n",
    "                \"evolutions\": golden.additional_metadata.get(\"evolutions\", []),\n",
    "                \"source\": \"deepeval_synthesizer\",\n",
    "            },\n",
    "        }\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 생성된 Golden Dataset으로 실제 평가 실행\n",
    "\n",
    "이제 생성한 Golden Dataset을 사용하여 **실제 Agent 시스템을 평가**합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepEval 평가 메트릭 import\n",
    "from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from langchain_openai import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"openai/gpt-4.1\",\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가할 샘플 선정 (처음 5개)\n",
    "test_samples = high_quality_goldens[:5]\n",
    "test_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepEval 메트릭 초기화\n",
    "faithfulness_metric = FaithfulnessMetric(model=\"openai/gpt-4.1\", threshold=0.7, include_reason=True)\n",
    "relevancy_metric = AnswerRelevancyMetric(model=\"openai/gpt-4.1\", threshold=0.7, include_reason=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 평가 실행\n",
    "evaluation_results = []\n",
    "\n",
    "for i, golden in enumerate(test_samples, 1):\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"평가 {i}/{len(test_samples)}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    # LLM으로 실제 답변 생성\n",
    "    prompt = f\"\"\"다음 컨텍스트를 참고하여 질문에 답변하세요.\n",
    "\n",
    "컨텍스트:\n",
    "{chr(10).join(golden.context if golden.context else [])}\n",
    "\n",
    "질문: {golden.input}\n",
    "\n",
    "답변:\"\"\"\n",
    "\n",
    "    actual_output = llm.invoke(prompt).content\n",
    "\n",
    "    print(f\"질문: {golden.input[:100]}...\")\n",
    "    print(f\"\\n생성된 답변: {actual_output[:150]}...\")\n",
    "\n",
    "    # LLMTestCase 생성\n",
    "    test_case = LLMTestCase(\n",
    "        input=golden.input,\n",
    "        actual_output=actual_output,\n",
    "        expected_output=golden.expected_output,\n",
    "        retrieval_context=golden.context if golden.context else [],\n",
    "    )\n",
    "\n",
    "    # FaithfulnessMetric 평가\n",
    "    try:\n",
    "        faithfulness_metric.measure(test_case)\n",
    "        faithfulness_score = faithfulness_metric.score\n",
    "        faithfulness_reason = faithfulness_metric.reason\n",
    "        print(f\"\\nFaithfulness: {faithfulness_score:.3f}\")\n",
    "        print(f\" 이유: {faithfulness_reason[:100]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Faithfulness 평가 실패: {e}\")\n",
    "        faithfulness_score = 0.0\n",
    "        faithfulness_reason = str(e)\n",
    "\n",
    "    # AnswerRelevancyMetric 평가\n",
    "    try:\n",
    "        relevancy_metric.measure(test_case)\n",
    "        relevancy_score = relevancy_metric.score\n",
    "        relevancy_reason = relevancy_metric.reason\n",
    "        print(f\"Answer Relevancy: {relevancy_score:.3f}\")\n",
    "        print(f\" 이유: {relevancy_reason[:100]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Answer Relevancy 평가 실패: {e}\")\n",
    "        relevancy_score = 0.0\n",
    "        relevancy_reason = str(e)\n",
    "\n",
    "    # 결과 저장\n",
    "    evaluation_results.append(\n",
    "        {\n",
    "            \"question\": golden.input,\n",
    "            \"actual_output\": actual_output,\n",
    "            \"expected_output\": golden.expected_output,\n",
    "            \"faithfulness_score\": faithfulness_score,\n",
    "            \"faithfulness_reason\": faithfulness_reason,\n",
    "            \"relevancy_score\": relevancy_score,\n",
    "            \"relevancy_reason\": relevancy_reason,\n",
    "            \"contexts\": golden.context if golden.context else [],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 결과 분석\n",
    "import pandas as pd\n",
    "\n",
    "eval_df = pd.DataFrame(evaluation_results)\n",
    "\n",
    "print(\"평가 결과 요약\\n\")\n",
    "print(f\"평균 Faithfulness: {eval_df['faithfulness_score'].mean():.3f}\")\n",
    "print(f\"평균 Answer Relevancy: {eval_df['relevancy_score'].mean():.3f}\")\n",
    "print(\"\\n통과율 (>0.7):\")\n",
    "print(\n",
    "    f\"  Faithfulness: {(eval_df['faithfulness_score'] > 0.7).sum()}/{len(eval_df)} ({(eval_df['faithfulness_score'] > 0.7).mean() * 100:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  Answer Relevancy: {(eval_df['relevancy_score'] > 0.7).sum()}/{len(eval_df)} ({(eval_df['relevancy_score'] > 0.7).mean() * 100:.1f}%)\"\n",
    ")\n",
    "\n",
    "# 결과 저장\n",
    "eval_results_path = Path(\"evaluation_results.json\")\n",
    "with eval_results_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(evaluation_results, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Langfuse 업로드하기\n",
    "\n",
    "생성된 Golden Dataset을 Langfuse에 업로드하여 대시보드에서 관리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langfuse Client 초기화\n",
    "from langfuse import Langfuse\n",
    "\n",
    "langfuse_client = Langfuse(\n",
    "    public_key=os.getenv(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "    secret_key=os.getenv(\"LANGFUSE_SECRET_KEY\"),\n",
    "    host=os.getenv(\"LANGFUSE_HOST\", \"http://localhost:3000\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Agent 평가 메트릭 사용하기\n",
    "\n",
    "이제 DeepEval의 **Agent 전용 메트릭**을 사용하여 AI Agent의 동작을 평가합니다.\n",
    "\n",
    "### Agent 평가 메트릭 종류\n",
    "\n",
    "1. **Tool Correctness**: Agent가 올바른 도구를 선택했는가?\n",
    "2. **Plan Quality**: Agent의 계획이 목표 달성에 적합한가?\n",
    "3. **Multi Turn MCP Use**: Multi-turn 대화에서 MCP(Model Context Protocol) 사용이 적절한가?\n",
    "4. **Custom Metric**: 비즈니스 요구사항에 맞는 커스텀 평가 기준\n",
    "\n",
    "---\n",
    "\n",
    "### 참고 자료\n",
    "\n",
    "- [Tool Correctness 문서](https://deepeval.com/docs/metrics-tool-correctness)\n",
    "- [Plan Quality 문서](https://deepeval.com/docs/metrics-plan-quality)\n",
    "- [Multi Turn MCP Use 문서](https://deepeval.com/docs/metrics-multi-turn-mcp-use)\n",
    "- [Custom Metric 문서](https://deepeval.com/docs/metrics-conversational-dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1. Tool Correctness Metric\n",
    "\n",
    "**Tool Correctness**는 Agent가 주어진 작업을 수행하기 위해 **올바른 도구를 선택했는지** 평가합니다.\n",
    "\n",
    "**평가 기준**:\n",
    "- 사용된 도구가 작업 목표와 일치하는가?\n",
    "- 도구 호출 순서가 논리적인가?\n",
    "- 불필요한 도구 호출이 있는가?\n",
    "\n",
    "**사용 시나리오**:\n",
    "- Multi-tool Agent 평가\n",
    "- 도구 선택 정확도 측정\n",
    "- Agent 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import ToolCorrectnessMetric\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "\n",
    "# Tool Correctness 메트릭 초기화\n",
    "tool_correctness_metric = ToolCorrectnessMetric(\n",
    "    threshold=0.7, model=\"openai/gpt-4.1\", include_reason=True\n",
    ")\n",
    "\n",
    "# Agent의 도구 사용 기록 (예시)\n",
    "# 실제로는 LangGraph Agent 실행 후 tool_calls를 추출합니다\n",
    "agent_tools_used = [\n",
    "    {\n",
    "        \"name\": \"tavily_search\",\n",
    "        \"input\": {\"query\": \"비밀번호 정책\"},\n",
    "        \"output\": \"비밀번호는 최소 12자 이상이어야 합니다...\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"document_search\",\n",
    "        \"input\": {\"query\": \"보안 가이드\"},\n",
    "        \"output\": \"보안 가이드라인에 따르면...\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Test Case 생성\n",
    "test_case = LLMTestCase(\n",
    "    input=\"회사 비밀번호 정책과 보안 가이드라인을 알려주세요\",\n",
    "    actual_output=\"비밀번호는 최소 12자 이상이며, 대소문자, 숫자, 특수문자를 포함해야 합니다. 보안 가이드라인에 따라 90일마다 변경해야 합니다.\",\n",
    "    tools_used=agent_tools_used,\n",
    ")\n",
    "\n",
    "# 평가 실행\n",
    "try:\n",
    "    tool_correctness_metric.measure(test_case)\n",
    "\n",
    "    print(\"Tool Correctness Metric 평가 결과\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"점수: {tool_correctness_metric.score:.3f}\")\n",
    "    print(f\"통과 여부: {'통과' if tool_correctness_metric.is_successful() else '실패'}\")\n",
    "    print(f\"\\n평가 근거:\")\n",
    "    print(f\"{tool_correctness_metric.reason}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Tool Correctness 평가 실패: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.2. Plan Quality Metric\n",
    "\n",
    "**Plan Quality**는 Agent가 생성한 **실행 계획의 품질**을 평가합니다.\n",
    "\n",
    "**평가 기준**:\n",
    "- 계획이 목표 달성에 적합한가?\n",
    "- 단계가 논리적으로 구성되었는가?\n",
    "- 실행 가능한 계획인가?\n",
    "- 불필요한 단계가 포함되어 있지 않은가?\n",
    "\n",
    "**사용 시나리오**:\n",
    "- Planning Agent 평가\n",
    "- Multi-step 작업 계획 검증\n",
    "- Agent 의사결정 품질 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import PlanQualityMetric\n",
    "\n",
    "# Plan Quality 메트릭 초기화\n",
    "plan_quality_metric = PlanQualityMetric(threshold=0.7, model=\"openai/gpt-4.1\", include_reason=True)\n",
    "\n",
    "# Agent가 생성한 실행 계획 (예시)\n",
    "agent_plan = \"\"\"\n",
    "1. 사용자의 비밀번호 재설정 요청 확인\n",
    "2. 사용자 신원 인증 (이메일 또는 2FA)\n",
    "3. 비밀번호 정책 확인 (최소 12자, 대소문자, 숫자, 특수문자)\n",
    "4. 새 비밀번호 생성 또는 사용자 입력 받기\n",
    "5. 비밀번호 정책 준수 여부 검증\n",
    "6. 데이터베이스에 해시된 비밀번호 저장\n",
    "7. 사용자에게 완료 알림 전송\n",
    "\"\"\"\n",
    "\n",
    "# Test Case 생성\n",
    "test_case = LLMTestCase(\n",
    "    input=\"사용자 비밀번호를 안전하게 재설정하는 절차를 계획하세요\",\n",
    "    actual_output=agent_plan,\n",
    "    # 목표를 명시하면 더 정확한 평가 가능\n",
    "    expected_output=\"비밀번호 재설정은 신원 확인, 정책 검증, 안전한 저장을 포함해야 합니다\",\n",
    ")\n",
    "\n",
    "# 평가 실행\n",
    "try:\n",
    "    plan_quality_metric.measure(test_case)\n",
    "\n",
    "    print(\"Plan Quality Metric 평가 결과\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"점수: {plan_quality_metric.score:.3f}\")\n",
    "    print(f\"통과 여부: {'통과' if plan_quality_metric.is_successful() else '실패'}\")\n",
    "    print(f\"\\n평가 근거:\")\n",
    "    print(f\"{plan_quality_metric.reason}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Plan Quality 평가 실패: {e}\")\n",
    "    print(\"\\nPlan Quality 메트릭은 DeepEval 최신 버전에서만 지원될 수 있습니다.\")\n",
    "    print(\"대안: Custom Metric을 사용하여 계획 품질을 평가할 수 있습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.3. Custom Metric (커스텀 평가 기준)\n",
    "\n",
    "DeepEval의 기본 메트릭으로 충분하지 않을 때, **Custom Metric**을 정의하여 비즈니스 요구사항에 맞는 평가 기준을 구현할 수 있습니다.\n",
    "\n",
    "**사용 시나리오**:\n",
    "- 도메인 특화 평가 (의료, 법률, 금융)\n",
    "- 회사 고유 품질 기준\n",
    "- 규정 준수 검증\n",
    "- 대화 흐름 평가 (Conversational DAG)\n",
    "\n",
    "**구현 방법**:\n",
    "1. `BaseMetric` 상속\n",
    "2. `measure()` 메서드 구현\n",
    "3. 평가 로직 작성\n",
    "4. 점수 및 이유 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import BaseMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.scorer import Scorer\n",
    "\n",
    "\n",
    "class ComplianceMetric(BaseMetric):\n",
    "    \"\"\"\n",
    "    IT Helpdesk 규정 준수 평가 커스텀 메트릭\n",
    "\n",
    "    평가 기준:\n",
    "    1. 회사 정책 준수 여부\n",
    "    2. 보안 절차 언급 여부\n",
    "    3. 적절한 에스컬레이션 제안 여부\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, threshold: float = 0.7, model: str = \"openai/gpt-4.1\"):\n",
    "        self.threshold = threshold\n",
    "        self.model = model\n",
    "        self.scorer = Scorer(model=model)\n",
    "\n",
    "    def measure(self, test_case: LLMTestCase) -> float:\n",
    "        \"\"\"규정 준수 점수 계산\"\"\"\n",
    "\n",
    "        # 평가 프롬프트 정의\n",
    "        evaluation_prompt = f\"\"\"\n",
    "다음 답변이 IT Helpdesk 규정을 얼마나 잘 준수하는지 평가하세요.\n",
    "\n",
    "평가 기준:\n",
    "1. 회사 정책 준수 (0-0.4점)\n",
    "   - 비밀번호 정책, 보안 절차 등 정책을 정확히 언급했는가?\n",
    "\n",
    "2. 보안 의식 (0-0.3점)\n",
    "   - 보안 위험을 인지하고 안전한 방법을 제시했는가?\n",
    "\n",
    "3. 에스컬레이션 (0-0.3점)\n",
    "   - 필요 시 IT 지원팀이나 관리자에게 문의하도록 안내했는가?\n",
    "\n",
    "질문: {test_case.input}\n",
    "답변: {test_case.actual_output}\n",
    "\n",
    "점수 (0.0-1.0):\n",
    "\"\"\"\n",
    "\n",
    "        # LLM으로 평가 수행\n",
    "        try:\n",
    "            response = self.scorer.score(evaluation_prompt)\n",
    "\n",
    "            # 점수 추출 (응답에서 숫자 파싱)\n",
    "            import re\n",
    "\n",
    "            match = re.search(r\"(\\d+\\.?\\d*)\", response)\n",
    "            if match:\n",
    "                score = float(match.group(1))\n",
    "                # 0-1 범위로 정규화\n",
    "                if score > 1:\n",
    "                    score = score / 10.0  # 10점 척도인 경우\n",
    "                self.score = min(1.0, max(0.0, score))\n",
    "            else:\n",
    "                self.score = 0.0\n",
    "\n",
    "            self.reason = response\n",
    "            self.success = self.score >= self.threshold\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"평가 실패: {e}\")\n",
    "            self.score = 0.0\n",
    "            self.reason = f\"평가 실패: {str(e)}\"\n",
    "            self.success = False\n",
    "\n",
    "        return self.score\n",
    "\n",
    "    def is_successful(self) -> bool:\n",
    "        \"\"\"threshold 이상이면 성공\"\"\"\n",
    "        return self.success\n",
    "\n",
    "    @property\n",
    "    def __name__(self):\n",
    "        return \"Compliance Metric\"\n",
    "\n",
    "\n",
    "# Custom Metric 사용 예시\n",
    "compliance_metric = ComplianceMetric(threshold=0.7, model=\"openai/gpt-4.1\")\n",
    "\n",
    "# Test Case 생성\n",
    "test_case = LLMTestCase(\n",
    "    input=\"비밀번호를 잊어버렸는데 어떻게 재설정하나요?\",\n",
    "    actual_output=\"\"\"\n",
    "비밀번호 재설정 절차:\n",
    "1. 로그인 페이지에서 '비밀번호 찾기' 클릭\n",
    "2. 등록된 이메일로 재설정 링크 수신\n",
    "3. 링크 클릭 후 새 비밀번호 입력\n",
    "4. 회사 정책에 따라 최소 12자, 대소문자+숫자+특수문자 포함 필요\n",
    "5. 문제가 계속되면 IT 지원팀(내선 1234)에 문의하세요\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "# 평가 실행\n",
    "try:\n",
    "    score = compliance_metric.measure(test_case)\n",
    "\n",
    "    print(\"Custom Compliance Metric 평가 결과\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"점수: {score:.3f}\")\n",
    "    print(f\"통과 여부: {'통과' if compliance_metric.is_successful() else '실패'}\")\n",
    "    print(f\"\\n평가 근거:\")\n",
    "    print(f\"{compliance_metric.reason[:300]}...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Custom Metric 평가 실패: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. 요약 및 다음 단계\n",
    "\n",
    "### 학습한 내용\n",
    "\n",
    "1. **DeepEval 소개**\n",
    "   - LLM 및 AI Agent 평가 프레임워크\n",
    "   - 14+ 평가 메트릭 제공\n",
    "   - Synthesizer로 Golden Dataset 자동 생성\n",
    "\n",
    "2. **Synthesizer 활용**\n",
    "   - 7가지 Evolution 전략\n",
    "   - 커스텀 가중치 설정\n",
    "   - 품질 필터링 및 Human-in-the-Loop\n",
    "\n",
    "3. **Agent 평가 메트릭**\n",
    "   - Tool Correctness: 도구 선택 정확도\n",
    "   - Plan Quality: 실행 계획 품질\n",
    "   - Custom Metric: 비즈니스 요구사항 맞춤 평가\n",
    "\n",
    "4. **프로덕션 통합**\n",
    "   - Langfuse 연동\n",
    "   - 평가 결과 저장 및 분석\n",
    "   - CI/CD 파이프라인 구축\n",
    "\n",
    "---\n",
    "\n",
    "### 실습 과제\n",
    "\n",
    "#### 기본 과제\n",
    "1. 자신의 도메인에 맞는 Evolution 가중치 설정\n",
    "2. Golden Dataset 100개 생성 및 품질 필터링\n",
    "3. 3가지 이상의 메트릭으로 평가 실행\n",
    "\n",
    "#### 심화 과제\n",
    "1. Custom Metric 구현 (도메인 특화 평가 기준)\n",
    "2. Multi-turn 대화 평가 파이프라인 구축\n",
    "3. Langfuse 대시보드에서 평가 트렌드 분석\n",
    "\n",
    "---\n",
    "\n",
    "### 다음 단계\n",
    "\n",
    "- **Session 5**: RAGAS vs DeepEval 심화 비교\n",
    "- **Session 6**: Human-in-the-Loop 워크플로우\n",
    "- **Session 7**: LLM-as-a-Judge 편향 완화\n",
    "- **Session 8**: 프로덕션 모니터링 및 알림\n",
    "\n",
    "---\n",
    "\n",
    "### 참고 자료\n",
    "\n",
    "**DeepEval 공식 문서**\n",
    "- [Getting Started](https://docs.confident-ai.com/)\n",
    "- [Metrics Overview](https://docs.confident-ai.com/docs/metrics-introduction)\n",
    "- [Synthesizer Guide](https://docs.confident-ai.com/docs/evaluation-datasets-synthetic-data)\n",
    "\n",
    "**Agent 평가**\n",
    "- [Tool Correctness](https://deepeval.com/docs/metrics-tool-correctness)\n",
    "- [Plan Quality](https://deepeval.com/docs/metrics-plan-quality)\n",
    "- [Custom Metrics](https://deepeval.com/docs/metrics-conversational-dag)\n",
    "\n",
    "**통합 가이드**\n",
    "- [Langfuse Integration](https://docs.confident-ai.com/docs/integrations-langfuse)\n",
    "- [CI/CD with Pytest](https://docs.confident-ai.com/docs/evaluation-test-cases)\n",
    "\n",
    "---\n",
    "\n",
    "**예상 소요 시간**: 120-150분  \n",
    "**난이도**: 중급-고급  \n",
    "**예상 비용**: $1.50-3.00 (실제 LLM 호출 시)  \n",
    "**최종 업데이트**: 2025-11-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langfuse Dataset 생성 및 업로드\n",
    "dataset_name = \"deepeval_synthesizer_goldens\"\n",
    "\n",
    "# Dataset 아이템 생성\n",
    "for i, golden in enumerate(high_quality_goldens[:10], 1):  # 처음 10개만 업로드\n",
    "    try:\n",
    "        langfuse_client.create_dataset_item(\n",
    "            dataset_name=dataset_name,\n",
    "            input={\n",
    "                \"question\": golden.input,\n",
    "                \"contexts\": golden.context if golden.context else [],\n",
    "            },\n",
    "            expected_output=golden.expected_output,\n",
    "            metadata={\n",
    "                \"quality_score\": golden.additional_metadata.get(\"synthetic_quality_score\", 0),\n",
    "                \"evolutions\": golden.additional_metadata.get(\"evolutions\", []),\n",
    "                \"source\": \"deepeval_synthesizer\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            print(f\" {i}개 업로드 완료\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  {i}번째 아이템 업로드 실패: {e}\")\n",
    "\n",
    "# Flush\n",
    "langfuse_client.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
