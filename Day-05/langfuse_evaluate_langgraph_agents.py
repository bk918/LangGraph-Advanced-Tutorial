"""# LangGraph ì—ì´ì „íŠ¸ í‰ê°€í•˜ê¸°

ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” [Langfuse](https://langfuse.com)ì™€ [Hugging Face Datasets](https://huggingface.co/datasets)ë¥¼ ì‚¬ìš©í•˜ì—¬ **[LangGraph agents](https://github.com/langchain-ai/langgraph)ì˜ ë‚´ë¶€ ë‹¨ê³„(traces)ë¥¼ ëª¨ë‹ˆí„°ë§**í•˜ê³  **ì„±ëŠ¥ì„ í‰ê°€**í•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤.

ì´ ê°€ì´ë“œëŠ” íŒ€ì´ ì—ì´ì „íŠ¸ë¥¼ ë¹ ë¥´ê³  ì•ˆì •ì ìœ¼ë¡œ í”„ë¡œë•ì…˜ì— ë°°í¬í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” **ì˜¨ë¼ì¸** ë° **ì˜¤í”„ë¼ì¸** í‰ê°€ ë©”íŠ¸ë¦­ì„ ë‹¤ë£¹ë‹ˆë‹¤. í‰ê°€ ì „ëµì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ë ¤ë©´ [ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸](https://langfuse.com/blog/2025-03-04-llm-evaluation-101-best-practices-and-challenges)ë¥¼ í™•ì¸í•˜ì„¸ìš”.

**AI ì—ì´ì „íŠ¸ í‰ê°€ê°€ ì¤‘ìš”í•œ ì´ìœ :**
- ì‘ì—… ì‹¤íŒ¨ ë˜ëŠ” ì°¨ì„ ì˜ ê²°ê³¼ ë°œìƒ ì‹œ ë””ë²„ê¹… ë¬¸ì œ
- ì‹¤ì‹œê°„ìœ¼ë¡œ ë¹„ìš© ë° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- ì§€ì†ì ì¸ í”¼ë“œë°±ì„ í†µí•´ ì‹ ë¢°ì„±ê³¼ ì•ˆì „ì„± í–¥ìƒ
"""

"""## ë‹¨ê³„ 1: í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

Langfuse í´ë¼ìš°ë“œì— ê°€ì…í•˜ê±°ë‚˜ ìì²´ í˜¸ìŠ¤íŒ…í•˜ì—¬ Langfuse API í‚¤ë¥¼ ë°›ìœ¼ì„¸ìš”.
"""

import os

# Get keys for your project from the project settings page: https://cloud.langfuse.com
os.environ["LANGFUSE_PUBLIC_KEY"] = "pk-lf-..."
os.environ["LANGFUSE_SECRET_KEY"] = "sk-lf-..."
os.environ["LANGFUSE_BASE_URL"] = "https://us.cloud.langfuse.com" # ğŸ‡ºğŸ‡¸ US region

# Your openai key
os.environ["OPENAI_API_KEY"] = "sk-proj-..."

"""í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ë©´ ì´ì œ Langfuse í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. get_client()ëŠ” í™˜ê²½ ë³€ìˆ˜ì— ì œê³µëœ ìê²© ì¦ëª…ì„ ì‚¬ìš©í•˜ì—¬ Langfuse í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""

from langfuse import get_client

langfuse = get_client()

# Verify connection
if langfuse.auth_check():
    print("Langfuse client is authenticated and ready!")
else:
    print("Authentication failed. Please check your credentials and host.")

"""## ë‹¨ê³„ 2: ê³„ì¸¡ í…ŒìŠ¤íŠ¸

ì—¬ê¸° ê°„ë‹¨í•œ Q&A ì—ì´ì „íŠ¸ê°€ ìˆìŠµë‹ˆë‹¤. ê³„ì¸¡ì´ ì˜¬ë°”ë¥´ê²Œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ì‹¤í–‰í•©ë‹ˆë‹¤. ëª¨ë“  ê²ƒì´ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ë©´ ê´€ì°° ê°€ëŠ¥ì„± ëŒ€ì‹œë³´ë“œì—ì„œ ë¡œê·¸/ìŠ¤íŒ¬ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

from typing import Annotated

from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from typing_extensions import TypedDict


class State(TypedDict):
    # Messages have the type "list". The `add_messages` function in the annotation defines how this state key should be updated
    # (in this case, it appends messages to the list, rather than overwriting them)
    messages: Annotated[list, add_messages]


graph_builder = StateGraph(State)

llm = ChatOpenAI(model="gpt-4o", temperature=0.2)


# The chatbot node function takes the current State as input and returns an updated messages list. This is the basic pattern for all LangGraph node functions.
def chatbot(state: State):
    return {"messages": [llm.invoke(state["messages"])]}


# Add a "chatbot" node. Nodes represent units of work. They are typically regular python functions.
graph_builder.add_node("chatbot", chatbot)

# Add an entry point. This tells our graph where to start its work each time we run it.
graph_builder.set_entry_point("chatbot")

# Set a finish point. This instructs the graph "any time this node is run, you can exit."
graph_builder.set_finish_point("chatbot")

# To be able to run our graph, call "compile()" on the graph builder. This creates a "CompiledGraph" we can use invoke on our state.
graph = graph_builder.compile()

from langfuse.langchain import CallbackHandler

# Initialize Langfuse CallbackHandler for Langchain (tracing)
langfuse_handler = CallbackHandler()

for s in graph.stream(
    {"messages": [HumanMessage(content="What is Langfuse?")]},
    config={"callbacks": [langfuse_handler]},
):
    print(s)

"""Langfuse Traces ëŒ€ì‹œë³´ë“œë¥¼ í™•ì¸í•˜ì—¬ ìŠ¤íŒ¬ê³¼ ë¡œê·¸ê°€ ê¸°ë¡ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.

## ë‹¨ê³„ 3: ë” ë³µì¡í•œ ì—ì´ì „íŠ¸ ê´€ì°° ë° í‰ê°€

ê³„ì¸¡ì´ ì‘ë™í•˜ëŠ” ê²ƒì„ í™•ì¸í–ˆìœ¼ë‹ˆ ì´ì œ ë” ë³µì¡í•œ ì¿¼ë¦¬ë¥¼ ì‹œë„í•˜ì—¬ ê³ ê¸‰ ë©”íŠ¸ë¦­(í† í° ì‚¬ìš©ëŸ‰, ì§€ì—° ì‹œê°„, ë¹„ìš© ë“±)ì´ ì–´ë–»ê²Œ ì¶”ì ë˜ëŠ”ì§€ í™•ì¸í•´ ë´…ì‹œë‹¤.
"""

import os
from typing import Any, TypedDict

from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import END, START, StateGraph


class EmailState(TypedDict):
    email: dict[str, Any]
    is_spam: bool | None
    spam_reason: str | None
    email_category: str | None
    draft_response: str | None
    messages: list[dict[str, Any]]


# Initialize LLM
model = ChatOpenAI(model="gpt-4o", temperature=0)


class EmailState(TypedDict):
    email: dict[str, Any]
    is_spam: bool | None
    draft_response: str | None
    messages: list[dict[str, Any]]


# Define nodes
def read_email(state: EmailState):
    email = state["email"]
    print(f"Alfred is processing an email from {email['sender']} with subject: {email['subject']}")
    return {}


def classify_email(state: EmailState):
    email = state["email"]

    prompt = f"""
As Alfred the butler of Mr wayne and it's SECRET identity Batman, analyze this email and determine if it is spam or legitimate and should be brought to Mr wayne's attention.

Email:
From: {email["sender"]}
Subject: {email["subject"]}
Body: {email["body"]}

First, determine if this email is spam.
answer with SPAM or HAM if it's legitimate. Only return the answer
Answer :
    """
    messages = [HumanMessage(content=prompt)]
    response = model.invoke(messages)

    response_text = response.content.lower()
    print(response_text)
    is_spam = "spam" in response_text and "ham" not in response_text

    if not is_spam:
        new_messages = state.get("messages", []) + [
            {"role": "user", "content": prompt},
            {"role": "assistant", "content": response.content},
        ]
    else:
        new_messages = state.get("messages", [])

    return {"is_spam": is_spam, "messages": new_messages}


def handle_spam(state: EmailState):
    print("Alfred has marked the email as spam.")
    print("The email has been moved to the spam folder.")
    return {}


def drafting_response(state: EmailState):
    email = state["email"]

    prompt = f"""
As Alfred the butler, draft a polite preliminary response to this email.

Email:
From: {email["sender"]}
Subject: {email["subject"]}
Body: {email["body"]}

Draft a brief, professional response that Mr. Wayne can review and personalize before sending.
    """

    messages = [HumanMessage(content=prompt)]
    response = model.invoke(messages)

    new_messages = state.get("messages", []) + [
        {"role": "user", "content": prompt},
        {"role": "assistant", "content": response.content},
    ]

    return {"draft_response": response.content, "messages": new_messages}


def notify_mr_wayne(state: EmailState):
    email = state["email"]

    print("\n" + "=" * 50)
    print(f"Sir, you've received an email from {email['sender']}.")
    print(f"Subject: {email['subject']}")
    print("\nI've prepared a draft response for your review:")
    print("-" * 50)
    print(state["draft_response"])
    print("=" * 50 + "\n")

    return {}


# Define routing logic
def route_email(state: EmailState) -> str:
    if state["is_spam"]:
        return "spam"
    else:
        return "legitimate"


# ê·¸ë˜í”„ ìƒì„±
email_graph = StateGraph(EmailState)

# ë…¸ë“œ ì¶”ê°€
email_graph.add_node("read_email", read_email)  # read_email ë…¸ë“œëŠ” read_mail í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•¨
email_graph.add_node(
    "classify_email", classify_email
)  # classify_email ë…¸ë“œëŠ” classify_email í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•¨
email_graph.add_node("handle_spam", handle_spam)  # ë™ì¼í•œ ë¡œì§
email_graph.add_node("drafting_response", drafting_response)  # ë™ì¼í•œ ë¡œì§
email_graph.add_node("notify_mr_wayne", notify_mr_wayne)  # ë™ì¼í•œ ë¡œì§

# ì—£ì§€ ì¶”ê°€
email_graph.add_edge(START, "read_email")  # ì‹œì‘ í›„ "read_email" ë…¸ë“œë¡œ ì´ë™

email_graph.add_edge("read_email", "classify_email")  # ì½ê¸° í›„ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰

# ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€
email_graph.add_conditional_edges(
    "classify_email",  # ë¶„ë¥˜ í›„ "route_email" í•¨ìˆ˜ë¥¼ ì‹¤í–‰
    route_email,
    {
        "spam": "handle_spam",  # "Spam"ì„ ë°˜í™˜í•˜ë©´ "handle_spam" ë…¸ë“œë¡œ ì´ë™
        "legitimate": "drafting_response",  # ì •ìƒ ë©”ì¼ì´ë©´ "drafting response" ë…¸ë“œë¡œ ì´ë™
    },
)

# ìµœì¢… ì—£ì§€ ì¶”ê°€
email_graph.add_edge("handle_spam", END)  # ìŠ¤íŒ¸ ì²˜ë¦¬ í›„ í•­ìƒ ì¢…ë£Œ
email_graph.add_edge("drafting_response", "notify_mr_wayne")
email_graph.add_edge("notify_mr_wayne", END)  # ì›¨ì¸ ì”¨ì—ê²Œ ì•Œë¦¼ í›„ ì¢…ë£Œ

# ê·¸ë˜í”„ ì»´íŒŒì¼
compiled_graph = email_graph.compile()

# í…ŒìŠ¤íŠ¸ìš© ì˜ˆì‹œ ì´ë©”ì¼
legitimate_email = {
    "sender": "Joker",
    "subject": "Found you Batman ! ",
    "body": "Mr. Wayne,I found your secret identity ! I know you're batman ! Ther's no denying it, I have proof of that and I'm coming to find you soon. I'll get my revenge. JOKER",
}

spam_email = {
    "sender": "Crypto bro",
    "subject": "The best investment of 2025",
    "body": "Mr Wayne, I just launched an ALT coin and want you to buy some !",
}

from langfuse.langchain import CallbackHandler

# Langchainìš© Langfuse CallbackHandler ì´ˆê¸°í™” (ì¶”ì )
langfuse_handler = CallbackHandler()

# ì •ìƒ ì´ë©”ì¼ ì²˜ë¦¬
print("\nProcessing legitimate email...")
legitimate_result = compiled_graph.invoke(
    input={"email": legitimate_email, "is_spam": None, "draft_response": None, "messages": []},
    config={"callbacks": [langfuse_handler]},
)

# ìŠ¤íŒ¸ ì´ë©”ì¼ ì²˜ë¦¬
print("\nProcessing spam email...")
spam_result = compiled_graph.invoke(
    input={"email": spam_email, "is_spam": None, "draft_response": None, "messages": []},
    config={"callbacks": [langfuse_handler]},
)

"""### Trace êµ¬ì¡°

LangfuseëŠ” ì—ì´ì „íŠ¸ ë¡œì§ì˜ ê° ë‹¨ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” **spans**ë¥¼ í¬í•¨í•˜ëŠ” **trace**ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ traceëŠ” ì „ì²´ ì—ì´ì „íŠ¸ ì‹¤í–‰ê³¼ ë‹¤ìŒì— ëŒ€í•œ í•˜ìœ„ ìŠ¤íŒ¬ì„ í¬í•¨í•©ë‹ˆë‹¤:
- ë„êµ¬ í˜¸ì¶œ (get_weather)
- LLM í˜¸ì¶œ ('gpt-4o'ë¥¼ ì‚¬ìš©í•œ Responses API)

ì´ë¥¼ ê²€ì‚¬í•˜ì—¬ ì‹œê°„ì´ ì–´ë””ì— ì†Œë¹„ë˜ëŠ”ì§€, ì–¼ë§ˆë‚˜ ë§ì€ í† í°ì´ ì‚¬ìš©ë˜ëŠ”ì§€ ë“±ì„ ì •í™•íˆ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

## ì˜¨ë¼ì¸ í‰ê°€

ì˜¨ë¼ì¸ í‰ê°€ëŠ” ì‹¤ì œ í™˜ê²½, ì¦‰ í”„ë¡œë•ì…˜ì—ì„œ ì‹¤ì œ ì‚¬ìš© ì¤‘ì— ì—ì´ì „íŠ¸ë¥¼ í‰ê°€í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì—¬ê¸°ì—ëŠ” ì‹¤ì œ ì‚¬ìš©ì ìƒí˜¸ ì‘ìš©ì— ëŒ€í•œ ì—ì´ì „íŠ¸ì˜ ì„±ëŠ¥ì„ ëª¨ë‹ˆí„°ë§í•˜ê³  ê²°ê³¼ë¥¼ ì§€ì†ì ìœ¼ë¡œ ë¶„ì„í•˜ëŠ” ê²ƒì´ í¬í•¨ë©ë‹ˆë‹¤.

ë‹¤ì–‘í•œ í‰ê°€ ê¸°ë²•ì— ëŒ€í•œ ê°€ì´ë“œë¥¼ [ì—¬ê¸°](https://langfuse.com/blog/2025-03-04-llm-evaluation-101-best-practices-and-challenges)ì— ì‘ì„±í–ˆìŠµë‹ˆë‹¤.

### í”„ë¡œë•ì…˜ì—ì„œ ì¶”ì í•  ì¼ë°˜ì ì¸ ë©”íŠ¸ë¦­

1. **ë¹„ìš©** â€” ê³„ì¸¡ì€ í† í° ì‚¬ìš©ëŸ‰ì„ ìº¡ì²˜í•˜ë©°, í† í°ë‹¹ ê°€ê²©ì„ í• ë‹¹í•˜ì—¬ ëŒ€ëµì ì¸ ë¹„ìš©ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. **ì§€ì—° ì‹œê°„** â€” ê° ë‹¨ê³„ ë˜ëŠ” ì „ì²´ ì‹¤í–‰ì„ ì™„ë£Œí•˜ëŠ” ë° ê±¸ë¦¬ëŠ” ì‹œê°„ì„ ê´€ì°°í•©ë‹ˆë‹¤.
3. **ì‚¬ìš©ì í”¼ë“œë°±** â€” ì‚¬ìš©ìëŠ” ì§ì ‘ í”¼ë“œë°±(ì°¬ì„±/ë°˜ëŒ€)ì„ ì œê³µí•˜ì—¬ ì—ì´ì „íŠ¸ë¥¼ ê°œì„ í•˜ê±°ë‚˜ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
4. **LLM-as-a-Judge** â€” ë³„ë„ì˜ LLMì„ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ì˜ ì¶œë ¥ì„ ê±°ì˜ ì‹¤ì‹œê°„ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤(ì˜ˆ: ë…ì„± ë˜ëŠ” ì •í™•ì„± í™•ì¸).

ì—ì´ì „íŠ¸ê°€ ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ì— ì„ë² ë“œëœ ê²½ìš° ì§ì ‘ ì‚¬ìš©ì í”¼ë“œë°±(ì±„íŒ… UIì˜ ì°¬ì„±/ë°˜ëŒ€ ë“±)ì„ ê¸°ë¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

from langfuse import get_client

langfuse = get_client()

# ì˜µì…˜ 1: ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ì—ì„œ ìƒì„±ëœ span ê°ì²´ ì‚¬ìš©
with langfuse.start_as_current_span(name="langgraph-request") as span:
    # ... LangGraph ì‹¤í–‰ ...

    # span ê°ì²´ë¥¼ ì‚¬ìš©í•œ ì ìˆ˜ ê¸°ë¡
    span.score_trace(
        name="user-feedback", value=1, data_type="NUMERIC", comment="This was correct, thank you"
    )

# ì˜µì…˜ 2: ì»¨í…ìŠ¤íŠ¸ ë‚´ì—ì„œ langfuse.score_current_trace() ì‚¬ìš©
with langfuse.start_as_current_span(name="langgraph-request") as span:
    # ... LangGraph ì‹¤í–‰ ...

    # í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•œ ì ìˆ˜ ê¸°ë¡
    langfuse.score_current_trace(name="user-feedback", value=1, data_type="NUMERIC")

# ì˜µì…˜ 3: trace IDì™€ í•¨ê»˜ create_score() ì‚¬ìš© (ì»¨í…ìŠ¤íŠ¸ ì™¸ë¶€)
langfuse.create_score(
    trace_id="predefined-trace-id",  # ìœ íš¨í•œ trace id í˜•ì‹ì´ì–´ì•¼ í•¨ (ë¬¸ì„œ ì°¸ì¡°)
    name="user-feedback",
    value=1,
    data_type="NUMERIC",
    comment="This was correct, thank you",
)

"""ì‚¬ìš©ì í”¼ë“œë°±ì€ Langfuseì— ìº¡ì²˜ë©ë‹ˆë‹¤:

#### 4. ìë™í™”ëœ LLM-as-a-Judge ì ìˆ˜ ë¶€ì—¬

LLM-as-a-JudgeëŠ” ì—ì´ì „íŠ¸ì˜ ì¶œë ¥ì„ ìë™ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ë˜ ë‹¤ë¥¸ ë°©ë²•ì…ë‹ˆë‹¤. ë³„ë„ì˜ LLM í˜¸ì¶œì„ ì„¤ì •í•˜ì—¬ ì¶œë ¥ì˜ ì •í™•ì„±, ë…ì„±, ìŠ¤íƒ€ì¼ ë˜ëŠ” ê´€ì‹¬ ìˆëŠ” ê¸°íƒ€ ê¸°ì¤€ì„ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ì›Œí¬í”Œë¡œìš°**:
1. **í‰ê°€ í…œí”Œë¦¿**ì„ ì •ì˜í•©ë‹ˆë‹¤. ì˜ˆ: "í…ìŠ¤íŠ¸ê°€ ë…ì„±ì´ ìˆëŠ”ì§€ í™•ì¸"
2. íŒë‹¨ ëª¨ë¸ë¡œ ì‚¬ìš©í•  ëª¨ë¸ì„ ì„¤ì •í•©ë‹ˆë‹¤. ì´ ê²½ìš° `gpt-4o-mini`ì…ë‹ˆë‹¤.
3. ì—ì´ì „íŠ¸ê°€ ì¶œë ¥ì„ ìƒì„±í•  ë•Œë§ˆë‹¤ í•´ë‹¹ ì¶œë ¥ì„ í…œí”Œë¦¿ê³¼ í•¨ê»˜ "íŒë‹¨" LLMì— ì „ë‹¬í•©ë‹ˆë‹¤.
4. íŒë‹¨ LLMì€ ê´€ì°° ê°€ëŠ¥ì„± ë„êµ¬ì— ê¸°ë¡í•˜ëŠ” ë“±ê¸‰ ë˜ëŠ” ë ˆì´ë¸”ë¡œ ì‘ë‹µí•©ë‹ˆë‹¤.
"""

# ìŠ¤íŒ¸ ì´ë©”ì¼ ì²˜ë¦¬
print("\nProcessing spam email...")
spam_result = compiled_graph.invoke(
    input={"email": spam_email, "is_spam": None, "draft_response": None, "messages": []},
    config={"callbacks": [langfuse_handler]},
)

"""ìŠ¤íŒ¸ ì´ë©”ì¼ ì²˜ë¦¬ ì¤‘...
Alfredê°€ Crypto broë¡œë¶€í„° ì˜¨ ì´ë©”ì¼ì„ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤. ì œëª©: The best investment of 2025
spam
Alfredê°€ ì´ë©”ì¼ì„ ìŠ¤íŒ¸ìœ¼ë¡œ í‘œì‹œí–ˆìŠµë‹ˆë‹¤.
ì´ë©”ì¼ì´ ìŠ¤íŒ¸ í´ë”ë¡œ ì´ë™ë˜ì—ˆìŠµë‹ˆë‹¤.

ì´ ì˜ˆì œì˜ ë‹µë³€ì´ "ë…ì„± ì—†ìŒ"ìœ¼ë¡œ íŒë‹¨ëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### 5. ê´€ì°° ê°€ëŠ¥ì„± ë©”íŠ¸ë¦­ ê°œìš”

ì´ëŸ¬í•œ ëª¨ë“  ë©”íŠ¸ë¦­ì€ ëŒ€ì‹œë³´ë“œì—ì„œ í•¨ê»˜ ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì—¬ëŸ¬ ì„¸ì…˜ì—ì„œ ì—ì´ì „íŠ¸ì˜ ì„±ëŠ¥ì„ ë¹ ë¥´ê²Œ í™•ì¸í•˜ê³  ì‹œê°„ ê²½ê³¼ì— ë”°ë¥¸ í’ˆì§ˆ ë©”íŠ¸ë¦­ì„ ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì˜¤í”„ë¼ì¸ í‰ê°€

ì˜¨ë¼ì¸ í‰ê°€ëŠ” ì‹¤ì‹œê°„ í”¼ë“œë°±ì— í•„ìˆ˜ì ì´ì§€ë§Œ **ì˜¤í”„ë¼ì¸ í‰ê°€**ë„ í•„ìš”í•©ë‹ˆë‹¤â€”ê°œë°œ ì „ ë˜ëŠ” ê°œë°œ ì¤‘ ì²´ê³„ì ì¸ ê²€ì‚¬. 
ì´ëŠ” ë³€ê²½ ì‚¬í•­ì„ í”„ë¡œë•ì…˜ì— ë°°í¬í•˜ê¸° ì „ì— í’ˆì§ˆê³¼ ì‹ ë¢°ì„±ì„ ìœ ì§€í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.

### ë°ì´í„°ì…‹ í‰ê°€

ì˜¤í”„ë¼ì¸ í‰ê°€ì—ì„œëŠ” ì¼ë°˜ì ìœ¼ë¡œ:
1. ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ ë³´ìœ (í”„ë¡¬í”„íŠ¸ ë° ì˜ˆìƒ ì¶œë ¥ ìŒ)
2. í•´ë‹¹ ë°ì´í„°ì…‹ì—ì„œ ì—ì´ì „íŠ¸ ì‹¤í–‰
3. ì¶œë ¥ì„ ì˜ˆìƒ ê²°ê³¼ì™€ ë¹„êµí•˜ê±°ë‚˜ ì¶”ê°€ ì ìˆ˜ ë¶€ì—¬ ë©”ì»¤ë‹ˆì¦˜ ì‚¬ìš©

ì•„ë˜ì—ì„œëŠ” ì§ˆë¬¸ê³¼ ì˜ˆìƒ ë‹µë³€ì´ í¬í•¨ëœ [q&a-dataset](https://huggingface.co/datasets/junzhang1207/search-dataset)ì„ ì‚¬ìš©í•˜ì—¬ ì´ ì ‘ê·¼ ë°©ì‹ì„ ì‹œì—°í•©ë‹ˆë‹¤.
"""

import pandas as pd
from datasets import load_dataset

# Hugging Faceì—ì„œ search-dataset ê°€ì ¸ì˜¤ê¸°
dataset = load_dataset("junzhang1207/search-dataset", split="train")
df = pd.DataFrame(dataset)
print("First few rows of search-dataset:")
print(df.head())

"""ë‹¤ìŒìœ¼ë¡œ, ì‹¤í–‰ì„ ì¶”ì í•˜ê¸° ìœ„í•´ Langfuseì—ì„œ ë°ì´í„°ì…‹ ì—”í‹°í‹°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ë°ì´í„°ì…‹ì˜ ê° í•­ëª©ì„ ì‹œìŠ¤í…œì— ì¶”ê°€í•©ë‹ˆë‹¤."""

from langfuse import Langfuse

langfuse = Langfuse()

langfuse_dataset_name = "qa-dataset_langgraph-agent"

# Langfuseì—ì„œ ë°ì´í„°ì…‹ ìƒì„±
langfuse.create_dataset(
    name=langfuse_dataset_name,
    description="q&a dataset uploaded from Hugging Face",
    metadata={"date": "2025-03-21", "type": "benchmark"},
)

df_30 = df.sample(30)  # ì´ ì˜ˆì œì—ì„œëŠ” 30ê°œì˜ ë°ì´í„°ì…‹ ì§ˆë¬¸ë§Œ ì—…ë¡œë“œ

for idx, row in df_30.iterrows():
    langfuse.create_dataset_item(
        dataset_name=langfuse_dataset_name,
        input={"text": row["question"]},
        expected_output={"text": row["expected_answer"]},
    )

"""#### ë°ì´í„°ì…‹ì—ì„œ ì—ì´ì „íŠ¸ ì‹¤í–‰

ë¨¼ì € OpenAI ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ê°„ë‹¨í•œ LangGraph ì—ì´ì „íŠ¸ë¥¼ ì¡°ë¦½í•©ë‹ˆë‹¤.
"""

from typing import Annotated

from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from typing_extensions import TypedDict


class State(TypedDict):
    messages: Annotated[list, add_messages]


graph_builder = StateGraph(State)

llm = ChatOpenAI(model="gpt-4.5-preview")


def chatbot(state: State):
    return {"messages": [llm.invoke(state["messages"])]}


graph_builder.add_node("chatbot", chatbot)
graph_builder.set_entry_point("chatbot")
graph_builder.set_finish_point("chatbot")

graph = graph_builder.compile()

"""ë‹¤ìŒìœ¼ë¡œ, ë‹¤ìŒì„ ìˆ˜í–‰í•˜ëŠ” í—¬í¼ í•¨ìˆ˜ `my_agent()`ë¥¼ ì •ì˜í•©ë‹ˆë‹¤:
1. Langfuse trace ìƒì„±
2. LangGraph ì‹¤í–‰ì„ ê³„ì¸¡í•˜ê¸° ìœ„í•´ `langfuse_handler_trace` ê°€ì ¸ì˜¤ê¸°
3. ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³  `langfuse_handler_trace`ë¥¼ í˜¸ì¶œì— ì „ë‹¬
"""

from langchain_openai import ChatOpenAI
from langfuse import get_client
from langfuse.langchain import CallbackHandler


class State(TypedDict):
    messages: Annotated[list, add_messages]


graph_builder = StateGraph(State)
llm = ChatOpenAI(model="gpt-4o")
langfuse = get_client()


def chatbot(state: State):
    return {"messages": [llm.invoke(state["messages"])]}


graph_builder.add_node("chatbot", chatbot)
graph_builder.set_entry_point("chatbot")
graph_builder.set_finish_point("chatbot")
graph = graph_builder.compile()


def my_agent(question, langfuse_handler):
    # Langfuse spanì„ í†µí•´ traceë¥¼ ìƒì„±í•˜ê³  ë‚´ë¶€ì—ì„œ Langchain ì‚¬ìš©
    with langfuse.start_as_current_span(name="my-langgraph-agent") as root_span:
        # ë‹¨ê³„ 2: LangChain ì²˜ë¦¬
        response = graph.invoke(
            input={"messages": [HumanMessage(content=question)]},
            config={"callbacks": [langfuse_handler]},
        )

        # trace ì¶œë ¥ ì—…ë°ì´íŠ¸
        root_span.update_trace(input=question, output=response["messages"][1].content)

        print(question)
        print(response["messages"][1].content)

    return response["messages"][1].content


"""ë§ˆì§€ë§‰ìœ¼ë¡œ, ê° ë°ì´í„°ì…‹ í•­ëª©ì„ ë°˜ë³µí•˜ê³ , ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³ , traceë¥¼ ë°ì´í„°ì…‹ í•­ëª©ì— ì—°ê²°í•©ë‹ˆë‹¤. ì›í•˜ëŠ” ê²½ìš° ë¹ ë¥¸ í‰ê°€ ì ìˆ˜ë¥¼ ì²¨ë¶€í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤."""

from langfuse import get_client
from langfuse.langchain import CallbackHandler

# Langchainìš© Langfuse CallbackHandler ì´ˆê¸°í™” (ì¶”ì )
langfuse_handler = CallbackHandler()
langfuse = get_client()

dataset = langfuse.get_dataset("qa-dataset_langgraph-agent")

for item in dataset.items:
    # ìë™ trace ì—°ê²°ì„ ìœ„í•´ item.run() ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì‚¬ìš©
    with item.run(
        run_name="run_gpt-4o",
        run_description="My first run",
        run_metadata={"model": "gpt-4o"},
    ) as root_span:
        # ì´ ë¸”ë¡ ë‚´ì˜ ëª¨ë“  ì‘ì—…ì€ ë°ì´í„°ì…‹ í•­ëª©ì— ëŒ€í•œ traceì˜ ì¼ë¶€

        # ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œì§ í˜¸ì¶œ - ë°ì½”ë ˆì´í„°, ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €,
        # ìˆ˜ë™ ê´€ì°° ë“± ëª¨ë“  ì¡°í•© ì‚¬ìš© ê°€ëŠ¥
        with langfuse.start_as_current_generation(
            name="llm-call", model="gpt-4o", input=item.input
        ) as generation:
            # LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œì§ ì‘ì„± ìœ„ì¹˜
            output = my_agent(str(item.input), langfuse_handler)
            generation.update(output=output)

        # ì„ íƒì‚¬í•­: ì˜ˆìƒ ì¶œë ¥ê³¼ ë¹„êµí•˜ì—¬ ê²°ê³¼ ì ìˆ˜ ë¶€ì—¬
        root_span.score_trace(
            name="user-feedback",
            value=1,
            comment="This is a comment",  # ì„ íƒì‚¬í•­, ì¶”ë¡  ì¶”ê°€ì— ìœ ìš©
        )

# ì‹¤í—˜ ì‹¤í–‰ ì¢…ë£Œ ì‹œ ëª¨ë“  ë°ì´í„°ê°€ ì„œë²„ë¡œ ì „ì†¡ë˜ë„ë¡ langfuse í´ë¼ì´ì–¸íŠ¸ flush
langfuse.flush()
