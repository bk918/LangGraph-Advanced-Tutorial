from langfuse import get_client

langfuse = get_client()

# ì—°ê²° í™•ì¸
if langfuse.auth_check():
    print("Langfuse client is authenticated and ready!")
else:
    print("Authentication failed. Please check your credentials and host.")

"""## ì˜ˆì œ 1: LangGraphë¥¼ ì‚¬ìš©í•œ ê°„ë‹¨í•œ ì±—ë´‡ ì•±

**ì´ ì„¹ì…˜ì—ì„œ ìˆ˜í–‰í•  ì‘ì—…:**

*   ì¼ë°˜ì ì¸ ì§ˆë¬¸ì— ë‹µë³€í•  ìˆ˜ ìˆëŠ” ì§€ì› ì±—ë´‡ì„ LangGraphë¡œ êµ¬ì¶•
*   Langfuseë¥¼ ì‚¬ìš©í•˜ì—¬ ì±—ë´‡ì˜ ì…ë ¥ê³¼ ì¶œë ¥ì„ ì¶”ì 

ê¸°ë³¸ ì±—ë´‡ìœ¼ë¡œ ì‹œì‘í•˜ì—¬ ë‹¤ìŒ ì„¹ì…˜ì—ì„œ ë” ê³ ê¸‰ ë©€í‹° ì—ì´ì „íŠ¸ ì„¤ì •ì„ êµ¬ì¶•í•˜ë©´ì„œ ì£¼ìš” LangGraph ê°œë…ì„ ì†Œê°œí•©ë‹ˆë‹¤.

### ì—ì´ì „íŠ¸ ìƒì„±

`StateGraph`ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤. `StateGraph` ê°ì²´ëŠ” ì±—ë´‡ì˜ êµ¬ì¡°ë¥¼ ìƒíƒœ ë¨¸ì‹ ìœ¼ë¡œ ì •ì˜í•©ë‹ˆë‹¤. LLMê³¼ ì±—ë´‡ì´ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë…¸ë“œë¥¼ ì¶”ê°€í•˜ê³ , ë´‡ì´ ì´ëŸ¬í•œ í•¨ìˆ˜ ê°„ì— ì–´ë–»ê²Œ ì „í™˜í•˜ëŠ”ì§€ ì§€ì •í•˜ëŠ” ì—£ì§€ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
"""

from typing import Annotated

from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from typing_extensions import TypedDict


class State(TypedDict):
    messages: Annotated[list, add_messages]


graph_builder = StateGraph(State)

llm = ChatOpenAI(model="gpt-4o", temperature=0.2)


# chatbot ë…¸ë“œ í•¨ìˆ˜ëŠ” í˜„ì¬ Stateë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ì—…ë°ì´íŠ¸ëœ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. ì´ê²ƒì€ ëª¨ë“  LangGraph ë…¸ë“œ í•¨ìˆ˜ì˜ ê¸°ë³¸ íŒ¨í„´ì…ë‹ˆë‹¤.
def chatbot(state: State):
    return {"messages": [llm.invoke(state["messages"])]}


# "chatbot" ë…¸ë“œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. ë…¸ë“œëŠ” ì‘ì—… ë‹¨ìœ„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì¼ë°˜ íŒŒì´ì¬ í•¨ìˆ˜ì…ë‹ˆë‹¤.
graph_builder.add_node("chatbot", chatbot)

# ì§„ì…ì ì„ ì¶”ê°€í•©ë‹ˆë‹¤. ì´ê²ƒì€ ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•  ë•Œë§ˆë‹¤ ì–´ë””ì„œ ì‹œì‘í• ì§€ ì•Œë ¤ì¤ë‹ˆë‹¤.
graph_builder.set_entry_point("chatbot")

# ì¢…ë£Œì ì„ ì„¤ì •í•©ë‹ˆë‹¤. ì´ê²ƒì€ ê·¸ë˜í”„ì— "ì´ ë…¸ë“œê°€ ì‹¤í–‰ë  ë•Œë§ˆë‹¤ ì¢…ë£Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"ë¼ê³  ì§€ì‹œí•©ë‹ˆë‹¤.
graph_builder.set_finish_point("chatbot")

# ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ ê·¸ë˜í”„ ë¹Œë”ì—ì„œ "compile()"ì„ í˜¸ì¶œí•©ë‹ˆë‹¤. ì´ê²ƒì€ ìƒíƒœì—ì„œ invokeí•  ìˆ˜ ìˆëŠ” "CompiledGraph"ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
graph = graph_builder.compile()

"""### í˜¸ì¶œì— Langfuseë¥¼ ì½œë°±ìœ¼ë¡œ ì¶”ê°€

ì´ì œ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ë‹¨ê³„ë¥¼ ì¶”ì í•˜ê¸° ìœ„í•´ [LangChainìš© Langfuse ì½œë°± í•¸ë“¤ëŸ¬](https://langfuse.com/integrations/frameworks/langchain)ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤: `config={"callbacks": [langfuse_handler]}`
"""

from langfuse.langchain import CallbackHandler

# Langchainìš© Langfuse CallbackHandler ì´ˆê¸°í™” (ì¶”ì ìš©)
langfuse_handler = CallbackHandler()

for s in graph.stream(
    {"messages": [HumanMessage(content="What is Langfuse?")]},
    config={"callbacks": [langfuse_handler]},
):
    print(s)

"""
### LangGraph Serverì—ì„œ Langfuse ì‚¬ìš©

[LangGraph Server](https://langchain-ai.github.io/langgraph/concepts/langgraph_server/)ë¥¼ ì‚¬ìš©í•  ë•Œ Langfuseë¥¼ ì½œë°±ìœ¼ë¡œ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

LangGraph Serverë¥¼ ì‚¬ìš©í•˜ë©´ LangGraph Serverê°€ ê·¸ë˜í”„ í˜¸ì¶œì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ê·¸ë˜í”„ë¥¼ ì„ ì–¸í•  ë•Œ Langfuse ì½œë°±ì„ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.
"""

from typing import Annotated

from langchain_openai import ChatOpenAI
from langfuse.langchain import CallbackHandler
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from typing_extensions import TypedDict


class State(TypedDict):
    messages: Annotated[list, add_messages]


graph_builder = StateGraph(State)

llm = ChatOpenAI(model="gpt-4o", temperature=0.2)


def chatbot(state: State):
    return {"messages": [llm.invoke(state["messages"])]}


graph_builder.add_node("chatbot", chatbot)
graph_builder.set_entry_point("chatbot")
graph_builder.set_finish_point("chatbot")

# Langchainìš© Langfuse CallbackHandler ì´ˆê¸°í™” (ì¶”ì ìš©)
langfuse_handler = CallbackHandler()

# ì»´íŒŒì¼ëœ ê·¸ë˜í”„ì—ì„œ "with_config"ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
# "compile"ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ ì½œë°±ì´ í¬í•¨ëœ "CompiledGraph"ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
# ì´ë ‡ê²Œ í•˜ë©´ ë§¤ë²ˆ ìˆ˜ë™ìœ¼ë¡œ ì½œë°±ì„ ì¶”ê°€í•˜ì§€ ì•Šê³ ë„ ìë™ìœ¼ë¡œ ê·¸ë˜í”„ë¥¼ ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
graph = graph_builder.compile().with_config({"callbacks": [langfuse_handler]})

"""## ì˜ˆì œ 2: LangGraphë¥¼ ì‚¬ìš©í•œ ë©€í‹° ì—ì´ì „íŠ¸ ì• í”Œë¦¬ì¼€ì´ì…˜

**ì´ ì„¹ì…˜ì—ì„œ ìˆ˜í–‰í•  ì‘ì—…**:

*   2ê°œì˜ ì‹¤í–‰ ì—ì´ì „íŠ¸ êµ¬ì¶•: LangChain WikipediaAPIWrapperë¥¼ ì‚¬ìš©í•˜ì—¬ Wikipediaë¥¼ ê²€ìƒ‰í•˜ëŠ” ì—°êµ¬ ì—ì´ì „íŠ¸ í•˜ë‚˜ì™€ í˜„ì¬ ì‹œê°„ì„ ì•Œë ¤ì£¼ëŠ” ì»¤ìŠ¤í…€ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ì—ì´ì „íŠ¸ í•˜ë‚˜
*   ì‚¬ìš©ì ì§ˆë¬¸ì„ ë‘ ì—ì´ì „íŠ¸ ì¤‘ í•˜ë‚˜ì— ìœ„ì„í•˜ëŠ” ì—ì´ì „íŠ¸ ìŠˆí¼ë°”ì´ì € êµ¬ì¶•
*   ìŠˆí¼ë°”ì´ì €ì™€ ì‹¤í–‰ ì—ì´ì „íŠ¸ì˜ ë‹¨ê³„ë¥¼ ì¶”ì í•˜ê¸° ìœ„í•´ Langfuse í•¸ë“¤ëŸ¬ë¥¼ ì½œë°±ìœ¼ë¡œ ì¶”ê°€
"""

"""### ë„êµ¬ ìƒì„±

ì´ ì˜ˆì œì—ì„œëŠ” Wikipedia ì—°êµ¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì—ì´ì „íŠ¸ í•˜ë‚˜ì™€ í˜„ì¬ ì‹œê°„ì„ ì•Œë ¤ì£¼ëŠ” ì—ì´ì „íŠ¸ í•˜ë‚˜ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.
ì•„ë˜ì—ì„œ ì‚¬ìš©í•  ë„êµ¬ë¥¼ ì •ì˜í•©ë‹ˆë‹¤:
"""

from datetime import datetime
from typing import Annotated

from langchain.tools import Tool
from langchain_community.tools import WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper

# Wikipediaë¥¼ ê²€ìƒ‰í•˜ëŠ” ë„êµ¬ë¥¼ ì •ì˜í•©ë‹ˆë‹¤
wikipedia_tool = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())

# í˜„ì¬ ë‚ ì§œ/ì‹œê°„ì„ ë°˜í™˜í•˜ëŠ” ìƒˆ ë„êµ¬ë¥¼ ì •ì˜í•©ë‹ˆë‹¤
datetime_tool = Tool(
    name="Datetime",
    func=lambda x: datetime.now().isoformat(),
    description="Returns the current datetime",
)

"""### í—¬í¼ ìœ í‹¸ë¦¬í‹°

ìƒˆ ì—ì´ì „íŠ¸ ì›Œì»¤ ë…¸ë“œë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒì„ ë‹¨ìˆœí™”í•˜ê¸° ìœ„í•´ ì•„ë˜ì— í—¬í¼ í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
"""

from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_core.messages import BaseMessage, HumanMessage
from langchain_openai import ChatOpenAI


def create_agent(llm: ChatOpenAI, system_prompt: str, tools: list):
    # ê° ì›Œì»¤ ë…¸ë“œì—ëŠ” ì´ë¦„ê³¼ ì¼ë¶€ ë„êµ¬ê°€ ì œê³µë©ë‹ˆë‹¤.
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                system_prompt,
            ),
            MessagesPlaceholder(variable_name="messages"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ]
    )
    agent = create_openai_tools_agent(llm, tools, prompt)
    executor = AgentExecutor(agent=agent, tools=tools)
    return executor


def agent_node(state, agent, name):
    result = agent.invoke(state)
    return {"messages": [HumanMessage(content=result["output"], name=name)]}


"""### ì—ì´ì „íŠ¸ ìŠˆí¼ë°”ì´ì € ìƒì„±

í•¨ìˆ˜ í˜¸ì¶œì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ì›Œì»¤ ë…¸ë“œë¥¼ ì„ íƒí•˜ê±°ë‚˜ ì²˜ë¦¬ë¥¼ ì™„ë£Œí•©ë‹ˆë‹¤.
"""

from langchain_core.output_parsers.openai_functions import JsonOutputFunctionsParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

members = ["Researcher", "CurrentTime"]
system_prompt = (
    "You are a supervisor tasked with managing a conversation between the"
    " following workers:  {members}. Given the following user request,"
    " respond with the worker to act next. Each worker will perform a"
    " task and respond with their results and status. When finished,"
    " respond with FINISH."
)
# ìš°ë¦¬ íŒ€ ìŠˆí¼ë°”ì´ì €ëŠ” LLM ë…¸ë“œì…ë‹ˆë‹¤. ì²˜ë¦¬í•  ë‹¤ìŒ ì—ì´ì „íŠ¸ë¥¼ ì„ íƒí•˜ê³  ì‘ì—…ì´ ì™„ë£Œë˜ëŠ” ì‹œì ì„ ê²°ì •í•©ë‹ˆë‹¤
options = ["FINISH"] + members

# OpenAI í•¨ìˆ˜ í˜¸ì¶œì„ ì‚¬ìš©í•˜ë©´ ì¶œë ¥ íŒŒì‹±ì´ ë” ì‰¬ì›Œì§‘ë‹ˆë‹¤
function_def = {
    "name": "route",
    "description": "Select the next role.",
    "parameters": {
        "title": "routeSchema",
        "type": "object",
        "properties": {
            "next": {
                "title": "Next",
                "anyOf": [
                    {"enum": options},
                ],
            }
        },
        "required": ["next"],
    },
}

# ChatPromptTemplateì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        MessagesPlaceholder(variable_name="messages"),
        (
            "system",
            "Given the conversation above, who should act next?"
            " Or should we FINISH? Select one of: {options}",
        ),
    ]
).partial(options=str(options), members=", ".join(members))

llm = ChatOpenAI(model="gpt-4o")

# Construction of the chain for the supervisor agent
supervisor_chain = (
    prompt
    | llm.bind_functions(functions=[function_def], function_call="route")
    | JsonOutputFunctionsParser()
)

"""### ê·¸ë˜í”„ êµ¬ì„±"""

import functools
import operator
from collections.abc import Sequence
from typing import TypedDict

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langgraph.graph import END, START, StateGraph


# ì—ì´ì „íŠ¸ ìƒíƒœëŠ” ê·¸ë˜í”„ì˜ ê° ë…¸ë“œì— ëŒ€í•œ ì…ë ¥ì…ë‹ˆë‹¤
class AgentState(TypedDict):
    # ì£¼ì„ì€ ê·¸ë˜í”„ì— ìƒˆ ë©”ì‹œì§€ê°€ í•­ìƒ í˜„ì¬ ìƒíƒœì— ì¶”ê°€ëœë‹¤ê³  ì•Œë ¤ì¤ë‹ˆë‹¤
    messages: Annotated[Sequence[BaseMessage], operator.add]
    # 'next' í•„ë“œëŠ” ë‹¤ìŒì— ì–´ë””ë¡œ ë¼ìš°íŒ…í• ì§€ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤
    next: str


# create_agent í—¬í¼ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—°êµ¬ ì—ì´ì „íŠ¸ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤
research_agent = create_agent(
    llm,
    tools=[wikipedia_tool],
    system_prompt="You are a web researcher.",
)
research_node = functools.partial(agent_node, agent=research_agent, name="Researcher")

# create_agent í—¬í¼ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê°„ ì—ì´ì „íŠ¸ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤
currenttime_agent = create_agent(
    llm,
    tools=[datetime_tool],
    system_prompt="You can tell the current time at",
)
currenttime_node = functools.partial(agent_node, agent=currenttime_agent, name="CurrentTime")

workflow = StateGraph(AgentState)

# "chatbot" ë…¸ë“œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. ë…¸ë“œëŠ” ì‘ì—… ë‹¨ìœ„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì¼ë°˜ íŒŒì´ì¬ í•¨ìˆ˜ì…ë‹ˆë‹¤.
workflow.add_node("Researcher", research_node)
workflow.add_node("CurrentTime", currenttime_node)
workflow.add_node("supervisor", supervisor_chain)

# ì‘ì—…ì´ ì™„ë£Œë˜ë©´ ì›Œì»¤ê°€ í•­ìƒ ìŠˆí¼ë°”ì´ì €ì—ê²Œ "ë³´ê³ "í•˜ê¸°ë¥¼ ì›í•©ë‹ˆë‹¤
for member in members:
    workflow.add_edge(member, "supervisor")

# ì¡°ê±´ë¶€ ì—£ì§€ëŠ” ì¼ë°˜ì ìœ¼ë¡œ í˜„ì¬ ê·¸ë˜í”„ ìƒíƒœì— ë”°ë¼ ë‹¤ë¥¸ ë…¸ë“œë¡œ ë¼ìš°íŒ…í•˜ëŠ” "if" ë¬¸ì„ í¬í•¨í•©ë‹ˆë‹¤.
# ì´ëŸ¬í•œ í•¨ìˆ˜ëŠ” í˜„ì¬ ê·¸ë˜í”„ ìƒíƒœë¥¼ ìˆ˜ì‹ í•˜ê³  ë‹¤ìŒì— í˜¸ì¶œí•  ë…¸ë“œë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë¬¸ìì—´ ë˜ëŠ” ë¬¸ìì—´ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
conditional_map = {k: k for k in members}
conditional_map["FINISH"] = END
workflow.add_conditional_edges("supervisor", lambda x: x["next"], conditional_map)

# ì§„ì…ì ì„ ì¶”ê°€í•©ë‹ˆë‹¤. ì´ê²ƒì€ ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•  ë•Œë§ˆë‹¤ ì–´ë””ì„œ ì‹œì‘í• ì§€ ì•Œë ¤ì¤ë‹ˆë‹¤.
workflow.add_edge(START, "supervisor")

# ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ ê·¸ë˜í”„ ë¹Œë”ì—ì„œ "compile()"ì„ í˜¸ì¶œí•©ë‹ˆë‹¤. ì´ê²ƒì€ ìƒíƒœì—ì„œ invokeí•  ìˆ˜ ìˆëŠ” "CompiledGraph"ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
graph_2 = workflow.compile()

"""### í˜¸ì¶œì— Langfuseë¥¼ ì½œë°±ìœ¼ë¡œ ì¶”ê°€

[Langfuse í•¸ë“¤ëŸ¬](https://langfuse.com/integrations/frameworks/langchain)ë¥¼ ì½œë°±ìœ¼ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤: `config={"callbacks": [langfuse_handler]}`
"""

from langfuse.langchain import CallbackHandler

# Langchainìš© Langfuse CallbackHandler ì´ˆê¸°í™” (ì¶”ì ìš©)
langfuse_handler = CallbackHandler()

# Langfuse í•¸ë“¤ëŸ¬ë¥¼ ì½œë°±ìœ¼ë¡œ ì¶”ê°€: config={"callbacks": [langfuse_handler]}
# Langfuseì—ì„œ trace ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©ë  ì„ íƒì  'run_name'ì„ ì„¤ì •í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤
for s in graph_2.stream(
    {"messages": [HumanMessage(content="How does photosynthesis work?")]},
    config={"callbacks": [langfuse_handler]},
):
    print(s)
    print("----")

# Langfuse í•¸ë“¤ëŸ¬ë¥¼ ì½œë°±ìœ¼ë¡œ ì¶”ê°€: config={"callbacks": [langfuse_handler]}
for s in graph_2.stream(
    {"messages": [HumanMessage(content="What time is it?")]},
    config={"callbacks": [langfuse_handler]},
):
    print(s)
    print("----")

"""
## Multi LangGraph ì—ì´ì „íŠ¸

í•˜ë‚˜ì˜ LangGraph ì—ì´ì „íŠ¸ê°€ í•˜ë‚˜ ì´ìƒì˜ ë‹¤ë¥¸ LangGraph ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” ì„¤ì •ì´ ìˆìŠµë‹ˆë‹¤.
ë©€í‹° ì—ì´ì „íŠ¸ ì‹¤í–‰ì— ëŒ€í•œ ëª¨ë“  í•´ë‹¹ ìŠ¤íŒ¬ì„ í•˜ë‚˜ì˜ ë‹¨ì¼ traceë¡œ ê²°í•©í•˜ë ¤ë©´ ì»¤ìŠ¤í…€ `trace_id`ë¥¼ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë¨¼ì €, ë‘ ì—ì´ì „íŠ¸ ëª¨ë‘ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” trace_idë¥¼ ìƒì„±í•˜ì—¬ ì—ì´ì „íŠ¸ ì‹¤í–‰ì„ í•˜ë‚˜ì˜ Langfuse traceë¡œ ê·¸ë£¹í™”í•©ë‹ˆë‹¤.
"""

from langfuse import Langfuse, get_client
from langfuse.langchain import CallbackHandler

langfuse = get_client()

# ì™¸ë¶€ ì‹œìŠ¤í…œì—ì„œ ê²°ì •ë¡ ì  trace IDë¥¼ ìƒì„±í•©ë‹ˆë‹¤
predefined_trace_id = Langfuse.create_trace_id()

# Langchainìš© Langfuse CallbackHandler ì´ˆê¸°í™” (ì¶”ì ìš©)
langfuse_handler = CallbackHandler()

"""ë‹¤ìŒìœ¼ë¡œ ì„œë¸Œ ì—ì´ì „íŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."""

from typing import Annotated

from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from typing_extensions import TypedDict


class State(TypedDict):
    messages: Annotated[list, add_messages]


graph_builder = StateGraph(State)

llm = ChatOpenAI(model="gpt-4o", temperature=0.2)


def chatbot(state: State):
    return {"messages": [llm.invoke(state["messages"])]}


graph_builder.add_node("chatbot", chatbot)
graph_builder.set_entry_point("chatbot")
graph_builder.set_finish_point("chatbot")
sub_agent = graph_builder.compile()

"""ê·¸ëŸ° ë‹¤ìŒ, ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•´ research-sub-agentë¥¼ ì‚¬ìš©í•˜ëŠ” ë„êµ¬ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."""

from langchain_core.tools import tool


@tool
def langgraph_research(question):
    """ë‹¤ì–‘í•œ ì£¼ì œì— ëŒ€í•œ ì—°êµ¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""

    with langfuse.start_as_current_span(
        name="ğŸ¤–-sub-research-agent", trace_context={"trace_id": predefined_trace_id}
    ) as span:
        span.update_trace(input=question)

        response = sub_agent.invoke(
            {"messages": [HumanMessage(content=question)]}, config={"callbacks": [langfuse_handler]}
        )

        span.update_trace(output=response["messages"][1].content)

    return response["messages"][1].content


"""Set up a second simple LangGraph agent that uses the new `langgraph_research`."""

from langchain.agents import create_agent
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4.1", temperature=0.2)

main_agent = create_agent(model=llm, tools=[langgraph_research])

user_question = "What is Langfuse?"

# trace_contextì™€ í•¨ê»˜ ë¯¸ë¦¬ ì •ì˜ëœ trace IDë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤
with langfuse.start_as_current_span(
    name="ğŸ¤–-main-agent", trace_context={"trace_id": predefined_trace_id}
) as span:
    span.update_trace(input=user_question)

    # LangChain ì‹¤í–‰ì´ ì´ traceì˜ ì¼ë¶€ê°€ ë©ë‹ˆë‹¤
    response = main_agent.invoke(
        {"messages": [{"role": "user", "content": user_question}]},
        config={"callbacks": [langfuse_handler]},
    )

    span.update_trace(output=response["messages"][1].content)

print(f"Trace ID: {predefined_trace_id}")  # ë‚˜ì¤‘ì— scoringì— ì‚¬ìš©í•©ë‹ˆë‹¤

"""
## traceì— ì ìˆ˜ ì¶”ê°€í•˜ê¸°

[ì ìˆ˜(Scores)](https://langfuse.com/docs/scores/overview)ëŠ” ë‹¨ì¼ ê´€ì°° ë˜ëŠ” ì „ì²´ traceë¥¼ í‰ê°€í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ëŸ°íƒ€ì„ì— ì»¤ìŠ¤í…€ í’ˆì§ˆ ê²€ì‚¬ë¥¼ êµ¬í˜„í•˜ê±°ë‚˜ ì‚¬ëŒì´ ê°œì…í•˜ëŠ”(human-in-the-loop) í‰ê°€ í”„ë¡œì„¸ìŠ¤ë¥¼ ìš©ì´í•˜ê²Œ í•©ë‹ˆë‹¤.

ì•„ë˜ ì˜ˆì œì—ì„œëŠ” íŠ¹ì • ìŠ¤íŒ¬ì— ëŒ€í•œ `relevance`(ìˆ«ì ì ìˆ˜)ì™€ ì „ì²´ traceì— ëŒ€í•œ `feedback`(ë²”ì£¼í˜• ì ìˆ˜)ì˜ ì ìˆ˜ë¥¼ ë§¤ê¸°ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í•˜ê³  ê°œì„ í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.

**â†’ [Langfuseì˜ ì»¤ìŠ¤í…€ ì ìˆ˜](https://langfuse.com/docs/scores/custom)ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ê¸°.**
"""

from langfuse import get_client

langfuse = get_client()

# ì˜µì…˜ 1: ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ì—ì„œ ë°˜í™˜ëœ span ê°ì²´ ì‚¬ìš©
with langfuse.start_as_current_span(name="langgraph-request") as span:
    # ... LangGraph ì‹¤í–‰ ...

    # span ê°ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì ìˆ˜ ë§¤ê¸°ê¸°
    span.score_trace(
        name="user-feedback", value=1, data_type="NUMERIC", comment="This was correct, thank you"
    )

# ì˜µì…˜ 2: ì»¨í…ìŠ¤íŠ¸ ë‚´ì— ìˆëŠ” ê²½ìš° langfuse.score_current_trace() ì‚¬ìš©
with langfuse.start_as_current_span(name="langgraph-request") as span:
    # ... LangGraph ì‹¤í–‰ ...

    # í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì ìˆ˜ ë§¤ê¸°ê¸°
    langfuse.score_current_trace(name="user-feedback", value=1, data_type="NUMERIC")

# ì˜µì…˜ 3: ì»¨í…ìŠ¤íŠ¸ ì™¸ë¶€ì—ì„œ trace IDì™€ í•¨ê»˜ create_score() ì‚¬ìš©
langfuse.create_score(
    trace_id=predefined_trace_id,
    name="user-feedback",
    value=1,
    data_type="NUMERIC",
    comment="This was correct, thank you",
)

"""## Langfuseë¡œ í”„ë¡¬í”„íŠ¸ ê´€ë¦¬

[Langfuse í”„ë¡¬í”„íŠ¸ ê´€ë¦¬](https://langfuse.com/docs/prompts/example-langchain)ë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê³  ë²„ì „ì„ ê´€ë¦¬í•˜ì„¸ìš”. ì´ ì˜ˆì œì—ì„œëŠ” SDKë¥¼ í†µí•´ ì‚¬ìš©ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ í”„ë¡œë•ì…˜ì—ì„œëŠ” ì‚¬ìš©ìê°€ SDKë¥¼ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹  Langfuse UIë¥¼ í†µí•´ í”„ë¡¬í”„íŠ¸ë¥¼ ì—…ë°ì´íŠ¸í•˜ê³  ê´€ë¦¬í•©ë‹ˆë‹¤.

Langfuse í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ CMS(ì½˜í…ì¸  ê´€ë¦¬ ì‹œìŠ¤í…œ)ì…ë‹ˆë‹¤. ë˜ëŠ” Langfuse UIì—ì„œ í”„ë¡¬í”„íŠ¸ë¥¼ í¸ì§‘í•˜ê³  ë²„ì „ì„ ê´€ë¦¬í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

*   Langfuse í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ì—ì„œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‹ë³„í•˜ëŠ” `Name`
*   `{{input variables}}`ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì´ ìˆëŠ” í”„ë¡¬í”„íŠ¸
*   í”„ë¡¬í”„íŠ¸ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì¦‰ì‹œ ì‚¬ìš©í•˜ë ¤ë©´ `production`ì„ í¬í•¨í•˜ëŠ” `labels`
"""

from langfuse import get_client

langfuse = get_client()

langfuse.create_prompt(
    name="translator_system-prompt",
    prompt="You are a translator that translates every input text into Spanish.",
    labels=["production"],
)

"""![View prompt in Langfuse UI](https://langfuse.com/images/cookbook/integration-langgraph/integration_langgraph_prompt_example.png)

Use the utility method `.get_langchain_prompt()` to transform the Langfuse prompt into a string that can be used in Langchain.


**Context:** Langfuse declares input variables in prompt templates using double brackets (`{{input variable}}`). Langchain uses single brackets for declaring input variables in PromptTemplates (`{input variable}`). The utility method `.get_langchain_prompt()` replaces the double brackets with single brackets. In this example, however, we don't use any variables in our prompt.
"""

# Get current production version of prompt and transform the Langfuse prompt into a string that can be used in Langchain
langfuse_system_prompt = langfuse.get_prompt("translator_system-prompt")
langchain_system_prompt = langfuse_system_prompt.get_langchain_prompt()  # ì´ ë¶€ë¶„ì´ ì¤‘ìš”í•¨!

print(langchain_system_prompt)

"""Now we can use the new system prompt string to update our assistant."""

from typing import Annotated

from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from typing_extensions import TypedDict


class State(TypedDict):
    messages: Annotated[list, add_messages]


graph_builder = StateGraph(State)

llm = ChatOpenAI(model="gpt-4.1", temperature=0.2)

# ë²ˆì—­ê¸° ì–´ì‹œìŠ¤í„´íŠ¸ìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤
system_prompt = {"role": "system", "content": langchain_system_prompt}


def chatbot(state: State):
    messages_with_system_prompt = [system_prompt] + state["messages"]
    response = llm.invoke(messages_with_system_prompt)
    return {"messages": [response]}


graph_builder.add_node("chatbot", chatbot)
graph_builder.set_entry_point("chatbot")
graph_builder.set_finish_point("chatbot")
graph = graph_builder.compile()

from langfuse.langchain import CallbackHandler

# Langchainìš© Langfuse CallbackHandler ì´ˆê¸°í™” (ì¶”ì ìš©)
langfuse_handler = CallbackHandler()

# Langfuse í•¸ë“¤ëŸ¬ë¥¼ ì½œë°±ìœ¼ë¡œ ì¶”ê°€: config={"callbacks": [langfuse_handler]}
for s in graph.stream(
    {"messages": [HumanMessage(content="What is Langfuse?")]},
    config={"callbacks": [langfuse_handler]},
):
    print(s)

"""## LangGraph traceì— ì»¤ìŠ¤í…€ ìŠ¤íŒ¬ ì¶”ê°€í•˜ê¸°

ë•Œë•Œë¡œ LangGraph traceì— ì»¤ìŠ¤í…€ ìŠ¤íŒ¬ì„ ì¶”ê°€í•˜ëŠ” ê²ƒì´ ìœ ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ [GitHub í† ë¡  ìŠ¤ë ˆë“œ](https://github.com/orgs/langfuse/discussions/2988#discussioncomment-11634600)ì—ì„œ ì´ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì˜ ì˜ˆì œë¥¼ ì œê³µí•©ë‹ˆë‹¤.
"""
