{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## LLM-as-a-Judge í¸í–¥ ìœ í˜•\n",
    "\n",
    "### ì•”ì‹œì  í¸í–¥ (Implicit Bias)\n",
    "1. **ì¥í™©í•¨/ê¸¸ì´ í¸í–¥** (Verbosity Bias): ë” ê¸´ ë‹µë³€ì— ë†’ì€ ì ìˆ˜\n",
    "2. **í’ë¶€í•œ ì½˜í…ì¸  í¸í–¥** (Rich Content Bias): ë¶ˆí•„ìš”í•œ ì •êµí•¨ ì„ í˜¸\n",
    "3. **CoT í¸í–¥** (Chain-of-Thought Bias): ëª…ì‹œì  ì¶”ë¡  ê³¼ì • ì„ í˜¸\n",
    "4. **ê°ì„±/ì–´ì¡° í¸í–¥** (Sentiment Bias): ê¸ì •ì  ì–´ì¡° ì„ í˜¸\n",
    "\n",
    "### ëª…ì‹œì  í¸í–¥ (Explicit Bias)\n",
    "5. **ì‚¬ì‹¤ ì˜¤ë¥˜ í¸í–¥** (Factual Error Bias): ì˜¤ë¥˜ì— ëŒ€í•œ ê³¼ë„í•œ/ê³¼ì†Œí•œ ë°˜ì‘\n",
    "6. **ì¸ê¸° í¸í–¥** (Popularity Bias): ì£¼ë¥˜ ì˜ê²¬ ì„ í˜¸\n",
    "7. **ì„±ë³„/ë¬¸í™” í¸í–¥** (Demographic Bias): ê³ ì •ê´€ë… ê¸°ë°˜ í‰ê°€\n",
    "\n",
    "### ì‹œìŠ¤í…œì  í¸í–¥ (Systematic Bias)\n",
    "8. **ìœ„ì¹˜ í¸í–¥** (Position Bias): ì²« ë²ˆì§¸ ë‹µë³€ ì„ í˜¸\n",
    "9. **ìê¸° ê°•í™” í¸í–¥** (Self-Enhancement Bias): ë™ì¼ ëª¨ë¸ ë‹µë³€ ì„ í˜¸\n",
    "10. **ì ìˆ˜ ë²”ìœ„ í¸í–¥** (Score Range Bias): ì ìˆ˜ ë¶„í¬ ì™œê³¡\n",
    "11. **ë™ì˜ í¸í–¥** (Agreement Bias): False Negative ë†’ìŒ (ì‹¤ì œ ì˜¤ë¥˜ë¥¼ ë†“ì¹¨)\n",
    "12. **ì¼ê´€ì„± í¸í–¥** (Consistency Bias): ì´ì „ íŒë‹¨ì— ê³¼ë„í•˜ê²Œ ì˜ì¡´\n",
    "\n",
    "## ì°¸ê³  ìë£Œ\n",
    "\n",
    "- [CALM Framework](https://arxiv.org/abs/2510.12462)\n",
    "- [Anthropic: Constitutional AI](https://www.anthropic.com/index/constitutional-ai-harmlessness-from-ai-feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T07:12:12.684235Z",
     "iopub.status.busy": "2025-10-30T07:12:12.684060Z",
     "iopub.status.idle": "2025-10-30T07:12:12.694435Z",
     "shell.execute_reply": "2025-10-30T07:12:12.693791Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Literal\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openrouter_llm import create_openrouter_llm\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T07:12:12.696603Z",
     "iopub.status.busy": "2025-10-30T07:12:12.696444Z",
     "iopub.status.idle": "2025-10-30T07:12:12.699697Z",
     "shell.execute_reply": "2025-10-30T07:12:12.699188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IMPLICIT:\n",
      "  â€¢ verbosity_bias\n",
      "  â€¢ rich_content_bias\n",
      "  â€¢ cot_bias\n",
      "  â€¢ sentiment_bias\n",
      "\n",
      "EXPLICIT:\n",
      "  â€¢ factual_error_bias\n",
      "  â€¢ popularity_bias\n",
      "  â€¢ demographic_bias\n",
      "\n",
      "SYSTEMATIC:\n",
      "  â€¢ position_bias\n",
      "  â€¢ self_enhancement_bias\n",
      "  â€¢ score_range_bias\n",
      "  â€¢ agreement_bias\n",
      "  â€¢ consistency_bias\n",
      "\n",
      "ì´ 12ê°€ì§€ í¸í–¥ ìœ í˜•\n"
     ]
    }
   ],
   "source": [
    "# í¸í–¥ ìœ í˜• ì •ì˜\n",
    "BIAS_TYPES = {\n",
    "    \"implicit\": [\"verbosity_bias\", \"rich_content_bias\", \"cot_bias\", \"sentiment_bias\"],\n",
    "    \"explicit\": [\"factual_error_bias\", \"popularity_bias\", \"demographic_bias\"],\n",
    "    \"systematic\": [\n",
    "        \"position_bias\",\n",
    "        \"self_enhancement_bias\",\n",
    "        \"score_range_bias\",\n",
    "        \"agreement_bias\",\n",
    "        \"consistency_bias\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "for category, biases in BIAS_TYPES.items():\n",
    "    print(f\"\\n{category.upper()}:\")\n",
    "    for bias in biases:\n",
    "        print(f\"  â€¢ {bias}\")\n",
    "\n",
    "total_biases = sum(len(biases) for biases in BIAS_TYPES.values())\n",
    "print(f\"\\nì´ {total_biases}ê°€ì§€ í¸í–¥ ìœ í˜•\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. í¸í–¥ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤\n",
    "\n",
    "ë™ì¼í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆì§€ë§Œ í‘œí˜„ ë°©ì‹ì´ ë‹¤ë¥¸ ë‹µë³€ì„ ë¹„êµí•˜ì—¬ í¸í–¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T07:12:12.701253Z",
     "iopub.status.busy": "2025-10-30T07:12:12.701150Z",
     "iopub.status.idle": "2025-10-30T07:12:12.704249Z",
     "shell.execute_reply": "2025-10-30T07:12:12.703806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Verbosity Bias Test]\n",
      "ì§§ì€ ë‹µë³€: ë¹„ë°€ë²ˆí˜¸ëŠ” 12ì ì´ìƒ, ëŒ€ì†Œë¬¸ì+ìˆ«ì+íŠ¹ìˆ˜ë¬¸ì í•„ìš”\n",
      "\n",
      "ê¸´ ë‹µë³€: ë¹„ë°€ë²ˆí˜¸ ì •ì±…ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì²«ì§¸, ê¸¸ì´ëŠ” ìµœì†Œ 12ì ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
      "    ì´ëŠ” ë¸Œë£¨íŠ¸í¬ìŠ¤ ê³µê²©ì„ ë°©ì§€í•˜ê¸° ìœ„í•œ ì¤‘ìš”í•œ ë³´ì•ˆ ì¡°ì¹˜ì…ë‹ˆë‹¤. ë‘˜ì§¸, ëŒ€ë¬¸ìì™€ ì†Œë¬¸ìë¥¼ í˜¼í•©í•´ì•¼ í•©ë‹ˆë‹¤.\n",
      "    ì´ë¥¼ í†µí•´ ë¹„ë°€ë²ˆí˜¸ì˜ ë³µì¡ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì…‹ì§¸, ìˆ«ìë¥¼ ìµœì†Œ 1ê°œ ì´ìƒ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.\n",
      "    ë„·ì§¸, íŠ¹ìˆ˜ë¬¸ì(!@#$ ë“±)ë¥¼ ìµœì†Œ 1ê°œ ì´ìƒ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ìš”êµ¬ì‚¬í•­ì€ NIST ê°€ì´ë“œë¼ì¸ì—\n",
      "    ê¸°ë°˜í•˜ë©°, ì¡°ì§ì˜ ì •ë³´ ë³´ì•ˆì„ ê°•í™”í•˜ê¸° ìœ„í•´ í•„ìˆ˜ì ì…ë‹ˆë‹¤.\n",
      "\n",
      "ê¸°ëŒ€ ê²°ê³¼: ë‘ ë‹µë³€ ëª¨ë‘ ë™ì¼í•œ ì •ë³´ â†’ ë™ì¼í•œ ì ìˆ˜ (í¸í–¥ ì—†ìŒ)\n"
     ]
    }
   ],
   "source": [
    "# ì¥í™©í•¨ í¸í–¥ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤\n",
    "verbosity_test = {\n",
    "    \"question\": \"ë¹„ë°€ë²ˆí˜¸ ì •ì±…ì´ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "    \"context\": \"ë¹„ë°€ë²ˆí˜¸ëŠ” ìµœì†Œ 12ì ì´ìƒ, ëŒ€ì†Œë¬¸ì, ìˆ«ì, íŠ¹ìˆ˜ë¬¸ìë¥¼ ê°ê° 1ê°œ ì´ìƒ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.\",\n",
    "    \"short_answer\": \"ë¹„ë°€ë²ˆí˜¸ëŠ” 12ì ì´ìƒ, ëŒ€ì†Œë¬¸ì+ìˆ«ì+íŠ¹ìˆ˜ë¬¸ì í•„ìš”\",\n",
    "    \"long_answer\": \"\"\"ë¹„ë°€ë²ˆí˜¸ ì •ì±…ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì²«ì§¸, ê¸¸ì´ëŠ” ìµœì†Œ 12ì ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "    ì´ëŠ” ë¸Œë£¨íŠ¸í¬ìŠ¤ ê³µê²©ì„ ë°©ì§€í•˜ê¸° ìœ„í•œ ì¤‘ìš”í•œ ë³´ì•ˆ ì¡°ì¹˜ì…ë‹ˆë‹¤. ë‘˜ì§¸, ëŒ€ë¬¸ìì™€ ì†Œë¬¸ìë¥¼ í˜¼í•©í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "    ì´ë¥¼ í†µí•´ ë¹„ë°€ë²ˆí˜¸ì˜ ë³µì¡ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì…‹ì§¸, ìˆ«ìë¥¼ ìµœì†Œ 1ê°œ ì´ìƒ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "    ë„·ì§¸, íŠ¹ìˆ˜ë¬¸ì(!@#$ ë“±)ë¥¼ ìµœì†Œ 1ê°œ ì´ìƒ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ìš”êµ¬ì‚¬í•­ì€ NIST ê°€ì´ë“œë¼ì¸ì—\n",
    "    ê¸°ë°˜í•˜ë©°, ì¡°ì§ì˜ ì •ë³´ ë³´ì•ˆì„ ê°•í™”í•˜ê¸° ìœ„í•´ í•„ìˆ˜ì ì…ë‹ˆë‹¤.\"\"\",\n",
    "    \"expected\": \"ë‘ ë‹µë³€ ëª¨ë‘ ë™ì¼í•œ ì •ë³´ â†’ ë™ì¼í•œ ì ìˆ˜ (í¸í–¥ ì—†ìŒ)\",\n",
    "}\n",
    "\n",
    "# ìœ„ì¹˜ í¸í–¥ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤\n",
    "position_test = {\n",
    "    \"question\": \"VPN ì—°ê²°ì´ ì•ˆ ë©ë‹ˆë‹¤. í•´ê²° ë°©ë²•ì€?\",\n",
    "    \"answer_A\": \"1) ë„¤íŠ¸ì›Œí¬ ì„¤ì • í™•ì¸ 2) VPN í´ë¼ì´ì–¸íŠ¸ ì¬ì‹œì‘ 3) IT ì§€ì›íŒ€ ë¬¸ì˜\",\n",
    "    \"answer_B\": \"1) ë„¤íŠ¸ì›Œí¬ ì„¤ì • í™•ì¸ 2) VPN í´ë¼ì´ì–¸íŠ¸ ì¬ì‹œì‘ 3) IT ì§€ì›íŒ€ ë¬¸ì˜\",\n",
    "    \"test\": \"ë™ì¼í•œ ë‹µë³€ì„ ìˆœì„œë§Œ ë°”ê¿” í‰ê°€ â†’ ì ìˆ˜ ì°¨ì´ í™•ì¸\",\n",
    "    \"expected\": \"ìˆœì„œì™€ ë¬´ê´€í•˜ê²Œ ë™ì¼í•œ ì ìˆ˜ (í¸í–¥ ì—†ìŒ)\",\n",
    "}\n",
    "\n",
    "print(\"\\n[Verbosity Bias Test]\")\n",
    "print(f\"ì§§ì€ ë‹µë³€: {verbosity_test['short_answer']}\")\n",
    "print(f\"\\nê¸´ ë‹µë³€: {verbosity_test['long_answer']}\")\n",
    "print(f\"\\nê¸°ëŒ€ ê²°ê³¼: {verbosity_test['expected']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## ì•™ìƒë¸” í‰ê°€ ì „ëµ\n",
    "\n",
    "### Strategy 1: Majority (ë‹¤ìˆ˜ê²°)\n",
    "- 3ê°œ ì´ìƒì˜ Judgeê°€ í‰ê°€\n",
    "- ê³¼ë°˜ìˆ˜ ì˜ê²¬ ì±„íƒ\n",
    "- ì¥ì : ê°œë³„ Judgeì˜ ëœë¤ ì˜¤ë¥˜ ì™„í™”\n",
    "\n",
    "### Strategy 2: Minority-Veto (ì†Œìˆ˜ ê±°ë¶€ê¶Œ)\n",
    "- í•œ ëª…ì´ë¼ë„ ì‹¤íŒ¨ íŒì • â†’ ì „ì²´ ì‹¤íŒ¨\n",
    "- ë™ì˜ í¸í–¥ (Agreement Bias) ì™„í™”\n",
    "- ì•ˆì „ì„± ì¤‘ì‹œ ì‹œìŠ¤í…œì— ì í•©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T07:12:12.706033Z",
     "iopub.status.busy": "2025-10-30T07:12:12.705902Z",
     "iopub.status.idle": "2025-10-30T07:12:12.710465Z",
     "shell.execute_reply": "2025-10-30T07:12:12.710025Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class JudgeResult:\n",
    "    \"\"\"ê°œë³„ Judge í‰ê°€ ê²°ê³¼\"\"\"\n",
    "\n",
    "    judge_id: str\n",
    "    passed: bool\n",
    "    score: float\n",
    "    reasoning: str\n",
    "\n",
    "\n",
    "class EnsembleLLMJudge:\n",
    "    \"\"\"ì•™ìƒë¸” LLM Judge í‰ê°€ê¸°\"\"\"\n",
    "\n",
    "    # ë£¨ë¸Œë¦­ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "    RUBRIC_PROMPT = \"\"\"\n",
    "ì•„ë˜ ë£¨ë¸Œë¦­ì— ë”°ë¼ ë‹µë³€ì„ í‰ê°€í•˜ì„¸ìš”:\n",
    "\n",
    "**ì ìˆ˜ ê¸°ì¤€**:\n",
    "- 4ì : ì»¨í…ìŠ¤íŠ¸ ê·¼ê±° ì™„ë²½, ëª¨ë“  ì£¼ì¥ ì§€ì§€ë¨\n",
    "- 3ì : ëŒ€ë¶€ë¶„ ê·¼ê±° ì¶©ë¶„, ì†Œìˆ˜ ë¯¸ì§€ì› (1-2ê°œ)\n",
    "- 2ì : ì¼ë¶€ ê·¼ê±° ìˆìœ¼ë‚˜ ë¶€ì¡± (50% ë¯¸ë§Œ ì§€ì§€)\n",
    "- 1ì : ì»¨í…ìŠ¤íŠ¸ ê·¼ê±° ê±°ì˜ ì—†ìŒ\n",
    "\n",
    "**ë¬´ì‹œí•  ìš”ì†Œ** (í¸í–¥ ë°©ì§€):\n",
    "- ë‹µë³€ ê¸¸ì´ (ì§§ì•„ë„ ì •í™•í•˜ë©´ ë†’ì€ ì ìˆ˜)\n",
    "- ë¬¸ì²´ (ê°„ê²°í•œ ë‹µë³€ë„ ë™ì¼ í‰ê°€)\n",
    "- ì–´ì¡° (ì¤‘ë¦½ì  vs ê¸ì •ì )\n",
    "- ì •êµí•¨ (ë¶ˆí•„ìš”í•œ ë¶€ì—° ì„¤ëª…)\n",
    "\n",
    "**í‰ê°€ ê³¼ì •**:\n",
    "1. ì»¨í…ìŠ¤íŠ¸ì—ì„œ ê° ì£¼ì¥ì˜ ê·¼ê±° í™•ì¸\n",
    "2. ì§€ì§€ë˜ëŠ” ì£¼ì¥ ë¹„ìœ¨ ê³„ì‚°\n",
    "3. ë£¨ë¸Œë¦­ ê¸°ì¤€ìœ¼ë¡œ ì ìˆ˜ í• ë‹¹\n",
    "4. ê·¼ê±°ì™€ í•¨ê»˜ ì ìˆ˜ ë°˜í™˜\n",
    "\n",
    "<data>\n",
    "ì§ˆë¬¸: {question}\n",
    "---\n",
    "ë‹µë³€: {answer}\n",
    "---\n",
    "ì»¨í…ìŠ¤íŠ¸: {context}\n",
    "</data>\n",
    "\n",
    "ì¶œë ¥ í˜•ì‹ (JSON):\n",
    "{{\n",
    "  \"score\": <1-4>,\n",
    "  \"reasoning\": \"<í‰ê°€ ê·¼ê±°>\",\n",
    "  \"supported_claims\": [\"<ì§€ì§€ëœ ì£¼ì¥ ëª©ë¡>\"],\n",
    "  \"unsupported_claims\": [\"<ë¯¸ì§€ì› ì£¼ì¥ ëª©ë¡>\"]\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    def __init__(self, judge_models: List[str], strategy: Literal[\"majority\", \"minority_veto\"]):\n",
    "        self.judge_models = judge_models\n",
    "        self.strategy = strategy\n",
    "\n",
    "    def evaluate(self, question: str, answer: str, context: str) -> Dict[str, Any]:\n",
    "        \"\"\"ì•™ìƒë¸” í‰ê°€ ìˆ˜í–‰\"\"\"\n",
    "        judge_results = []\n",
    "\n",
    "        # ê° judge ëª¨ë¸ë¡œ í‰ê°€ ìˆ˜í–‰\n",
    "        for model_name in self.judge_models:\n",
    "            try:\n",
    "                # LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "                llm = create_openrouter_llm(model=model_name, temperature=0.0)\n",
    "\n",
    "                # í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…\n",
    "                prompt = self.RUBRIC_PROMPT.format(\n",
    "                    question=question, answer=answer, context=context\n",
    "                )\n",
    "\n",
    "                # LLM í˜¸ì¶œ\n",
    "                response = llm.invoke(prompt)\n",
    "                response_text = response.content\n",
    "\n",
    "                # JSON íŒŒì‹±\n",
    "                # LLMì´ ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ìœ¼ë¡œ ê°ì‹¸ëŠ” ê²½ìš° ì²˜ë¦¬\n",
    "                if \"```json\" in response_text:\n",
    "                    response_text = response_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "                elif \"```\" in response_text:\n",
    "                    response_text = response_text.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "\n",
    "                result_dict = json.loads(response_text)\n",
    "\n",
    "                # ì ìˆ˜ ì¶”ì¶œ (1-4 ì²™ë„)\n",
    "                score = float(result_dict.get(\"score\", 0))\n",
    "                reasoning = result_dict.get(\"reasoning\", \"No reasoning provided\")\n",
    "\n",
    "                # í†µê³¼ ì—¬ë¶€ ê²°ì • (ì ìˆ˜ 3 ì´ìƒì´ë©´ í†µê³¼)\n",
    "                passed = score >= 3.0\n",
    "\n",
    "                # ì •ê·œí™”ëœ ì ìˆ˜ (0-1 ë²”ìœ„ë¡œ ë³€í™˜)\n",
    "                normalized_score = score / 4.0\n",
    "\n",
    "                # JudgeResult ìƒì„±\n",
    "                judge_result = JudgeResult(\n",
    "                    judge_id=model_name,\n",
    "                    passed=passed,\n",
    "                    score=normalized_score,\n",
    "                    reasoning=reasoning,\n",
    "                )\n",
    "                judge_results.append(judge_result)\n",
    "\n",
    "                print(f\"âœ“ {model_name}: Score={score}/4 ({'í†µê³¼' if passed else 'ì‹¤íŒ¨'})\")\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"âš ï¸ {model_name}: JSON íŒŒì‹± ì‹¤íŒ¨ - {e}\")\n",
    "                # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©\n",
    "                judge_results.append(\n",
    "                    JudgeResult(\n",
    "                        judge_id=model_name,\n",
    "                        passed=False,\n",
    "                        score=0.0,\n",
    "                        reasoning=f\"JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}\",\n",
    "                    )\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ {model_name}: í‰ê°€ ì‹¤íŒ¨ - {e}\")\n",
    "                # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©\n",
    "                judge_results.append(\n",
    "                    JudgeResult(\n",
    "                        judge_id=model_name,\n",
    "                        passed=False,\n",
    "                        score=0.0,\n",
    "                        reasoning=f\"í‰ê°€ ì‹¤íŒ¨: {str(e)}\",\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # ì•™ìƒë¸” ì „ëµ ì ìš©\n",
    "        if self.strategy == \"minority_veto\":\n",
    "            # í•œ ëª…ì´ë¼ë„ ì‹¤íŒ¨ â†’ ì „ì²´ ì‹¤íŒ¨\n",
    "            final_passed = all(jr.passed for jr in judge_results)\n",
    "        else:  # majority\n",
    "            # ê³¼ë°˜ìˆ˜ í†µê³¼ â†’ ì „ì²´ í†µê³¼\n",
    "            passed_count = sum(1 for jr in judge_results if jr.passed)\n",
    "            final_passed = passed_count > len(judge_results) / 2\n",
    "\n",
    "        avg_score = sum(jr.score for jr in judge_results) / len(judge_results)\n",
    "\n",
    "        return {\n",
    "            \"passed\": final_passed,\n",
    "            \"average_score\": avg_score,\n",
    "            \"strategy\": self.strategy,\n",
    "            \"judge_results\": judge_results,\n",
    "            \"agreement_rate\": sum(1 for jr in judge_results if jr.passed == final_passed)\n",
    "            / len(judge_results),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## ì•™ìƒë¸” ì „ëµ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T07:12:12.711653Z",
     "iopub.status.busy": "2025-10-30T07:12:12.711573Z",
     "iopub.status.idle": "2025-10-30T07:12:12.714997Z",
     "shell.execute_reply": "2025-10-30T07:12:12.714659Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ openai/gpt-4.1: Score=4.0/4 (í†µê³¼)\n",
      "âœ“ anthropic/claude-sonnet-4.5: Score=4.0/4 (í†µê³¼)\n",
      "âœ“ openai/gpt-4.1: Score=4.0/4 (í†µê³¼)\n",
      "âœ“ anthropic/claude-sonnet-4.5: Score=4.0/4 (í†µê³¼)\n",
      "ì•™ìƒë¸” ì „ëµ ë¹„êµ (ì‹œë®¬ë ˆì´ì…˜)\n",
      "\n",
      "[Strategy 1: Majority]\n",
      "ìµœì¢… ê²°ê³¼: í†µê³¼\n",
      "í‰ê·  ì ìˆ˜: 1.000\n",
      "Judgeë³„ ê²°ê³¼:\n",
      "  - openai/gpt-4.1: í†µê³¼ (ì ìˆ˜: 1.000)\n",
      "  - anthropic/claude-sonnet-4.5: í†µê³¼ (ì ìˆ˜: 1.000)\n",
      "\n",
      "[Strategy 2: Minority-Veto]\n",
      "ìµœì¢… ê²°ê³¼: í†µê³¼\n",
      "í‰ê·  ì ìˆ˜: 1.000\n",
      "Judgeë³„ ê²°ê³¼:\n",
      "  - openai/gpt-4.1: í†µê³¼ (ì ìˆ˜: 1.000)\n",
      "  - anthropic/claude-sonnet-4.5: í†µê³¼ (ì ìˆ˜: 1.000)\n",
      "\n",
      "ğŸ’¡ ì „ëµ ì„ íƒ ê°€ì´ë“œ:\n",
      "  - Majority: ì¼ë°˜ì ì¸ í‰ê°€, ë¹„ìš© íš¨ìœ¨ì \n",
      "  - Minority-Veto: ì•ˆì „ì„± ì¤‘ì‹œ, False Negative ìµœì†Œí™”\n"
     ]
    }
   ],
   "source": [
    "# ì‹œë®¬ë ˆì´ì…˜ ì„¤ì •\n",
    "judge_models = [\"openai/gpt-4.1\", \"anthropic/claude-sonnet-4.5\"]\n",
    "\n",
    "# Strategy 1: Majority\n",
    "majority_judge = EnsembleLLMJudge(judge_models, strategy=\"majority\")\n",
    "majority_result = majority_judge.evaluate(\n",
    "    question=verbosity_test[\"question\"],\n",
    "    answer=verbosity_test[\"short_answer\"],\n",
    "    context=verbosity_test[\"context\"],\n",
    ")\n",
    "\n",
    "# Strategy 2: Minority-Veto\n",
    "veto_judge = EnsembleLLMJudge(judge_models, strategy=\"minority_veto\")\n",
    "veto_result = veto_judge.evaluate(\n",
    "    question=verbosity_test[\"question\"],\n",
    "    answer=verbosity_test[\"short_answer\"],\n",
    "    context=verbosity_test[\"context\"],\n",
    ")\n",
    "\n",
    "print(\"[Strategy 1: Majority]\")\n",
    "print(\"Majority: ì¼ë°˜ì ì¸ í‰ê°€, ë¹„ìš© íš¨ìœ¨ì \")\n",
    "print(f\"ìµœì¢… ê²°ê³¼: {'í†µê³¼' if majority_result['passed'] else 'ì‹¤íŒ¨'}\")\n",
    "print(f\"í‰ê·  ì ìˆ˜: {majority_result['average_score']:.3f}\")\n",
    "print(\"Judgeë³„ ê²°ê³¼:\")\n",
    "for jr in majority_result[\"judge_results\"]:\n",
    "    print(f\"  - {jr.judge_id}: {'í†µê³¼' if jr.passed else 'ì‹¤íŒ¨'} (ì ìˆ˜: {jr.score:.3f})\")\n",
    "\n",
    "print(\"\\n[Strategy 2: Minority-Veto]\")\n",
    "print(\"Minority-Veto: ì•ˆì „ì„± ì¤‘ì‹œ, False Negative ìµœì†Œí™”\")\n",
    "print(f\"ìµœì¢… ê²°ê³¼: {'í†µê³¼' if veto_result['passed'] else 'ì‹¤íŒ¨'}\")\n",
    "print(f\"í‰ê·  ì ìˆ˜: {veto_result['average_score']:.3f}\")\n",
    "print(\"Judgeë³„ ê²°ê³¼:\")\n",
    "for jr in veto_result[\"judge_results\"]:\n",
    "    print(f\"  - {jr.judge_id}: {'í†µê³¼' if jr.passed else 'ì‹¤íŒ¨'} (ì ìˆ˜: {jr.score:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 6. ë£¨ë¸Œë¦­ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ì„¤ê³„\n",
    "\n",
    "í¸í–¥ì„ ì™„í™”í•˜ëŠ” í•µì‹¬ ì „ëµì€ ëª…í™•í•œ í‰ê°€ ê¸°ì¤€ (Rubric)ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T07:12:12.716443Z",
     "iopub.status.busy": "2025-10-30T07:12:12.716346Z",
     "iopub.status.idle": "2025-10-30T07:12:12.718394Z",
     "shell.execute_reply": "2025-10-30T07:12:12.717968Z"
    }
   },
   "outputs": [],
   "source": [
    "# ë£¨ë¸Œë¦­ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "rubric_prompt_template = \"\"\"\n",
    "ì•„ë˜ ë£¨ë¸Œë¦­ì— ë”°ë¼ ë‹µë³€ì„ í‰ê°€í•˜ì„¸ìš”:\n",
    "\n",
    "**ì ìˆ˜ ê¸°ì¤€**:\n",
    "- 4ì : ì»¨í…ìŠ¤íŠ¸ ê·¼ê±° ì™„ë²½, ëª¨ë“  ì£¼ì¥ ì§€ì§€ë¨\n",
    "- 3ì : ëŒ€ë¶€ë¶„ ê·¼ê±° ì¶©ë¶„, ì†Œìˆ˜ ë¯¸ì§€ì› (1-2ê°œ)\n",
    "- 2ì : ì¼ë¶€ ê·¼ê±° ìˆìœ¼ë‚˜ ë¶€ì¡± (50% ë¯¸ë§Œ ì§€ì§€)\n",
    "- 1ì : ì»¨í…ìŠ¤íŠ¸ ê·¼ê±° ê±°ì˜ ì—†ìŒ\n",
    "\n",
    "**ë¬´ì‹œí•  ìš”ì†Œ** (í¸í–¥ ë°©ì§€):\n",
    "- âŒ ë‹µë³€ ê¸¸ì´ (ì§§ì•„ë„ ì •í™•í•˜ë©´ ë†’ì€ ì ìˆ˜)\n",
    "- âŒ ë¬¸ì²´ (ê°„ê²°í•œ ë‹µë³€ë„ ë™ì¼ í‰ê°€)\n",
    "- âŒ ì–´ì¡° (ì¤‘ë¦½ì  vs ê¸ì •ì )\n",
    "- âŒ ì •êµí•¨ (ë¶ˆí•„ìš”í•œ ë¶€ì—° ì„¤ëª…)\n",
    "\n",
    "**í‰ê°€ ê³¼ì •**:\n",
    "1. ì»¨í…ìŠ¤íŠ¸ì—ì„œ ê° ì£¼ì¥ì˜ ê·¼ê±° í™•ì¸\n",
    "2. ì§€ì§€ë˜ëŠ” ì£¼ì¥ ë¹„ìœ¨ ê³„ì‚°\n",
    "3. ë£¨ë¸Œë¦­ ê¸°ì¤€ìœ¼ë¡œ ì ìˆ˜ í• ë‹¹\n",
    "4. ê·¼ê±°ì™€ í•¨ê»˜ ì ìˆ˜ ë°˜í™˜\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "ë‹µë³€: {answer}\n",
    "ì»¨í…ìŠ¤íŠ¸: {context}\n",
    "\n",
    "ì¶œë ¥ í˜•ì‹ (JSON):\n",
    "{{\n",
    "  \"score\": <1-4>,\n",
    "  \"reasoning\": \"<í‰ê°€ ê·¼ê±°>\",\n",
    "  \"supported_claims\": [\"<ì§€ì§€ëœ ì£¼ì¥ ëª©ë¡>\"],\n",
    "  \"unsupported_claims\": [\"<ë¯¸ì§€ì› ì£¼ì¥ ëª©ë¡>\"]\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## ë£¨ë¸Œë¦­ í”„ë¡¬í”„íŠ¸ ì ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T07:12:12.719541Z",
     "iopub.status.busy": "2025-10-30T07:12:12.719458Z",
     "iopub.status.idle": "2025-10-30T07:12:12.723064Z",
     "shell.execute_reply": "2025-10-30T07:12:12.722642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ë£¨ë¸Œë¦­ ê¸°ë°˜ í‰ê°€ ê²°ê³¼ (Verbosity Bias Test)\n",
      "\n",
      "[ì§§ì€ ë‹µë³€]\n",
      "ì ìˆ˜: 4/4\n",
      "ê·¼ê±°: ë‹µë³€ì˜ ëª¨ë“  ì£¼ì¥ì€ ì»¨í…ìŠ¤íŠ¸ì— ì˜í•´ ì™„ë²½í•˜ê²Œ ì§€ì§€ëœë‹¤. ë‹µë³€ì˜ ê° í•­ëª©(12ì ì´ìƒ, ëŒ€ì†Œë¬¸ì í¬í•¨, ìˆ«ì í¬í•¨, íŠ¹ìˆ˜ë¬¸ì í¬í•¨)ì´ ëª¨ë‘ ì»¨í…ìŠ¤íŠ¸ì— ëª…ì‹œë˜ì–´ ìˆë‹¤. ëˆ„ë½ë˜ê±°ë‚˜ ì»¨í…ìŠ¤íŠ¸ì™€ ì–´ê¸‹ë‚œ ì£¼ì¥ì€ ì—†ë‹¤.\n",
      "ì§€ì§€ëœ ì£¼ì¥: ['ë¹„ë°€ë²ˆí˜¸ëŠ” 12ì ì´ìƒì´ì–´ì•¼ í•œë‹¤', 'ëŒ€ë¬¸ìì™€ ì†Œë¬¸ìë¥¼ í¬í•¨í•´ì•¼ í•œë‹¤', 'ìˆ«ìë¥¼ í¬í•¨í•´ì•¼ í•œë‹¤', 'íŠ¹ìˆ˜ë¬¸ìë¥¼ í¬í•¨í•´ì•¼ í•œë‹¤']\n",
      "\n",
      "[ê¸´ ë‹µë³€]\n",
      "ì ìˆ˜: 4/4\n",
      "ê·¼ê±°: ë‹µë³€ì˜ ëª¨ë“  ì£¼ì¥ì€ ì»¨í…ìŠ¤íŠ¸ì— ëª…ì‹œëœ ì¡°ê±´(ìµœì†Œ 12ì, ëŒ€ì†Œë¬¸ì, ìˆ«ì, íŠ¹ìˆ˜ë¬¸ì ê° 1ê°œ ì´ìƒ í¬í•¨)ê³¼ ì¼ì¹˜í•˜ë©°, ëª¨ë‘ ê·¼ê±°ë¥¼ ê°–ì¶”ê³  ìˆìŠµë‹ˆë‹¤. ì»¨í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ìš”êµ¬ì‚¬í•­ì„ ëª¨ë‘ ë°˜ì˜í•˜ì—¬ 100% ì§€ì§€ë©ë‹ˆë‹¤.\n",
      "ì§€ì§€ëœ ì£¼ì¥: ['ê¸¸ì´ëŠ” ìµœì†Œ 12ì ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.', 'ëŒ€ë¬¸ìì™€ ì†Œë¬¸ìë¥¼ í˜¼í•©í•´ì•¼ í•©ë‹ˆë‹¤.', 'ìˆ«ìë¥¼ ìµœì†Œ 1ê°œ ì´ìƒ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.', 'íŠ¹ìˆ˜ë¬¸ìë¥¼ ìµœì†Œ 1ê°œ ì´ìƒ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.']\n",
      "  í¸í–¥ ì—†ìŒ: ë‹µë³€ ê¸¸ì´ì™€ ë¬´ê´€í•˜ê²Œ ë™ì¼í•œ ì ìˆ˜\n"
     ]
    }
   ],
   "source": [
    "def evaluate_with_rubric(\n",
    "    question: str,\n",
    "    answer: str,\n",
    "    context: str,\n",
    "    model: str = \"openai/gpt-4.1-mini\",\n",
    "    temperature: float = 0.0,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"ë£¨ë¸Œë¦­ ê¸°ë°˜ LLM í‰ê°€\n",
    "\n",
    "    Args:\n",
    "        question: í‰ê°€í•  ì§ˆë¬¸\n",
    "        answer: í‰ê°€í•  ë‹µë³€\n",
    "        context: ì°¸ì¡° ì»¨í…ìŠ¤íŠ¸\n",
    "        model: ì‚¬ìš©í•  LLM ëª¨ë¸ (ê¸°ë³¸ê°’: openai/gpt-4.1-mini)\n",
    "        temperature: LLM ì˜¨ë„ (ê¸°ë³¸ê°’: 0.0)\n",
    "\n",
    "    Returns:\n",
    "        í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ (score, reasoning, supported_claims, unsupported_claims)\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "        llm = create_openrouter_llm(model=model, temperature=temperature)\n",
    "\n",
    "        # í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…\n",
    "        prompt = rubric_prompt_template.format(question=question, answer=answer, context=context)\n",
    "\n",
    "        # LLM í˜¸ì¶œ\n",
    "        response = llm.invoke(prompt)\n",
    "        response_text = response.content\n",
    "\n",
    "        # JSON íŒŒì‹± (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì²˜ë¦¬)\n",
    "        if \"```json\" in response_text:\n",
    "            response_text = response_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in response_text:\n",
    "            response_text = response_text.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "\n",
    "        result_dict = json.loads(response_text)\n",
    "\n",
    "        # ì ìˆ˜ ì¶”ì¶œ ë° ë°˜í™˜\n",
    "        return {\n",
    "            \"score\": int(result_dict.get(\"score\", 0)),\n",
    "            \"reasoning\": result_dict.get(\"reasoning\", \"No reasoning provided\"),\n",
    "            \"supported_claims\": result_dict.get(\"supported_claims\", []),\n",
    "            \"unsupported_claims\": result_dict.get(\"unsupported_claims\", []),\n",
    "        }\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
    "        print(f\"ì‘ë‹µ ë‚´ìš©: {response_text[:200]}...\")\n",
    "        # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜\n",
    "        return {\n",
    "            \"score\": 0,\n",
    "            \"reasoning\": f\"JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}\",\n",
    "            \"supported_claims\": [],\n",
    "            \"unsupported_claims\": [],\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"í‰ê°€ ì‹¤íŒ¨: {e}\")\n",
    "        # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜\n",
    "        return {\n",
    "            \"score\": 0,\n",
    "            \"reasoning\": f\"í‰ê°€ ì‹¤íŒ¨: {str(e)}\",\n",
    "            \"supported_claims\": [],\n",
    "            \"unsupported_claims\": [],\n",
    "        }\n",
    "\n",
    "\n",
    "# ì§§ì€ ë‹µë³€ í‰ê°€ (LLM ê¸°ë°˜)\n",
    "short_eval = evaluate_with_rubric(\n",
    "    question=verbosity_test[\"question\"],\n",
    "    answer=verbosity_test[\"short_answer\"],\n",
    "    context=verbosity_test[\"context\"],\n",
    "    model=\"openai/gpt-4.1\",\n",
    ")\n",
    "\n",
    "# ê¸´ ë‹µë³€ í‰ê°€ (LLM ê¸°ë°˜)\n",
    "long_eval = evaluate_with_rubric(\n",
    "    question=verbosity_test[\"question\"],\n",
    "    answer=verbosity_test[\"long_answer\"],\n",
    "    context=verbosity_test[\"context\"],\n",
    "    model=\"openai/gpt-4.1\",\n",
    ")\n",
    "\n",
    "print(\"\\në£¨ë¸Œë¦­ ê¸°ë°˜ í‰ê°€ ê²°ê³¼ (Verbosity Bias Test)\\n\")\n",
    "\n",
    "print(\"[ì§§ì€ ë‹µë³€]\")\n",
    "print(f\"ì ìˆ˜: {short_eval['score']}/4\")\n",
    "print(f\"ê·¼ê±°: {short_eval['reasoning']}\")\n",
    "print(f\"ì§€ì§€ëœ ì£¼ì¥: {short_eval['supported_claims']}\")\n",
    "\n",
    "print(\"\\n[ê¸´ ë‹µë³€]\")\n",
    "print(f\"ì ìˆ˜: {long_eval['score']}/4\")\n",
    "print(f\"ê·¼ê±°: {long_eval['reasoning']}\")\n",
    "print(f\"ì§€ì§€ëœ ì£¼ì¥: {long_eval['supported_claims']}\")\n",
    "\n",
    "\n",
    "if short_eval[\"score\"] == long_eval[\"score\"]:\n",
    "    print(\"  í¸í–¥ ì—†ìŒ: ë‹µë³€ ê¸¸ì´ì™€ ë¬´ê´€í•˜ê²Œ ë™ì¼í•œ ì ìˆ˜\")\n",
    "else:\n",
    "    diff = abs(short_eval[\"score\"] - long_eval[\"score\"])\n",
    "    bias_direction = (\n",
    "        \"ê¸´ ë‹µë³€ ì„ í˜¸\" if long_eval[\"score\"] > short_eval[\"score\"] else \"ì§§ì€ ë‹µë³€ ì„ í˜¸\"\n",
    "    )\n",
    "    print(f\"  í¸í–¥ ê°ì§€: ì ìˆ˜ ì°¨ì´ {diff}ì  ({bias_direction}, Verbosity Bias ê°€ëŠ¥ì„±)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ í¸í–¥ ì™„í™” ê¸°ë²•\n",
    "\n",
    "### 1. ë¸”ë¼ì¸ë“œ í‰ê°€ (Blind Evaluation)\n",
    "- ë‹µë³€ ìˆœì„œ ëœë¤í™”\n",
    "- ìœ„ì¹˜ í¸í–¥ (Position Bias) ì™„í™”\n",
    "\n",
    "### 2. ë³´ì • (Calibration)\n",
    "- ì¸ê°„ í‰ê°€ ë°ì´í„°ì…‹ìœ¼ë¡œ Judge ë³´ì •\n",
    "- ì ìˆ˜ ë²”ìœ„ í¸í–¥ (Score Range Bias) ì™„í™”\n",
    "\n",
    "### 3. Contrastive Decoding\n",
    "- ê°•í•œ ëª¨ë¸ê³¼ ì•½í•œ ëª¨ë¸ì˜ ì ìˆ˜ ì°¨ì´ í™œìš©\n",
    "- ìê¸° ê°•í™” í¸í–¥ (Self-Enhancement Bias) ì™„í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T07:12:12.724151Z",
     "iopub.status.busy": "2025-10-30T07:12:12.724080Z",
     "iopub.status.idle": "2025-10-30T07:12:12.727005Z",
     "shell.execute_reply": "2025-10-30T07:12:12.726678Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ë‹µë³€ 0: ì ìˆ˜ 0.711\n",
      "  ë‹µë³€ 1: ì ìˆ˜ 0.689\n",
      "  ë‹µë³€ 2: ì ìˆ˜ 0.624\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import Callable, Optional\n",
    "\n",
    "\n",
    "def create_llm_evaluator(\n",
    "    question: str,\n",
    "    context: str,\n",
    "    model: str = \"openai/gpt-4.1-mini\",\n",
    "    temperature: float = 0.0,\n",
    ") -> Callable[[str], float]:\n",
    "    \"\"\"LLM ê¸°ë°˜ í‰ê°€ í•¨ìˆ˜ ìƒì„±\n",
    "\n",
    "    Args:\n",
    "        question: í‰ê°€í•  ì§ˆë¬¸\n",
    "        context: ì°¸ì¡° ì»¨í…ìŠ¤íŠ¸\n",
    "        model: ì‚¬ìš©í•  LLM ëª¨ë¸\n",
    "        temperature: LLM ì˜¨ë„\n",
    "\n",
    "    Returns:\n",
    "        ë‹µë³€ì„ ë°›ì•„ ì ìˆ˜(0-1)ë¥¼ ë°˜í™˜í•˜ëŠ” í‰ê°€ í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    evaluation_prompt_template = \"\"\"\n",
    "You are an objective judge evaluating answer quality against a reference context.\n",
    "\n",
    "EVALUATION CRITERIA:\n",
    "- Score 1.0: Perfect alignment with context, all claims supported\n",
    "- Score 0.75: Mostly accurate, minor unsupported claims (1-2)\n",
    "- Score 0.5: Partially accurate, significant unsupported content\n",
    "- Score 0.25: Mostly inaccurate or irrelevant\n",
    "- Score 0.0: Completely incorrect or no alignment\n",
    "\n",
    "BIAS PREVENTION:\n",
    "- Ignore answer length (short accurate answers deserve high scores)\n",
    "- Ignore writing style (concise vs elaborate)\n",
    "- Ignore tone (neutral vs positive)\n",
    "- Focus ONLY on factual accuracy against the context\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer: {answer}\n",
    "\n",
    "Output format (JSON):\n",
    "{{\n",
    "  \"score\": <0.0-1.0>,\n",
    "  \"reasoning\": \"<brief explanation>\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    def evaluator(answer: str) -> float:\n",
    "        \"\"\"ë‹¨ì¼ ë‹µë³€ í‰ê°€\"\"\"\n",
    "        try:\n",
    "            llm = create_openrouter_llm(model=model, temperature=temperature)\n",
    "            prompt = evaluation_prompt_template.format(\n",
    "                question=question, context=context, answer=answer\n",
    "            )\n",
    "\n",
    "            response = llm.invoke(prompt)\n",
    "            response_text = response.content\n",
    "\n",
    "            # JSON íŒŒì‹±\n",
    "            if \"```json\" in response_text:\n",
    "                response_text = response_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "            elif \"```\" in response_text:\n",
    "                response_text = response_text.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "\n",
    "            result = json.loads(response_text)\n",
    "            score = float(result.get(\"score\", 0.0))\n",
    "\n",
    "            # ì ìˆ˜ ë²”ìœ„ ê²€ì¦\n",
    "            return max(0.0, min(1.0, score))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Evaluation failed - {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    return evaluator\n",
    "\n",
    "\n",
    "def blind_evaluation(\n",
    "    answers: List[str],\n",
    "    question: str,\n",
    "    context: str,\n",
    "    model: str = \"openai/gpt-4.1-mini\",\n",
    "    temperature: float = 0.0,\n",
    "    verbose: bool = True,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"ë¸”ë¼ì¸ë“œ í‰ê°€: ë‹µë³€ ìˆœì„œ ëœë¤í™”í•˜ì—¬ ìœ„ì¹˜ í¸í–¥ ì™„í™”\n",
    "\n",
    "    Args:\n",
    "        answers: í‰ê°€í•  ë‹µë³€ ë¦¬ìŠ¤íŠ¸\n",
    "        question: ì§ˆë¬¸\n",
    "        context: ì°¸ì¡° ì»¨í…ìŠ¤íŠ¸\n",
    "        model: ì‚¬ìš©í•  LLM ëª¨ë¸\n",
    "        temperature: LLM ì˜¨ë„\n",
    "        verbose: ì§„í–‰ ìƒí™© ì¶œë ¥ ì—¬ë¶€\n",
    "\n",
    "    Returns:\n",
    "        ì›ë˜ ìˆœì„œëŒ€ë¡œ ì •ë ¬ëœ í‰ê°€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"Starting blind evaluation for {len(answers)} answers...\")\n",
    "        print(f\"Model: {model}\")\n",
    "\n",
    "    # LLM í‰ê°€ í•¨ìˆ˜ ìƒì„±\n",
    "    evaluator_func = create_llm_evaluator(\n",
    "        question=question, context=context, model=model, temperature=temperature\n",
    "    )\n",
    "\n",
    "    # ì›ë³¸ ì¸ë±ìŠ¤ ê¸°ë¡\n",
    "    indexed_answers = [(i, ans) for i, ans in enumerate(answers)]\n",
    "\n",
    "    # ìˆœì„œ ëœë¤í™” (ìœ„ì¹˜ í¸í–¥ ë°©ì§€)\n",
    "    shuffled = indexed_answers.copy()\n",
    "    random.shuffle(shuffled)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Shuffled evaluation order: {[idx for idx, _ in shuffled]}\")\n",
    "\n",
    "    # í‰ê°€ ìˆ˜í–‰\n",
    "    results = []\n",
    "    for original_idx, answer in shuffled:\n",
    "        if verbose:\n",
    "            print(f\"Evaluating answer #{original_idx}... \", end=\"\")\n",
    "\n",
    "        score = evaluator_func(answer)\n",
    "        results.append({\"original_index\": original_idx, \"answer\": answer, \"score\": score})\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Score: {score:.3f}\")\n",
    "\n",
    "    # ì›ë˜ ìˆœì„œë¡œ ì¬ì •ë ¬\n",
    "    results_sorted = sorted(results, key=lambda x: x[\"original_index\"])\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nBlind evaluation completed\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    return results_sorted\n",
    "\n",
    "\n",
    "# ë¸”ë¼ì¸ë“œ í‰ê°€ ì‹¤í–‰ ì˜ˆì‹œ\n",
    "print(\"## 8. Blind Evaluation (Position Bias Mitigation)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë‹µë³€ ì„¸íŠ¸\n",
    "test_answers = [\n",
    "    verbosity_test[\"short_answer\"],\n",
    "    verbosity_test[\"long_answer\"],\n",
    "    \"ë¹„ë°€ë²ˆí˜¸ ì •ì±…: 12ì ì´ìƒ, ëŒ€ì†Œë¬¸ì í˜¼í•©, ìˆ«ì í¬í•¨, íŠ¹ìˆ˜ë¬¸ì ì‚¬ìš©\",\n",
    "]\n",
    "\n",
    "blind_results = blind_evaluation(\n",
    "    answers=test_answers,\n",
    "    question=verbosity_test[\"question\"],\n",
    "    context=verbosity_test[\"context\"],\n",
    "    model=\"openai/gpt-4.1-mini\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\nResults Summary:\")\n",
    "for result in blind_results:\n",
    "    answer_preview = (\n",
    "        result[\"answer\"][:50] + \"...\" if len(result[\"answer\"]) > 50 else result[\"answer\"]\n",
    "    )\n",
    "    print(f\"Answer #{result['original_index']}: {result['score']:.3f} - {answer_preview}\")\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"- Position Bias Mitigation: Answers evaluated in random order\")\n",
    "print(\"- Consistency Check: Similar answers should receive similar scores\")\n",
    "print(\"- Independence: Evaluation order should not affect scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 9. í¸í–¥ ì ìˆ˜ ì •ëŸ‰í™”\n",
    "\n",
    "í¸í–¥ì˜ ì‹¬ê°ë„ë¥¼ ì¸¡ì •í•˜ëŠ” ë©”íŠ¸ë¦­ì„ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T07:12:12.728075Z",
     "iopub.status.busy": "2025-10-30T07:12:12.728005Z",
     "iopub.status.idle": "2025-10-30T07:12:12.730863Z",
     "shell.execute_reply": "2025-10-30T07:12:12.730526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š í¸í–¥ ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)\n",
      "\n",
      "verbosity_bias: 0.033\n",
      "  âœ… í¸í–¥ ê±°ì˜ ì—†ìŒ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BiasTestCase:\n",
    "    \"\"\"í¸í–¥ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤\"\"\"\n",
    "\n",
    "    question: str\n",
    "    context: str\n",
    "    short_answer: Optional[str] = None\n",
    "    long_answer: Optional[str] = None\n",
    "    answer_A: Optional[str] = None\n",
    "    answer_B: Optional[str] = None\n",
    "    test_type: str = \"verbosity\"  # verbosity, position, etc.\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BiasScore:\n",
    "    \"\"\"í¸í–¥ ì ìˆ˜ ê²°ê³¼\"\"\"\n",
    "\n",
    "    bias_type: str\n",
    "    score: float\n",
    "    severity: str  # \"none\", \"low\", \"medium\", \"high\"\n",
    "    details: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"ì‹¬ê°ë„ ìë™ ê³„ì‚°\"\"\"\n",
    "        if self.score < 0.05:\n",
    "            self.severity = \"none\"\n",
    "        elif self.score < 0.15:\n",
    "            self.severity = \"low\"\n",
    "        elif self.score < 0.30:\n",
    "            self.severity = \"medium\"\n",
    "        else:\n",
    "            self.severity = \"high\"\n",
    "\n",
    "\n",
    "def evaluate_verbosity_bias(\n",
    "    test_case: BiasTestCase,\n",
    "    model: str = \"openai/gpt-4.1-mini\",\n",
    "    temperature: float = 0.0,\n",
    ") -> BiasScore:\n",
    "    \"\"\"ì¥í™©í•¨ í¸í–¥ í‰ê°€\n",
    "\n",
    "    ë™ì¼í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆì§€ë§Œ ê¸¸ì´ê°€ ë‹¤ë¥¸ ë‘ ë‹µë³€ì˜ ì ìˆ˜ ì°¨ì´ë¥¼ ì¸¡ì •\n",
    "\n",
    "    Args:\n",
    "        test_case: í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ (short_answer, long_answer í•„ìš”)\n",
    "        model: í‰ê°€ì— ì‚¬ìš©í•  LLM ëª¨ë¸\n",
    "        temperature: LLM ì˜¨ë„\n",
    "\n",
    "    Returns:\n",
    "        BiasScore ê°ì²´\n",
    "    \"\"\"\n",
    "    if not test_case.short_answer or not test_case.long_answer:\n",
    "        raise ValueError(\"Verbosity bias test requires both short_answer and long_answer\")\n",
    "\n",
    "    print(f\"Evaluating Verbosity Bias...\")\n",
    "    print(f\"Short answer length: {len(test_case.short_answer)} chars\")\n",
    "    print(f\"Long answer length: {len(test_case.long_answer)} chars\")\n",
    "\n",
    "    # í‰ê°€ í•¨ìˆ˜ ìƒì„±\n",
    "    evaluator = create_llm_evaluator(\n",
    "        question=test_case.question, context=test_case.context, model=model, temperature=temperature\n",
    "    )\n",
    "\n",
    "    # ë‘ ë‹µë³€ í‰ê°€\n",
    "    short_score = evaluator(test_case.short_answer)\n",
    "    long_score = evaluator(test_case.long_answer)\n",
    "\n",
    "    # ì ìˆ˜ ì°¨ì´ ê³„ì‚°\n",
    "    diff = abs(short_score - long_score)\n",
    "\n",
    "    print(f\"Short answer score: {short_score:.3f}\")\n",
    "    print(f\"Long answer score: {long_score:.3f}\")\n",
    "    print(f\"Score difference: {diff:.3f}\")\n",
    "\n",
    "    # í¸í–¥ ë°©í–¥ ê²°ì •\n",
    "    bias_direction = \"none\"\n",
    "    if diff > 0.05:\n",
    "        bias_direction = \"favors_long\" if long_score > short_score else \"favors_short\"\n",
    "\n",
    "    return BiasScore(\n",
    "        bias_type=\"verbosity_bias\",\n",
    "        score=diff,\n",
    "        severity=\"\",  # __post_init__ì—ì„œ ìë™ ê³„ì‚°\n",
    "        details={\n",
    "            \"short_score\": short_score,\n",
    "            \"long_score\": long_score,\n",
    "            \"bias_direction\": bias_direction,\n",
    "            \"short_length\": len(test_case.short_answer),\n",
    "            \"long_length\": len(test_case.long_answer),\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_position_bias(\n",
    "    test_case: BiasTestCase,\n",
    "    model: str = \"openai/gpt-4.1-mini\",\n",
    "    temperature: float = 0.0,\n",
    "    num_trials: int = 3,\n",
    ") -> BiasScore:\n",
    "    \"\"\"ìœ„ì¹˜ í¸í–¥ í‰ê°€\n",
    "\n",
    "    ë™ì¼í•œ ë‹µë³€ì„ ë‹¤ë¥¸ ìˆœì„œë¡œ í‰ê°€í•˜ì—¬ ìœ„ì¹˜ í¸í–¥ ì¸¡ì •\n",
    "\n",
    "    Args:\n",
    "        test_case: í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ (answer_A, answer_B í•„ìš”)\n",
    "        model: í‰ê°€ì— ì‚¬ìš©í•  LLM ëª¨ë¸\n",
    "        temperature: LLM ì˜¨ë„\n",
    "        num_trials: ë°˜ë³µ ì‹œí–‰ íšŸìˆ˜\n",
    "\n",
    "    Returns:\n",
    "        BiasScore ê°ì²´\n",
    "    \"\"\"\n",
    "    if not test_case.answer_A or not test_case.answer_B:\n",
    "        raise ValueError(\"Position bias test requires both answer_A and answer_B\")\n",
    "\n",
    "    print(f\"Evaluating Position Bias ({num_trials} trials)...\")\n",
    "\n",
    "    # ì—¬ëŸ¬ ë²ˆ ì‹œí–‰í•˜ì—¬ í‰ê·  ê³„ì‚° (ëœë¤ì„± ì™„í™”)\n",
    "    diffs = []\n",
    "\n",
    "    for trial in range(num_trials):\n",
    "        print(f\"Trial {trial + 1}/{num_trials}... \", end=\"\")\n",
    "\n",
    "        # ë¸”ë¼ì¸ë“œ í‰ê°€ ìˆ˜í–‰\n",
    "        results = blind_evaluation(\n",
    "            answers=[test_case.answer_A, test_case.answer_B],\n",
    "            question=test_case.question,\n",
    "            context=test_case.context,\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        score_A = results[0][\"score\"]\n",
    "        score_B = results[1][\"score\"]\n",
    "        diff = abs(score_A - score_B)\n",
    "        diffs.append(diff)\n",
    "\n",
    "        print(f\"Diff: {diff:.3f}\")\n",
    "\n",
    "    # í‰ê·  ì ìˆ˜ ì°¨ì´\n",
    "    avg_diff = sum(diffs) / len(diffs)\n",
    "\n",
    "    print(f\"Average score difference: {avg_diff:.3f}\")\n",
    "\n",
    "    return BiasScore(\n",
    "        bias_type=\"position_bias\",\n",
    "        score=avg_diff,\n",
    "        severity=\"\",  # __post_init__ì—ì„œ ìë™ ê³„ì‚°\n",
    "        details={\n",
    "            \"num_trials\": num_trials,\n",
    "            \"trial_diffs\": diffs,\n",
    "            \"std_dev\": (sum((d - avg_diff) ** 2 for d in diffs) / len(diffs)) ** 0.5,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def calculate_bias_score(\n",
    "    test_cases: List[BiasTestCase],\n",
    "    model: str = \"openai/gpt-4.1-mini\",\n",
    "    temperature: float = 0.0,\n",
    ") -> Dict[str, BiasScore]:\n",
    "    \"\"\"í¸í–¥ ì ìˆ˜ ê³„ì‚° (í”„ë¡œë•ì…˜ ë²„ì „)\n",
    "\n",
    "    Args:\n",
    "        test_cases: í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¦¬ìŠ¤íŠ¸\n",
    "        model: í‰ê°€ì— ì‚¬ìš©í•  LLM ëª¨ë¸\n",
    "        temperature: LLM ì˜¨ë„\n",
    "\n",
    "    Returns:\n",
    "        í¸í–¥ ìœ í˜•ë³„ BiasScore ë”•ì…”ë„ˆë¦¬\n",
    "    \"\"\"\n",
    "    bias_scores = {}\n",
    "\n",
    "    for test_case in test_cases:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"Test Type: {test_case.test_type}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        try:\n",
    "            if test_case.test_type == \"verbosity\":\n",
    "                bias_score = evaluate_verbosity_bias(test_case, model, temperature)\n",
    "                bias_scores[\"verbosity_bias\"] = bias_score\n",
    "\n",
    "            elif test_case.test_type == \"position\":\n",
    "                bias_score = evaluate_position_bias(test_case, model, temperature)\n",
    "                bias_scores[\"position_bias\"] = bias_score\n",
    "\n",
    "            else:\n",
    "                print(f\"Warning: Unknown test type '{test_case.test_type}', skipping\")\n",
    "                continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {test_case.test_type}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return bias_scores\n",
    "\n",
    "\n",
    "# í”„ë¡œë•ì…˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "print(\"## 9. Bias Score Calculation (Production)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì •ì˜\n",
    "production_test_cases = [\n",
    "    BiasTestCase(\n",
    "        question=verbosity_test[\"question\"],\n",
    "        context=verbosity_test[\"context\"],\n",
    "        short_answer=verbosity_test[\"short_answer\"],\n",
    "        long_answer=verbosity_test[\"long_answer\"],\n",
    "        test_type=\"verbosity\",\n",
    "    ),\n",
    "    BiasTestCase(\n",
    "        question=position_test[\"question\"],\n",
    "        context=\"VPN ì—°ê²° ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ì„œëŠ” ë„¤íŠ¸ì›Œí¬ ì„¤ì • í™•ì¸, VPN í´ë¼ì´ì–¸íŠ¸ ì¬ì‹œì‘, IT ì§€ì›íŒ€ ë¬¸ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.\",\n",
    "        answer_A=position_test[\"answer_A\"],\n",
    "        answer_B=position_test[\"answer_B\"],\n",
    "        test_type=\"position\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# í¸í–¥ ì ìˆ˜ ê³„ì‚°\n",
    "bias_scores = calculate_bias_score(\n",
    "    test_cases=production_test_cases, model=\"openai/gpt-4.1-mini\", temperature=0.0\n",
    ")\n",
    "\n",
    "# ê²°ê³¼ ìš”ì•½\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Bias Score Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for bias_type, bias_score in bias_scores.items():\n",
    "    print(f\"\\n{bias_type.upper()}:\")\n",
    "    print(f\"  Score: {bias_score.score:.3f}\")\n",
    "    print(f\"  Severity: {bias_score.severity}\")\n",
    "\n",
    "    if bias_score.severity == \"none\":\n",
    "        print(\"  Assessment: No significant bias detected\")\n",
    "    elif bias_score.severity == \"low\":\n",
    "        print(\"  Assessment: Minor bias present, acceptable for most use cases\")\n",
    "    elif bias_score.severity == \"medium\":\n",
    "        print(\"  Assessment: Moderate bias detected, consider mitigation strategies\")\n",
    "    else:\n",
    "        print(\"  Assessment: Significant bias detected, mitigation required\")\n",
    "\n",
    "    print(f\"  Details: {bias_score.details}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Production Notes:\")\n",
    "print(\"- All evaluations use actual LLM calls\")\n",
    "print(\"- Blind evaluation employed for position bias\")\n",
    "print(\"- Multiple trials used for statistical robustness\")\n",
    "print(\"- Severity levels guide mitigation decisions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
