# LLM 최적화 기법

대규모 언어 모델(LLM) 최적화 기법:
실제 프로덕션 지표(p95 지연시간, QPS, VRAM, $/1M 토큰)를 개선하는 핵심 레버

학습 단계 최적화: 정밀도 & 체크포인팅 → 샤딩/FSDP/ZeRO → PEFT & 8비트 옵티마이저 → 병렬화 (DP/TP/PP/3D), FlashAttention, 컴파일러, 배칭, 스케줄링, MoE.

추론 단계 최적화: 양자화 → paged-KV + GQA/MQA → 제거/배치 → 연속 배칭, 청크 프리필, 추측적 디코딩, 최적화된 런타임, 프루닝, 증류, 멀티테넌트 어댑터.

LLMOps 단계 최적화: 관찰성, 데이터/평가 위생, 안전/가드레일, 토폴로지/용량 계획.

[사용 방법]
목표를 선택 (p95 감소, QPS 증가, 더 긴 컨텍스트 처리, 비용 절감).
왼쪽→오른쪽 순서로 진행하며 한 번에 하나의 설정만 조정.
변경 전후를 측정하고 품질을 유지하는 개선 사항만 유지.

[이미지]
./LLM Optimization Techniques.jpeg
