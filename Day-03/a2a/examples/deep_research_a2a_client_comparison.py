# ruff: noqa: E402
"""
Deep Research A2A Client ê¸°ë°˜ í’ˆì§ˆ ë° ì„±ëŠ¥ ë¹„êµ ìŠ¤í¬ë¦½íŠ¸

LangGraphì™€ A2A SDK ì „ì²´ ìŠ¤íƒì˜ ì™„ì „í•œ ë¹„êµë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

ë¹„êµ í•­ëª©:
1. ë‹¨ê³„ë³„/ì—ì´ì „íŠ¸ë³„ ì‹¤í–‰ ì‹œê°„
2. MCP ë„êµ¬ ì‚¬ìš© íŒ¨í„´  
3. ë³‘ë ¬ ì²˜ë¦¬ íš¨ê³¼
4. ë³´ê³ ì„œ í’ˆì§ˆ ë©”íŠ¸ë¦­
5. ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰

A2A ë¶€ë¶„ì€ A2A Clientë¥¼ í†µí•´ Deep Research A2A Serverì— ìš”ì²­ì„ ë³´ëƒ…ë‹ˆë‹¤.
"""

import asyncio
import json
import time
import re
import subprocess
import sys
import socket
from datetime import datetime
from typing import Dict, Any, List, Tuple, Optional
from collections import defaultdict
import os

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì„¤ì •
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(PROJECT_ROOT)

import httpx
from langchain_core.messages import HumanMessage
from dotenv import load_dotenv

# A2A Client imports
from a2a.client import ClientFactory, A2ACardResolver, ClientConfig
from a2a.client.helpers import create_text_message_object
from a2a.types import AgentCard, TransportProtocol, Role, Message

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ .env íŒŒì¼ ë¡œë“œ
load_dotenv(os.path.join(PROJECT_ROOT, ".env"))


class PerformanceTracker:
    """ì„±ëŠ¥ ì¶”ì ì„ ìœ„í•œ ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.events = []
        self.start_time = time.time()
    
    def log_event(self, event_type: str, event_name: str, data: Dict[str, Any] = None):
        """ì´ë²¤íŠ¸ ë¡œê·¸"""
        self.events.append({
            "timestamp": time.time() - self.start_time,
            "type": event_type,
            "name": event_name,
            "data": data or {}
        })
    
    def get_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½"""
        stage_times = defaultdict(float)
        stage_counts = defaultdict(int)
        
        for i in range(len(self.events) - 1):
            if self.events[i]["type"] == "stage_start":
                for j in range(i + 1, len(self.events)):
                    if (self.events[j]["type"] == "stage_end" and 
                        self.events[j]["name"] == self.events[i]["name"]):
                        duration = self.events[j]["timestamp"] - self.events[i]["timestamp"]
                        stage_times[self.events[i]["name"]] += duration
                        stage_counts[self.events[i]["name"]] += 1
                        break
        
        return {
            "total_time": time.time() - self.start_time,
            "stage_times": dict(stage_times),
            "stage_counts": dict(stage_counts),
            "events": self.events
        }


class ReportQualityAnalyzer:
    """ë³´ê³ ì„œ í’ˆì§ˆ ë¶„ì„ê¸°"""
    
    @staticmethod
    def analyze_report(report: str) -> Dict[str, Any]:
        """ë³´ê³ ì„œ í’ˆì§ˆ ë©”íŠ¸ë¦­ ë¶„ì„"""
        if not report:
            return {
                "word_count": 0,
                "sentence_count": 0,
                "paragraph_count": 0,
                "section_count": 0,
                "subsection_count": 0,
                "reference_count": 0,
                "bullet_points": 0,
                "code_blocks": 0,
                "avg_sentence_length": 0,
                "structure_score": 0
            }
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­
        words = report.split()
        word_count = len(words)
        
        # ë¬¸ì¥ ìˆ˜ (ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ê¸°ì¤€)
        sentences = re.split(r'[.!?]+', report)
        sentence_count = len([s for s in sentences if s.strip()])
        
        # ë‹¨ë½ ìˆ˜
        paragraphs = report.split('\n\n')
        paragraph_count = len([p for p in paragraphs if p.strip()])
        
        # ì„¹ì…˜ ìˆ˜ (# ì œëª©)
        sections = re.findall(r'^#\s+.+$', report, re.MULTILINE)
        section_count = len(sections)
        
        # í•˜ìœ„ ì„¹ì…˜ ìˆ˜ (## ì œëª©)
        subsections = re.findall(r'^##\s+.+$', report, re.MULTILINE)
        subsection_count = len(subsections)
        
        # ì°¸ì¡°/ì¸ìš© ìˆ˜ (ëŒ€ê´„í˜¸ ë§í¬ í˜•ì‹)
        references = re.findall(r'\[([^\]]+)\]\(([^)]+)\)', report)
        reference_count = len(references)
        
        # ê¸€ë¨¸ë¦¬ ê¸°í˜¸
        bullet_points = len(re.findall(r'^\s*[-*]\s+', report, re.MULTILINE))
        
        # ì½”ë“œ ë¸”ë¡
        code_blocks = len(re.findall(r'```[\s\S]*?```', report))
        
        # í‰ê·  ë¬¸ì¥ ê¸¸ì´
        avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0
        
        # êµ¬ì¡°í™” ì ìˆ˜ (0-100)
        structure_score = min(100, (
            (section_count * 10) +
            (subsection_count * 5) +
            (paragraph_count * 2) +
            (bullet_points * 1) +
            (reference_count * 3)
        ))
        
        return {
            "word_count": word_count,
            "sentence_count": sentence_count,
            "paragraph_count": paragraph_count,
            "section_count": section_count,
            "subsection_count": subsection_count,
            "reference_count": reference_count,
            "bullet_points": bullet_points,
            "code_blocks": code_blocks,
            "avg_sentence_length": round(avg_sentence_length, 2),
            "structure_score": structure_score
        }


def is_port_in_use(host: str = "localhost", port: int = 8090) -> bool:
    """í¬íŠ¸ ì‚¬ìš© ì¤‘ì¸ì§€ í™•ì¸"""
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        try:
            s.settimeout(1)
            result = s.connect_ex((host, port))
            return result == 0
        except Exception:
            return False


async def check_server_running(host: str = "localhost", port: int = 8090) -> bool:
    """ì‹¤í–‰ ì¤‘ì¸ ì„œë²„ í™•ì¸ (í—¬ìŠ¤ ì²´í¬)"""
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(f"http://{host}:{port}/health", timeout=3.0)
            return response.status_code == 200
    except Exception:
        return False


async def get_server_status(host: str = "localhost", port: int = 8090) -> Dict[str, Any]:
    """ì„œë²„ ìƒíƒœ ìƒì„¸ ì •ë³´"""
    port_in_use = is_port_in_use(host, port)
    server_responding = await check_server_running(host, port)
    
    status = {
        "port_in_use": port_in_use,
        "server_responding": server_responding,
        "host": host,
        "port": port
    }
    
    if port_in_use and not server_responding:
        status["issue"] = "í¬íŠ¸ëŠ” ì‚¬ìš© ì¤‘ì´ì§€ë§Œ í—¬ìŠ¤ ì²´í¬ ì‹¤íŒ¨"
    elif not port_in_use and not server_responding:
        status["issue"] = "ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ"
    elif server_responding:
        status["status"] = "ì •ìƒ ì‘ë™ ì¤‘"
    
    return status


async def start_deep_research_a2a_server() -> Optional[subprocess.Popen]:
    """ê·¸ë˜í”„ ê¸°ë°˜ ì„ë² ë””ë“œ ì„œë²„ ì‚¬ìš© ê¶Œì¥: ì™¸ë¶€ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ëŒ€ì‹  ì„ë² ë””ë“œ ì‚¬ìš©"""
    print("â„¹ï¸  ê¶Œì¥: start_embedded_graph_serverë¥¼ ì‚¬ìš©í•˜ì„¸ìš” (ì„ë² ë””ë“œ ì„œë²„)")
    return None


async def run_langgraph_with_tracking(query: str) -> Tuple[Dict[str, Any], PerformanceTracker]:
    """LangGraph ì‹¤í–‰ with ìƒì„¸ ì¶”ì """
    tracker = PerformanceTracker()
    
    try:
        tracker.log_event("system_start", "LangGraph")
        
        # Import
        tracker.log_event("stage_start", "import")
        from src.lg_agents.deep_research_agent import deep_research_graph
        tracker.log_event("stage_end", "import")
        
        # ê·¸ë˜í”„ ì¤€ë¹„
        tracker.log_event("stage_start", "graph_setup")
        app = deep_research_graph
        tracker.log_event("stage_end", "graph_setup")
        
        # ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ê° ë‹¨ê³„ ì¶”ì )
        tracker.log_event("stage_start", "total_execution")
        
        step_results = {}
        async for event in app.astream({"messages": [HumanMessage(content=query)]}):
            for node_name, node_output in event.items():
                tracker.log_event("node_start", node_name)
                
                # ë…¸ë“œë³„ ê²°ê³¼ ì €ì¥
                if isinstance(node_output, dict):
                    step_results[node_name] = {
                        "output_size": len(str(node_output)),
                        "keys": list(node_output.keys()) if isinstance(node_output, dict) else []
                    }
                    
                    # íŠ¹ì • ë…¸ë“œ ìƒì„¸ ì •ë³´
                    if node_name == "research_supervisor":
                        if "raw_notes" in node_output:
                            tracker.log_event("data", "raw_notes_generated", {
                                "count": len(node_output.get("raw_notes", [])),
                                "size": sum(len(note) for note in node_output.get("raw_notes", []))
                            })
                    
                    if node_name == "final_report_generation":
                        if "final_report" in node_output:
                            tracker.log_event("data", "final_report_generated", {
                                "size": len(node_output.get("final_report", ""))
                            })
                
                tracker.log_event("node_end", node_name)
                
                # ìµœì¢… ê²°ê³¼ ì €ì¥
                if isinstance(node_output, dict) and "final_report" in node_output:
                    final_result = node_output
        
        tracker.log_event("stage_end", "total_execution")
        tracker.log_event("system_end", "LangGraph")
        
        return {
            "success": True,
            "final_report": final_result.get("final_report", "") if 'final_result' in locals() else "",
            "step_results": step_results,
            "raw_notes": final_result.get("raw_notes", []) if 'final_result' in locals() else [],
            "notes": final_result.get("notes", []) if 'final_result' in locals() else []
        }, tracker
        
    except Exception as e:
        tracker.log_event("error", "system_error", {"error": str(e)})
        return {
            "success": False,
            "error": str(e),
            "final_report": ""
        }, tracker


async def run_a2a_with_tracking(query: str) -> Tuple[Dict[str, Any], PerformanceTracker]:
    """A2A Clientë¥¼ í†µí•œ Deep Research ì‹¤í–‰ with ìƒì„¸ ì¶”ì """
    tracker = PerformanceTracker()
    
    try:
        tracker.log_event("system_start", "A2A_Client")
        
        # A2A Client ìƒì„±
        tracker.log_event("stage_start", "client_setup")
        # httpx.AsyncClientëŠ” with ë¸”ë¡ìœ¼ë¡œ ìˆ˜ëª… ì£¼ê¸°ë¥¼ ì¢í˜€ ì»¤ë„¥ì…˜ ëˆ„ìˆ˜ ë°©ì§€
        async with httpx.AsyncClient() as aio:
            resolver = A2ACardResolver(
                httpx_client=aio,
                base_url="http://localhost:8092",
            )
            agent_card: AgentCard = await resolver.get_agent_card()
        # resolver.get_agent_card() ì´í›„ì—ëŠ” ClientFactoryê°€ ë‚´ë¶€ í´ë¼ì´ì–¸íŠ¸ë¥¼ ê´€ë¦¬
        config = ClientConfig(
            streaming=True,
            supported_transports=[TransportProtocol.jsonrpc, TransportProtocol.http_json, TransportProtocol.grpc],
        )
        factory = ClientFactory(config=config)
        client = factory.create(card=agent_card)
        tracker.log_event("stage_end", "client_setup")
        
        # ì—ì´ì „íŠ¸ ì •ë³´ í™•ì¸
        tracker.log_event("stage_start", "agent_info")
        print("ğŸ”µ A2A ì—ì´ì „íŠ¸ ì •ë³´:")
        print(f"   - ì´ë¦„: {agent_card.name}")
        print(f"   - ì„¤ëª…: {agent_card.description}")
        print(f"   - ì§€ì› í”„ë¡œí† ì½œ: {agent_card.capabilities.streaming}")
        print("   - ê¸°ëŠ¥:")
        for skill in agent_card.skills:
            print(f"     * {skill.name}: {skill.description}")
        tracker.log_event("stage_end", "agent_info")
        
        # A2A ìš”ì²­ ì „ì†¡
        tracker.log_event("stage_start", "a2a_request")
        print(f"\\nğŸ”µ A2A Deep Research ìš”ì²­ ì „ì†¡: {query}")
        
        message: Message = create_text_message_object(
            role=Role.user,
            content=query
        )
        
        # A2A ì‘ë‹µ ì²˜ë¦¬ ë° ì¶”ì 
        response_text = ""
        step_events = []
        
        async for event in client.send_message(message):
            # A2A ì´ë²¤íŠ¸ëŠ” ë³µì¡í•œ tuple êµ¬ì¡°ë¡œ ì˜´: (Task, Event)
            if isinstance(event, tuple) and len(event) >= 1:
                task = event[0]  # ì²« ë²ˆì§¸ëŠ” Task ê°ì²´
                
                # Taskì—ì„œ artifacts í™•ì¸ (ìµœì¢… ì‘ë‹µ)
                if hasattr(task, 'artifacts') and task.artifacts:
                    for artifact in task.artifacts:
                        if hasattr(artifact, 'parts') and artifact.parts:
                            for part in artifact.parts:
                                root = getattr(part, 'root', None)
                                text_content = getattr(root, 'text', None)
                                if isinstance(text_content, str):
                                    if text_content not in response_text:
                                        response_text += text_content + "\n"
                
                # Task historyì—ì„œ ì¤‘ê°„ ë©”ì‹œì§€ë“¤ í™•ì¸ (ì§„í–‰ ê³¼ì •)
                if hasattr(task, 'history') and task.history:
                    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ë§Œ ì²˜ë¦¬ (ì¤‘ë³µ ë°©ì§€)
                    last_message = task.history[-1]
                    if (hasattr(last_message, 'role') and 
                        last_message.role.value == 'agent' and
                        hasattr(last_message, 'parts') and last_message.parts):
                        
                        for part in last_message.parts:
                            root = getattr(part, 'root', None)
                            text_content = getattr(root, 'text', None)
                            if isinstance(text_content, str) and text_content not in response_text:
                                response_text += text_content + "\n"
                                step_events.append({
                                    "timestamp": time.time() - tracker.start_time,
                                    "event": text_content
                                })
                                print(f"[A2A ì§„í–‰] {text_content}")
        
        tracker.log_event("stage_end", "a2a_request")        
        tracker.log_event("system_end", "A2A_Client")
        
        # step2 íŒ¨í„´: ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì‘ë‹µ ì²˜ë¦¬
        if response_text.strip():
            return {
                "success": True,
                "final_report": response_text.strip(),
                "raw_response": response_text,
                "step_events": step_events
            }, tracker
        else:
            print("âš ï¸ ë¹ˆ ì‘ë‹µ ìˆ˜ì‹ ë¨")
            return {
                "success": False,
                "error": "ë¹ˆ ì‘ë‹µ ìˆ˜ì‹ ",
                "final_report": "",
                "raw_response": response_text,
                "step_events": step_events
            }, tracker
            
    except Exception as e:
        tracker.log_event("error", "system_error", {"error": str(e)})
        return {
            "success": False,
            "error": str(e),
            "final_report": ""
        }, tracker


async def compare_systems():
    """ë‘ ì‹œìŠ¤í…œ ìƒì„¸ ë¹„êµ"""
    query = "AIê°€ êµìœ¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”"
    
    print("ğŸ”¬ Deep Research A2A Client ê¸°ë°˜ í’ˆì§ˆ ë° ì„±ëŠ¥ ë¹„êµ")
    print("=" * 80)
    print(f"ğŸ“‹ ì—°êµ¬ ì£¼ì œ: {query}")
    print(f"ğŸ• ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    # A2A ì„œë²„ ìƒíƒœ í™•ì¸ ë° ì‹œì‘
    print("\\nğŸ”µ Deep Research A2A ì„œë²„ ì¤€ë¹„ ì¤‘...")
    server_process = await start_deep_research_a2a_server()
    server_started_by_us = server_process is not None
    
    # ì„œë²„ ê°€ìš©ì„± ìµœì¢… í™•ì¸
    if not await check_server_running():
        print("âŒ A2A ì„œë²„ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - A2A ë¹„êµë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        print("ğŸ’¡ í•´ê²° ë°©ë²•:")
        print("   1. ê¶Œì¥: ì„ë² ë””ë“œ ê·¸ë˜í”„ ì„œë²„ ì‚¬ìš© (start_embedded_graph_server)")
        print("   2. í¬íŠ¸ 8090ì´ ì‚¬ìš© ì¤‘ì¸ì§€ í™•ì¸: lsof -i :8090")
        print("   3. í™˜ê²½ ë³€ìˆ˜ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸")
        return
    
    print("âœ… A2A ì„œë²„ ì‚¬ìš© ì¤€ë¹„ ì™„ë£Œ")
    if not server_started_by_us:
        print("ğŸ“ ì°¸ê³ : ê¸°ì¡´ì— ì‹¤í–‰ ì¤‘ì¸ ì„œë²„ë¥¼ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤")
    
    try:
        # LangGraph ì‹¤í–‰
        print("\\nğŸ”´ LangGraph Deep Research ì‹¤í–‰ ì¤‘...")
        lg_result, lg_tracker = await run_langgraph_with_tracking(query)
        lg_performance = lg_tracker.get_summary()
        
        # ì ì‹œ ëŒ€ê¸°
        await asyncio.sleep(2)
        
        # A2A ì‹¤í–‰
        print("\\nğŸ”µ A2A Deep Research (Client ê¸°ë°˜) ì‹¤í–‰ ì¤‘...")
        a2a_result, a2a_tracker = await run_a2a_with_tracking(query)
        a2a_performance = a2a_tracker.get_summary()
        
        # ë³´ê³ ì„œ í’ˆì§ˆ ë¶„ì„
        print("\\nğŸ“Š ë³´ê³ ì„œ í’ˆì§ˆ ë¶„ì„ ì¤‘...")
        lg_quality = ReportQualityAnalyzer.analyze_report(lg_result.get("final_report", ""))
        a2a_quality = ReportQualityAnalyzer.analyze_report(a2a_result.get("final_report", ""))
        
        # ê²°ê³¼ ì¶œë ¥
        print("\\n" + "=" * 80)
        print("ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼")
        print("=" * 80)
        
        # 1. ì‹¤í–‰ ì‹œê°„ ë¹„êµ
        print("\\nâ±ï¸  ì‹¤í–‰ ì‹œê°„ ë¹„êµ:")
        print(f"   ğŸ”´ LangGraph: {lg_performance['total_time']:.2f}ì´ˆ")
        if lg_performance['stage_times']:
            for stage, time in lg_performance['stage_times'].items():
                print(f"      - {stage}: {time:.2f}ì´ˆ")
        
        print(f"\\n   ğŸ”µ A2A (Client): {a2a_performance['total_time']:.2f}ì´ˆ")
        if a2a_performance['stage_times']:
            for stage, time in a2a_performance['stage_times'].items():
                print(f"      - {stage}: {time:.2f}ì´ˆ")
        
        # A2A ë‚´ë¶€ ì„±ëŠ¥ í†µê³„ ì¶œë ¥
        if a2a_result.get("performance_stats"):
            stats = a2a_result["performance_stats"]
            print("\\n   ğŸ”µ A2A ë‚´ë¶€ ë‹¨ê³„ë³„ ì‹œê°„:")
            for key, value in stats.items():
                if key.endswith('_time') and isinstance(value, (int, float)):
                    print(f"      - {key.replace('_time', '')}: {value:.2f}ì´ˆ")
            
            # ë³‘ë ¬ ì²˜ë¦¬ íš¨ê³¼ ê³„ì‚°
            if 'parallel_research_time' in stats and stats['parallel_research_time'] > 0:
                estimated_sequential = stats['parallel_research_time'] * stats.get('total_researchers', 3)
                speedup = estimated_sequential / stats['parallel_research_time']
                print(f"\\n   ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ íš¨ê³¼: {speedup:.2f}x ì†ë„ í–¥ìƒ (ì˜ˆìƒ)")
        
        # 2. ë³´ê³ ì„œ í’ˆì§ˆ ë¹„êµ
        print("\\nğŸ“ ë³´ê³ ì„œ í’ˆì§ˆ ë¹„êµ:")
        print("\\n   ğŸ”´ LangGraph:")
        for metric, value in lg_quality.items():
            print(f"      - {metric}: {value}")
        
        print("\\n   ğŸ”µ A2A:")
        for metric, value in a2a_quality.items():
            print(f"      - {metric}: {value}")
        
        # 3. MCP ë„êµ¬ ì‚¬ìš© ë¶„ì„
        print("\\nğŸ”§ MCP ë„êµ¬ ì‚¬ìš© ë¶„ì„:")
        print("   ğŸ”´ LangGraph:")
        lg_raw_notes = lg_result.get("raw_notes", [])
        lg_mcp_usage = analyze_mcp_usage(lg_raw_notes)
        for tool, count in lg_mcp_usage.items():
            print(f"      - {tool}: {count}íšŒ ì‚¬ìš©")
        
        print("\\n   ğŸ”µ A2A:")
        a2a_raw_notes = a2a_result.get("raw_research_notes", [])
        a2a_mcp_usage = analyze_mcp_usage(a2a_raw_notes)
        for tool, count in a2a_mcp_usage.items():
            print(f"      - {tool}: {count}íšŒ ì‚¬ìš©")
        
        # 4. ì¢…í•© í‰ê°€
        print("\\nğŸ† ì¢…í•© í‰ê°€:")
        
        # ì†ë„ ìš°ìœ„
        speed_winner = "LangGraph" if lg_performance['total_time'] < a2a_performance['total_time'] else "A2A"
        speed_diff = abs(lg_performance['total_time'] - a2a_performance['total_time'])
        print(f"   âš¡ ì†ë„: {speed_winner}ê°€ {speed_diff:.2f}ì´ˆ ë¹ ë¦„")
        
        # í’ˆì§ˆ ìš°ìœ„
        lg_score = lg_quality['structure_score'] + (lg_quality['word_count'] / 100)
        a2a_score = a2a_quality['structure_score'] + (a2a_quality['word_count'] / 100)
        quality_winner = "LangGraph" if lg_score > a2a_score else "A2A"
        print(f"   ğŸ“Š í’ˆì§ˆ ì ìˆ˜: LangGraph({lg_score:.1f}) vs A2A({a2a_score:.1f})")
        
        # ìƒì„¸ ê²°ê³¼ ì €ì¥
        detailed_results = {
            "timestamp": datetime.now().isoformat(),
            "query": query,
            "langgraph": {
                "success": lg_result.get("success", False),
                "performance": lg_performance,
                "quality": lg_quality,
                "mcp_usage": lg_mcp_usage,
                "report_preview": lg_result.get("final_report", "")[:500] + "..." if lg_result.get("final_report") else ""
            },
            "a2a": {
                "success": a2a_result.get("success", False),
                "performance": a2a_performance,
                "quality": a2a_quality,
                "mcp_usage": a2a_mcp_usage,
                "internal_stats": a2a_result.get("performance_stats", {}),
                "step_events": a2a_result.get("step_events", []),
                "report_preview": a2a_result.get("final_report", "")[:500] + "..." if a2a_result.get("final_report") else ""
            },
            "comparison": {
                "speed_winner": speed_winner,
                "speed_difference": speed_diff,
                "quality_winner": quality_winner,
                "lg_quality_score": lg_score,
                "a2a_quality_score": a2a_score
            }
        }
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        with open("deep_research_a2a_client_comparison.json", "w", encoding="utf-8") as f:
            json.dump(detailed_results, f, ensure_ascii=False, indent=2)
        
        print("\\nğŸ’¾ ìƒì„¸ ê²°ê³¼ê°€ deep_research_a2a_client_comparison.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ë³´ê³ ì„œ ì „ë¬¸ ì €ì¥
        if lg_result.get("final_report"):
            with open("langgraph_report_client_comparison.md", "w", encoding="utf-8") as f:
                f.write(lg_result["final_report"])
            print("ğŸ“„ LangGraph ë³´ê³ ì„œê°€ langgraph_report_client_comparison.mdì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        if a2a_result.get("final_report"):
            with open("a2a_report_client_comparison.md", "w", encoding="utf-8") as f:
                f.write(a2a_result["final_report"])
            print("ğŸ“„ A2A ë³´ê³ ì„œê°€ a2a_report_client_comparison.mdì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    finally:
        # A2A ì„œë²„ ì¢…ë£Œ (ìš°ë¦¬ê°€ ì‹œì‘í•œ ì„œë²„ë§Œ)
        if server_process and server_started_by_us:
            print("\\nğŸ”µ A2A ì„œë²„ ì¢…ë£Œ ì¤‘...")
            server_process.terminate()
            try:
                server_process.wait(timeout=5)
                print("âœ… A2A ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤")
            except subprocess.TimeoutExpired:
                print("âš ï¸  ì„œë²„ ì¢…ë£Œ íƒ€ì„ì•„ì›ƒ - ê°•ì œ ì¢…ë£Œí•©ë‹ˆë‹¤")
                server_process.kill()
                server_process.wait()
        elif not server_started_by_us:
            print("\\nğŸ“ ì°¸ê³ : ê¸°ì¡´ ì„œë²„ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€ë©ë‹ˆë‹¤")


def analyze_mcp_usage(notes: List[str]) -> Dict[str, int]:
    """MCP ë„êµ¬ ì‚¬ìš© ë¶„ì„"""
    usage = {"Tavily": 0, "arXiv": 0, "Serper": 0}
    
    for note in notes:
        if isinstance(note, str):
            if "[Tavily]" in note or "[tavily]" in note:
                usage["Tavily"] += 1
            if "[arXiv]" in note or "[arxiv]" in note:
                usage["arXiv"] += 1
            if "[Serper]" in note or "[serper]" in note:
                usage["Serper"] += 1
    
    return usage


if __name__ == "__main__":
    print("ğŸš€ Deep Research A2A Client ê¸°ë°˜ í’ˆì§ˆ ë° ì„±ëŠ¥ ë¹„êµ ì‹œì‘")
    print("""
    ğŸ“Œ ì‹¤í–‰ ì „ í™•ì¸ì‚¬í•­:
    1. MCP ì„œë²„ê°€ Dockerì—ì„œ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸ (Tavily:3001, arXiv:3002, Serper:3003)
    2. OPENAI_API_KEY, TAVILY_API_KEY, SERPER_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    
    ğŸ“Œ A2A ì„œë²„ ìë™ ê´€ë¦¬:
    - ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ A2A ì„œë²„ê°€ ìˆìœ¼ë©´ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤
    - ì‹¤í–‰ ì¤‘ì¸ ì„œë²„ê°€ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ìƒˆ ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤
    - ë¹„êµ ì™„ë£Œ í›„ ìš°ë¦¬ê°€ ì‹œì‘í•œ ì„œë²„ë§Œ ì¢…ë£Œí•©ë‹ˆë‹¤
    
    ğŸ“Œ A2A Client ê¸°ë°˜ ë¹„êµ:
    - LangGraph Deep ResearchëŠ” ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì‹¤í–‰
    - A2A Deep ResearchëŠ” A2A Clientë¥¼ í†µí•´ Deep Research A2A Serverì— ìš”ì²­
    - ì™„ì „í•œ orchestrationê³¼ ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥ì„ ë¹„êµ
    
    ğŸ’¡ ì„œë²„ ì¶©ëŒ ì‹œ í•´ê²° ë°©ë²•:
    - í¬íŠ¸ ì¶©ëŒ: lsof -ti:8090 | xargs kill -9
    - ê¶Œì¥: ì„ë² ë””ë“œ ê·¸ë˜í”„ ì„œë²„ ì‚¬ìš© (start_embedded_graph_server)
    """)
    
    try:
        asyncio.run(compare_systems())
    except KeyboardInterrupt:
        print("\\n\\ní”„ë¡œê·¸ë¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("\\nğŸ”§ ë¬¸ì œ í•´ê²° ê°€ì´ë“œ:")
        print("1. MCP ì„œë²„ í™•ì¸:")
        print("   docker ps | grep mcp")
        print("   docker-compose -f docker-compose.mcp.yml up -d")
        print("\\n2. A2A ì„œë²„ í¬íŠ¸ ì¶©ëŒ í•´ê²°:")
        print("   lsof -ti:8090 | xargs kill -9")
        print("   (ê¶Œì¥) ì„ë² ë””ë“œ ê·¸ë˜í”„ ì„œë²„ ì‚¬ìš©: start_embedded_graph_server")
        print("\\n3. í™˜ê²½ ë³€ìˆ˜ í™•ì¸:")
        print("   echo $OPENAI_API_KEY")
        print("   echo $TAVILY_API_KEY") 
        print("   echo $SERPER_API_KEY")
        print("\\n4. ìƒì„¸ ë””ë²„ê¹…:")
        print("   python -c 'import asyncio; from examples.deep_research_a2a_client_comparison import get_server_status; print(asyncio.run(get_server_status()))'")
        
        # ì„œë²„ ìƒíƒœ ìë™ í™•ì¸
        try:
            import asyncio
            status = asyncio.run(get_server_status())
            print(f"\\nğŸ“Š í˜„ì¬ ì„œë²„ ìƒíƒœ: {status}")
        except Exception:
            pass