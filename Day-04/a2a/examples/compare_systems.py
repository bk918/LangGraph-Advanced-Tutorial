# ruff: noqa: E402
"""
Step 3ì—ì„œ ì‚¬ìš©í•˜ëŠ” LangGraph vs A2A ì‹œìŠ¤í…œ ì‹¤ì œ ë¹„êµ ëª¨ë“ˆ

=== í•™ìŠµ ëª©í‘œ ===
ë™ì¼í•œ ì—°êµ¬ ì‘ì—…ì„ ë‘ ê°€ì§€ ë‹¤ë¥¸ ì•„í‚¤í…ì²˜ë¡œ ì‹¤í–‰í•˜ì—¬
ì‹¤ì œ ì„±ëŠ¥ ì°¨ì´ì™€ êµ¬í˜„ ë³µì¡ì„±ì„ ì§ì ‘ ë¹„êµí•©ë‹ˆë‹¤.

=== ë¹„êµ ëŒ€ìƒ ===
1. LangGraph Deep Research: ë³µì¡í•œ ìƒíƒœ ê·¸ë˜í”„ ë°©ì‹ (lg_agents/deep_research_agent.py)
   - 6ê°œ ë…¸ë“œì˜ ë³µì¡í•œ ìƒíƒœ ê·¸ë˜í”„ (clarify_with_user â†’ write_research_brief â†’ supervisor â†’ researcher â†’ compress_research â†’ final_report_generation)
   - ì¤‘ì²©ëœ ìƒíƒœ ê´€ë¦¬ (AgentState, SupervisorState, ResearcherState)
   - ì„œë¸Œê·¸ë˜í”„ì™€ Command ê°ì²´ë¡œ ë…¸ë“œ ê°„ ë¼ìš°íŒ…
   - ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ ë‚´ ìˆœì°¨ ì‹¤í–‰

2. A2A Deep Research: ë‹¨ìˆœí•œ ì—ì´ì „íŠ¸ í˜‘ì—… ë°©ì‹ (a2a_orchestrator/agents/deep_research.py)
   - 5ê°œ ë…ë¦½ ì—ì´ì „íŠ¸ì˜ Agent-to-Agent í†µì‹  (DeepResearchA2AAgent, PlannerA2AAgent, ResearcherA2AAgent, WriterA2AAgent, EvaluatorA2AAgent)
   - í‰ë©´ì  ì»¨í…ìŠ¤íŠ¸ ê³µìœ  (research_context ë”•ì…”ë„ˆë¦¬)
   - ë…ë¦½ ì‹¤í–‰ê³¼ í‘œì¤€í™”ëœ A2A í”„ë¡œí† ì½œ í†µì‹ 
   - ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥í•œ ë¶„ì‚° ì•„í‚¤í…ì²˜

=== ë¹„êµ ë©”íŠ¸ë¦­ ===
- ì‹¤í–‰ ì‹œê°„ (ì‹œì‘ë¶€í„° ì™„ë£Œê¹Œì§€)
- State/Context ë³µì¡ì„± (ê´€ë¦¬í•´ì•¼ í•  ë°ì´í„° êµ¬ì¡°)
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë° ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„±
- ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬ ëŠ¥ë ¥
- í™•ì¥ì„± (ìƒˆë¡œìš´ ì—ì´ì „íŠ¸ ì¶”ê°€ ìš©ì´ì„±)

=== ì‚¬ìš©ë²• ===
ì´ ëª¨ë“ˆì€ examples/step3_multiagent_systems.pyì—ì„œ importë˜ì–´
run_comparison() í•¨ìˆ˜ê°€ í˜¸ì¶œë˜ì–´ ì‹¤ì œ ë¹„êµ ì‹¤í—˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import asyncio
import json
import sys
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì • (ì ˆëŒ€ê²½ë¡œë¡œ ê³ ì •í•˜ì—¬ ì‹¤í–‰ CWDì™€ ë¬´ê´€í•˜ê²Œ ì €ì¥ë˜ë„ë¡ í•¨)
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# .env íŒŒì¼ ë¡œë“œ
load_dotenv(PROJECT_ROOT / ".env")

# ë¡œê¹… ì„¤ì •
from src.utils.logging_config import get_logger

logger = get_logger(__name__)


async def run_langgraph_deep_research(query: str):
    """LangGraph ë”¥ë¦¬ì„œì¹˜ êµ¬í˜„ì²´ ì‹¤í–‰ (ë³µì¡í•œ State ê´€ë¦¬)"""
    print("\n" + "=" * 80)
    print("ğŸ”´ LangGraph ë”¥ë¦¬ì„œì¹˜ êµ¬í˜„ì²´ ì‹¤í–‰")
    print("=" * 80)

    start_time = datetime.now()

    try:
        print("ğŸ“¥ LangGraph ë”¥ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸ ì„í¬íŠ¸ ì¤‘...")
        # LangGraph ê¸°ë°˜ ë”¥ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸ (ë³µì¡í•œ State ê´€ë¦¬)
        from src.lg_agents.deep_research.deep_research_agent import deep_research_graph
        from langchain_core.messages import HumanMessage

        print(f"ğŸ“ ë”¥ë¦¬ì„œì¹˜ ì¿¼ë¦¬ ì‹¤í–‰: {query}")
        print("ğŸ”„ LangGraph ë³µì¡í•œ State ê´€ë¦¬ë¡œ ì‹¤í–‰ ì¤‘...")

        # ì‹¤ì œ LangGraph ë”¥ë¦¬ì„œì¹˜ ì‹¤í–‰
        result = await deep_research_graph.ainvoke({"messages": [HumanMessage(content=query)]})

        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()

        print("âœ… LangGraph ë”¥ë¦¬ì„œì¹˜ ì‹¤í–‰ ì™„ë£Œ!")
        print(f"â±ï¸  ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")
        print(f"ğŸ“„ ê²°ê³¼ ê¸¸ì´: {len(str(result))} ë¬¸ì")

        return {
            "success": True,
            "execution_time": execution_time,
            "result": {
                "final_report": result.get("final_report", ""),
                "research_brief": result.get("research_brief", ""),
                "raw_notes_count": len(result.get("raw_notes", [])),
                "notes_count": len(result.get("notes", [])),
                "messages_count": len(result.get("messages", [])),
                "state_keys": list(result.keys()) if result else [],
            },
            "system_type": "LangGraph",
            "architecture": "ë³µì¡í•œ State ê´€ë¦¬, ì¤‘ì•™ ì§‘ì¤‘ì‹, ìˆœì°¨ ì‹¤í–‰",
        }

    except Exception as e:
        error_msg = f"LangGraph ë”¥ë¦¬ì„œì¹˜ ì‹¤í–‰ ì‹¤íŒ¨: {e}"
        print(f"âŒ {error_msg}")

        import traceback

        print("ğŸ” ì—ëŸ¬ ìƒì„¸:")
        print(traceback.format_exc())

        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()

        return {
            "success": False,
            "error": error_msg,
            "execution_time": execution_time,
            "system_type": "LangGraph ë”¥ë¦¬ì„œì¹˜",
        }


async def run_a2a_deep_research(
    query: str,
    endpoints: dict[str, str] | None = None,
    *,
    enable_hitl: bool = False,
    reviewer_id: str = "demo_reviewer",
    approval_timeout_seconds: int = 600,
):
    """A2A ë”¥ë¦¬ì„œì¹˜ êµ¬í˜„ì²´ ì‹¤í–‰ (ë‹¨ìˆœí•œ Context ê´€ë¦¬)

    - enable_hitl: ìµœì¢… ê²°ê³¼ë¬¼ì— ëŒ€í•´ HITL ìµœì¢… ìŠ¹ì¸ ë£¨í”„(ë‹¨ìˆœ ìŠ¹ì¸/ê±°ë¶€)ë¥¼ ì ìš©
      (Step 3 ê¸°ë³¸ ë¹„êµì—ëŠ” False ìœ ì§€, Step 4ì—ì„œ Trueë¡œ í™œìš©)
    """
    print("\n" + "=" * 80)
    print("ğŸ”µ A2A ë”¥ë¦¬ì„œì¹˜ êµ¬í˜„ì²´ ì‹¤í–‰")
    print("=" * 80)

    start_time = datetime.now()

    try:
        from src.a2a_integration.a2a_lg_client_utils import A2AClientManager

        # ì™¸ë¶€ì—ì„œ ì—”ë“œí¬ì¸íŠ¸ê°€ ì£¼ì–´ì§€ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì´ë¯¸ ë„ì›Œì§„ ì„œë²„)
        if endpoints and isinstance(endpoints, dict) and endpoints.get("deep_research"):
            base_url = endpoints["deep_research"]
            logger.info(f"ğŸ”— ì™¸ë¶€ ì œê³µ A2A ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©(DeepResearchA2AGraph): {base_url}")
            
            graph_input = {
                "messages": [
                    {"role": "human", "content": query},
                ],
            }
            
            logger.info(f"DeepResearchGraph ìŠ¤í™ì— ë§ëŠ” ë°ì´í„° Input ì„ ìœ„í•´ ì „ì²˜ë¦¬: {graph_input}")
            async with A2AClientManager(base_url=base_url) as client:
                merged = await client.send_data_merged(graph_input)

        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()

        
        final_report_text = ""
        try:
            if isinstance(merged, dict):
                # êµ¬ì¡°í™” ê²°ê³¼ì—ì„œ ìš°ì„ ì ìœ¼ë¡œ final_reportë¥¼ ì°¾ëŠ”ë‹¤
                final_report_text = str(merged.get("final_report") or merged.get("text") or "")
            else:
                final_report_text = str(merged)
        except Exception:
            final_report_text = str(merged)

        logger.info(f"A2A ë”¥ë¦¬ì„œì¹˜ ê²°ê³¼ ê¸¸ì´: {len(final_report_text)}")
        result = {
            "research_brief": "",
            "raw_notes_count": 0,
            "compressed_notes_count": 0,
            "final_report": final_report_text,
            "context_complexity": "ë‚®ìŒ (í‰ë©´ì  êµ¬ì¡°)",
            "execution_mode": "ë¶„ì‚°ì‹ (A2Aë¡œ ê·¸ë˜í”„ ë˜í•‘)",
        }

        # ì„ íƒ: ìµœì¢… ê²°ê³¼ì— ëŒ€í•´ HITL ìµœì¢… ìŠ¹ì¸ ìš”ì²­/ëŒ€ê¸° (ë‹¨ìˆœ í”Œë¡œìš°)
        if enable_hitl and result["final_report"]:
            try:
                from src.hitl.manager import hitl_manager
                from src.hitl.models import ApprovalType

                request = await hitl_manager.request_approval(
                    agent_id="a2a_deep_research",
                    approval_type=ApprovalType.FINAL_REPORT,
                    title="ìµœì¢… ë³´ê³ ì„œ ìŠ¹ì¸ ìš”ì²­",
                    description="A2A ê¸°ë°˜ ì—°êµ¬ ë³´ê³ ì„œ ê²€í†  ë° ìµœì¢… ìŠ¹ì¸ ìš”ì²­",
                    context={
                        "task_id": f"a2a_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                        "research_topic": query,
                        "final_report": result["final_report"],
                        "execution_mode": result["execution_mode"],
                    },
                    options=["ìŠ¹ì¸", "ê±°ë¶€"],
                    timeout_seconds=approval_timeout_seconds,
                    priority="high",
                )

                approved = await hitl_manager.wait_for_approval(
                    request.request_id, auto_approve_on_timeout=False
                )

                result["approval"] = {
                    "request_id": approved.request_id,
                    "status": approved.status.value,
                    "decision": approved.decision,
                    "decided_by": approved.decided_by,
                    "reason": approved.decision_reason,
                }
            except Exception as e:
                # HITL í™˜ê²½ì´ ì¤€ë¹„ë˜ì§€ ì•Šì€ ê²½ìš°ì—ë„ ë¹„êµ ì‹¤í—˜ì€ ê³„ì†
                result["approval_error"] = f"HITL ì²˜ë¦¬ ì‹¤íŒ¨: {e}"

        print("âœ… A2A ë”¥ë¦¬ì„œì¹˜ ì‹¤í–‰ ì™„ë£Œ!")
        print(f"â±ï¸  ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")
        print(f"ğŸ“„ ê²°ê³¼ ê¸¸ì´: {len(str(result))} ë¬¸ì")
        print("ğŸ—ï¸ Context ë³µì¡ì„±: ë‚®ìŒ (í‰ë©´ì  êµ¬ì¡°)")

        return {
            "success": True,
            "execution_time": execution_time,
            "result": result,
            "system_type": "A2A ë”¥ë¦¬ì„œì¹˜",
            "architecture": "ë‹¨ìˆœí•œ Context ê´€ë¦¬, ë¶„ì‚°ì‹, ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥",
        }

    except Exception as e:
        error_msg = f"A2A ë”¥ë¦¬ì„œì¹˜ ì‹¤í–‰ ì‹¤íŒ¨: {e}"
        print(f"âŒ {error_msg}")

        import traceback

        print("ğŸ” ì—ëŸ¬ ìƒì„¸:")
        print(traceback.format_exc())

        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()

        return {
            "success": False,
            "error": error_msg,
            "execution_time": execution_time,
            "system_type": "A2A ë”¥ë¦¬ì„œì¹˜",
        }

async def check_servers_basic():
    """ê¸°ë³¸ ì„œë²„ ìƒíƒœ ì²´í¬ (MCP + ê¸°ë³¸ A2A Supervisor í¬íŠ¸)"""
    # MCP ì„œë²„ ì²´í¬ (3000, 3001, 3002 í¬íŠ¸)
    import socket

    mcp_ports = [3000, 3001, 3002]
    mcp_running = []

    for port in mcp_ports:
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(1)
            result = sock.connect_ex(("localhost", port))
            sock.close()

            if result == 0:
                mcp_running.append(port)
                print(f"âœ… MCP ì„œë²„ í¬íŠ¸ {port}: ì‹¤í–‰ ì¤‘")
            else:
                print(f"âŒ MCP ì„œë²„ í¬íŠ¸ {port}: ì‹¤í–‰ ì•ˆë¨")
        except Exception:
            print(f"âŒ MCP ì„œë²„ í¬íŠ¸ {port}: ì—°ê²° ì‹¤íŒ¨")


    # A2A Supervisor ê¸°ë³¸ í¬íŠ¸(8090) í—¬ìŠ¤ì²´í¬
    a2a_healthy = False
    try:
        import httpx
        async with httpx.AsyncClient() as client:
            resp = await client.get("http://localhost:8090/health", timeout=1.5)
            a2a_healthy = resp.status_code == 200
    except Exception:
        a2a_healthy = False

    print("\nğŸ“Š ì²´í¬ ê²°ê³¼:")
    print(f"   MCP ì„œë²„: {len(mcp_running)}/{len(mcp_ports)} ê°œ ì‹¤í–‰ ì¤‘")
    print(f"   A2A Supervisor(8090): {'ì •ìƒ' if a2a_healthy else 'ë¹„ì •ìƒ/ë¯¸ì‹¤í–‰'}")

    return {"mcp_servers": mcp_running, "a2a_server": a2a_healthy}


async def run_comparison(query: str, endpoints: dict[str, str] | None = None, langgraph_run: bool = True, a2a_run: bool = True):
    """LangGraph ë”¥ë¦¬ì„œì¹˜ vs A2A ë”¥ë¦¬ì„œì¹˜ êµ¬í˜„ì²´ ë¹„êµ"""

    print("ğŸ¯ LangGraph ë”¥ë¦¬ì„œì¹˜ vs A2A ë”¥ë¦¬ì„œì¹˜ êµ¬í˜„ì²´ ë¹„êµ")
    print("=" * 80)
    print(f"ğŸ“‹ ì—°êµ¬ ì£¼ì œ: {query}")
    print(f"ğŸ• ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)

    print("ğŸ“ ë¹„êµ ëŒ€ìƒ:")
    print("   ğŸ”´ LangGraph ë”¥ë¦¬ì„œì¹˜: StateGraphë¡œ ìƒíƒœ ê´€ë¦¬, ì¤‘ì•™ ì§‘ì¤‘ì‹")
    print("   ğŸ”µ A2A ë”¥ë¦¬ì„œì¹˜: Contextë¡œ ìƒíƒœ ì „ë‹¬, ë¶„ì‚°ì‹")
    print("   ğŸ¤ ê³µí†µì : ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©, ë™ì¼í•œ ë…¼ë¦¬ íë¦„")
    print()

    # ì„œë²„ ìƒíƒœ ì‚¬ì „ ì²´í¬
    server_status = await check_servers_basic()
    print()

    # MCP ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ì§€ ì•Šìœ¼ë©´ ê²½ê³ 
    if not server_status["mcp_servers"]:
        print("âš ï¸  ê²½ê³ : MCP ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   MCP ì„œë²„ ì‹¤í–‰: docker-compose -f docker-compose.mcp.yml up")
        print()

    # ì „ì²´ ì‹¤í—˜ ì‹œì‘ ì‹œê°„
    total_start = datetime.now()
    
    if langgraph_run:
        # 1. LangGraph ë”¥ë¦¬ì„œì¹˜ ì‹¤í–‰
        langgraph_result = await run_langgraph_deep_research(query)
        # ì ì‹œ ëŒ€ê¸° (ì‹œìŠ¤í…œ ê°„ ê²©ë¦¬ë¥¼ ìœ„í•¨)
        await asyncio.sleep(2)

    if a2a_run:
        # 2. A2A ë”¥ë¦¬ì„œì¹˜ ì‹¤í–‰
        a2a_result = await run_a2a_deep_research(query, endpoints=endpoints)

    # ì „ì²´ ì‹¤í—˜ ì™„ë£Œ
    total_end = datetime.now()
    total_time = (total_end - total_start).total_seconds()

    # ê²°ê³¼ ë¹„êµ ì¶œë ¥
    print("\n" + "=" * 80)
    print("ğŸ“Š ì‹¤í–‰ ê²°ê³¼ ë¹„êµ")
    print("=" * 80)

    print(f"ğŸ• ì „ì²´ ì‹¤í—˜ ì‹œê°„: {total_time:.2f}ì´ˆ")
    print()

    # LangGraph ë”¥ë¦¬ì„œì¹˜ ê²°ê³¼
    if langgraph_run:
        print("ğŸ”´ LangGraph ë”¥ë¦¬ì„œì¹˜:")
        if langgraph_result.get("success", False):
            print("   âœ… ì„±ê³µ")
            print(f"   â±ï¸  ì‹¤í–‰ì‹œê°„: {langgraph_result['execution_time']:.2f}ì´ˆ")
            print(f"   ğŸ—ï¸  ì•„í‚¤í…ì²˜: {langgraph_result['architecture']}")
            print(
                f"   ğŸ“„ ê²°ê³¼ í¬ê¸°: {len(langgraph_result['result'].get('final_report', ''))} ë¬¸ì"
            )
        else:
            print(f"   âŒ ì‹¤íŒ¨: {langgraph_result['error']}")
            print(f"   â±ï¸  ì‹¤íŒ¨ê¹Œì§€ ì‹œê°„: {langgraph_result.get('execution_time', 0):.2f}ì´ˆ")

    # A2A ë”¥ë¦¬ì„œì¹˜ ê²°ê³¼
    if a2a_run:
        print("\nğŸ”µ A2A ë”¥ë¦¬ì„œì¹˜:")
        if a2a_result.get("success", False):
            print("   âœ… ì„±ê³µ")
            print(f"   â±ï¸  ì‹¤í–‰ì‹œê°„: {a2a_result['execution_time']:.2f}ì´ˆ")
            print(f"   ğŸ—ï¸  ì•„í‚¤í…ì²˜: {a2a_result['architecture']}")
            print(
                f"   ğŸ“„ ê²°ê³¼ í¬ê¸°: {len(a2a_result['result'].get('final_report', ''))} ë¬¸ì"
            )
        else:
            print(f"   âŒ ì‹¤íŒ¨: {a2a_result['error']}")
            print(f"   â±ï¸  ì‹¤íŒ¨ê¹Œì§€ ì‹œê°„: {a2a_result.get('execution_time', 0):.2f}ì´ˆ")

    # # ì‹¤íŒ¨ ì›ì¸ ë¶„ì„
    # if langgraph_run or a2a_run:
    #     if not langgraph_result.get("success", False) and not a2a_result.get("success", False):
    #         print("\nğŸ” ì‹¤íŒ¨ ì›ì¸ ë¶„ì„:")

    #     if not server_status["mcp_servers"]:
    #         print("   ğŸ“¡ MCP ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ")
    #         print("      â†’ Dockerë¡œ MCP ì„œë²„ë¥¼ ë¨¼ì € ì‹œì‘í•˜ì„¸ìš”")

    #     if not server_status["a2a_server"]:
    #         print("   ğŸŒ A2A ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ")
    #         print(
    #             "      â†’ í…ŒìŠ¤íŠ¸ ìš©ë„ë¡œëŠ” ì„ë² ë””ë“œ ì„œë²„ ì‚¬ìš© ê¶Œì¥: start_embedded_graph_server(...)"
    #         )

    # ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
    comparison_result = {
        "timestamp": datetime.now().isoformat(),
        "query": query,
        "total_experiment_time": total_time,
        "server_status": server_status,
        "langgraph_deep_research": langgraph_result or None,
        "a2a_deep_research": a2a_result or None,
        "comparison_type": "LangGraph ë”¥ë¦¬ì„œì¹˜ vs A2A ë”¥ë¦¬ì„œì¹˜ êµ¬í˜„ì²´ ë¹„êµ",
    }

    # ê²°ê³¼ë¥¼ reports/ í´ë”ì— ë‚ ì§œ í¬í•¨ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥
    reports_dir = PROJECT_ROOT / "reports" / "step3"
    reports_dir.mkdir(parents=True, exist_ok=True)
    filename = f"comparison_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    output_path = reports_dir / filename

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(comparison_result, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ğŸ ì‹¤í—˜ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # í˜¸ì¶œìì—ì„œ ê²½ë¡œë¥¼ ì•Œ ìˆ˜ ìˆë„ë¡ ë°˜í™˜ ë°ì´í„°ì— í¬í•¨
    comparison_result["output_path"] = str(output_path)
    return comparison_result
